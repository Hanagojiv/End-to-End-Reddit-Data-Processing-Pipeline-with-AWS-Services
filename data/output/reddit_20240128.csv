id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1945s14,Guess the data type ಠ_ಠ,,516,48,Thinker_Assignment,2024-01-11 16:31:27,https://i.redd.it/t9nn0bq88ubc1.png,0,False,False,False,False
198kif0,"My company just put out 3 data engineering jobs last year, guess who we got?","As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. 

We ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. 

They've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. 

All of them were looking for jobs for at least 4-6 months. 

Just wanted to share a data point on the state of the market last year in 2023. 

Funny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ",513,109,Justanotherguy2022,2024-01-17 01:44:50,https://www.reddit.com/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/,0,False,False,False,False
19bg4jf,I’m releasing a free data engineering boot camp in March,"Meeting 2 days per week for an hour each. 

Right now I’m thinking: 

- one week of SQL
- one week of Python (focusing on REST APIs too) 
- one week of Snowflake 
- one week of orchestration with Airflow
- one week of data quality 
- one week of communication and soft skills 

What other topics should be covered and/or removed? I want to keep it time boxed to 6 weeks. 

What other things should I consider when launching this? 

If you make a free account at dataexpert.io/signup you can get access once the boot camp launches. 

Thanks for your feedback in advance!",341,174,eczachly,2024-01-20 16:55:10,https://www.reddit.com/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/,0,False,False,False,False
18xj97r,Why does nothing ever get used?,"Dashboards, views, tables, pipelines, entire data marts. Why does 90% of the work I do never get used?   

I used to be one of the best BA's in my entire company so I am very good at requirements gathering and understanding what the business is trying to accomplish. Most of the work that I get comes from the CEO/VP level (global corporation not startup so real CEO and real VP 's) so a lot of people seem like they are very invested in solving these problems and my work always gets rave reviews.....but once things go into prod they basically never get touched.  

Six months ago I just.... stopped doing QA.. I have been relying on the ""scream test"", I mark tickets resolved and immediately move to prod and only do QA if someone screams that something is wrong. I have yet to hear back on anything.",253,66,None,2024-01-03 13:20:50,https://www.reddit.com/r/dataengineering/comments/18xj97r/why_does_nothing_ever_get_used/,0,False,False,False,False
19f9dba,"Well guys, this is the end",🥹,234,127,marclamberti,2024-01-25 13:30:21,https://i.redd.it/mnp1r3m49lec1.jpeg,0,False,False,False,False
199fg7g,Did I get bamboozled into a data engineering job?,"I'm coming up on 1.5 YoE at my job where my title is ""data analyst"". This is my first real job and I got it out of college. Up until today, I assumed that I was a data analyst doing data analysty things and building a career in data analytics. However, since finding out that data engineering is a separate thing, I've started to suspect that I may actually be working in an entry-level data engineering role.

The job description asked for mastery of Tableau and proficiency in Python. Since starting, I've used Python for scripting a fair amount, but have used Tableau EDA a grand total of zero times. They trained me up in Alteryx, an ETL tool, and now my work mainly consists of Alteryx, SQL, and Python.

90% of my work is building automated data pipelines for other teams; they come to us with some process that they're doing manually in Excel and we make it automatic for them. We follow an Agile framework, gather requirements, build and test, deploy and support. Our typical end product is an app that another team uses, not a dashboard.

Am I actually a trainee data engineer? ",209,58,WarCrimeWizard,2024-01-18 02:43:48,https://www.reddit.com/r/dataengineering/comments/199fg7g/did_i_get_bamboozled_into_a_data_engineering_job/,0,False,False,False,False
19c2ftl,Some Data Scientists write bad Python code and are stubborn in code reviews,"My first job title in tech was Data Scientist, now I'm officially a Data Engineer, but working somewhere in Data Science/Engineering, MLOps and as a Python Dev.

I'm not claiming to be a good programmer with two and a half years of professional experience, but I think some of our Data Scientists write bad Python code.  


Here I explain why:

* Using generic execptions instead of thinking about what error they really want to catch
* They try to encapsulate all functions as static methods in classes, even though it's okay to use free standing functions sometimes
* They don't use enums (or don't know what enums are used for)
* Sometimes they use bad method names -> they think `da_file2tbl_file()` is better than `convert_data_asset_to_mltalble()` (What do you think is better?)
* Overengineering: Use of design patterns with 70 lines of code, although one simple free-standing function with 10 lines would have sufficed (-> but I respect the fact that an effort is made here to learn and try out new things)
* Use of global variables, although this could easily have been solved with an instance variable or a parameter extension in the method header
* Too many useless and redundant comments like:  
`# Creating dataframe`  
`df = pd.DataFrame(...)`
* Use of magic strings/numbers instead of constants
* etc ...

What are your experiences with Data Scientists or Data Engineers using Python?

I don't despise anyone who makes such mistakes, but what's bad is that some Data Scientists are stubborn and say in code reviews: ""But I want to encapsulate all functions as static methods in a class or ""I think my 70-line design pattern is better than your 10-code-line function"" or ""I'd rather use global variables. I don't want to rewrite the code now."" I find that very annoying. Some people have too big an ego. But code reviews aren't about being the smartest in the room, they're about learning from each other and making the product better.  


Last year I started learning more programming languages. Kotlin and Rust.  I'm working on a personal project in Kotlin to rebuild our machine learning infrastructure and I'm still at tutorial level with Rust.  Both languages are amazing so far and both have already helped me to be a better (Python) programmer. What is your experience? Do you also think that learning more (statically typed) languages makes you a better developer? ",181,132,noisescience,2024-01-21 12:31:19,https://www.reddit.com/r/dataengineering/comments/19c2ftl/some_data_scientists_write_bad_python_code_and/,0,False,False,False,False
18wnsqj,Data Testing Cheat Sheet: 12 Essential Rules," 

1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.
2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.
3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.
4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).
5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.
6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.
7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.
8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA & production environments.
9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.
10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.
11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.
12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.

[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&format=png&auto=webp&s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)",178,10,icedqengineer,2024-01-02 11:59:27,https://www.reddit.com/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/,0,False,False,False,False
197t8fz,"Apache Iceberg: SQL and ACID semantics in the front, scalable object storage in the back",,176,27,bitsondatadev,2024-01-16 03:52:52,https://i.redd.it/731s003m5qcc1.jpeg,0,False,False,False,False
1913k8k,Who are the GOATS of DE?,"This can a subjective question, DE is still niche and there is no such thing as a ranking but wanted to know if you guys have a high role model in the area. 

For example in programming there are well respected names on the likes of Linus Torvald or Guido Van Rossum.

This can be any inspiring youtuber, book writer, DE influencer or whatever.",140,116,hot-bulbasur,2024-01-07 21:46:35,https://www.reddit.com/r/dataengineering/comments/1913k8k/who_are_the_goats_of_de/,0,False,False,False,False
1abmrzv,"yes, I really said it",,271,72,Awkward-Cupcake6219,2024-01-26 16:45:56,https://i.redd.it/8jacgiphctec1.jpeg,0,False,False,False,False
19difxp,Is the Data Space really this Complicated or am I just overthinking?,"For some reason, everytime I try to learn I see new tools and how they ease the existing work. And I end up wasting more time where if I spent that on actually learning, I would be way ahead. How do you know which tool to pick and choose(from the noise in the market) ?

https://preview.redd.it/ji5thy5f05ec1.png?width=2013&format=png&auto=webp&s=167f4e2afce621cc135d5a0ff7d5c484fedaa032",108,88,_areebpasha,2024-01-23 06:55:02,https://www.reddit.com/r/dataengineering/comments/19difxp/is_the_data_space_really_this_complicated_or_am_i/,0,False,False,False,False
19cak0s,what is it that you do for work again?,"&#x200B;

https://i.redd.it/zhg215lraudc1.gif",104,26,mesirmysir,2024-01-21 18:51:36,https://www.reddit.com/r/dataengineering/comments/19cak0s/what_is_it_that_you_do_for_work_again/,0,False,False,False,False
1935ykj,Reality check: How good are you at the skills in your tech stack?,"So let's start by saying this is by no means a post to complain but just to get a reality check and understand what you guys mean when you name a technology in your tech stack. I was navigating the sub and found the posts like the salary discussion or newbies asking for help, and you've got the usual comments throwing around a bunch of technologies like it's fresh water. Oh sure just learn aws, sql, python, powerbi, airflow, terraform and docker and you're good to go, took me a year. Obviously started questioning if I'm dumb since I've been at this job 4 years and I'm currently mastering SQL after spending 3 years on PowerBI only

So I want to understand when you name these tools and put them in your tech stack, how good are you actually at this and how much is it just ""I understand the basics and I can Google/ChatGPT the rest"" ?

Let's take SQL for example. There's a huge difference between ""Udemy course"" level of knowledge (you got the basic idea, can use SQL up to subqueries and Window Functions) and that one colleague that can write 1000+ lines of stored procedure from scratch and model JSON into tables level of knowledge. Or PowerBI: again there's a difference between ""I can drag and drop objects on the canvas and create a visualization, yaay let me add it to the CV"" and having read The definitive guide to DAX 700+ pages on PowerBI's programming language, understanding how Vertipaq engine works internally and so on. You might say it is overkill to invest that much time in one single tech but that's another topic I don't want to tackle now.

For example, I don't use Python daily at my job, but I can do some stuff with it with the help of Google and Chatgpt. I know the basics of programming, I've done a couple Udemy courses out of curiosity, I know what sets and dictionaries are, I can query an API, do some stuff with the common libraries for data manipulation and return the data. If I have to touch a Python script written by another DEV to modify something specific, I can do that. But I don't have profound knowledge of the internals, I wouldn't know how to optimize code, I probably couldn't do heavy tasks on Python-built infrastructure unless the task was very clear or build something enterprise-level from scratch myself. Do you think I should name Python in my tech stack? Is this an acceptable level of knowledge for you to name in the tech stack? 

So yeah I just need to know what's the idea of this sub on this topic because there's one of two possible outcomes:

1. I'm studying in the wrong way, and it's taking me a lot more than a normal person to really understand these tools
2. I am underselling myself and suffering from imposter syndrome or something like that

Cheers ",101,74,schizo_coder,2024-01-10 11:15:25,https://www.reddit.com/r/dataengineering/comments/1935ykj/reality_check_how_good_are_you_at_the_skills_in/,0,False,False,False,False
19501yg,My whole team hates DLTs and I don't blame them.,"We have been using databricks(aws) close to a year now and have started working with DLTs \[Delta Live Tables\]. I personally don't hate them as much as my teammates but I don't blame them, a lot of DLT limitations are in direct contradiction with the Databricks vision. Reasons listed below:

* You have to be on shared compute, this complicates reading data and writing back out to s3 if you need to drop a file (need to be in single user mode)
* BIGGEST COMPLAINT: You cannot ""hop"" catalogs or even schemas. This is so weird to me. They are rolling out DLT and UC \[Unity Catalog\] and are pushing customers hard on both, but DLTs directly contradict the medallion architecture. You want to have data land in a bronze catalog, then move it to a silver, gold, etc. Great, create a pipeline for each and kill your job runtime because you now have to spin up 3 different computes. They had a private preview that allowed you to write to multiple schemas but they killed it. Why?
* DLTs have to run from workspace notebooks, because GIT providers can only point to a users specific repo, not a config'd repo and branch like a notebook job. Luckily we have DABs to control our deployment and skirt this issue but it seems so odd to us. The DLT documentation recommends setting up a repo per pipeline, thats insane!
* Cannot share compute across multiple pipelines. To my second point, the limitation of not being able to hop schemas/catalogs wouldn't matter if I could specify DLT compute to use in the three pipelines processing the data. That solves a huge problem.
* Documentation, community support is still weak.

I do love the automation DLT brings with ingesting data, specifically CDC data. But there are some pain-points that make absolutely no sense. I think Databricks is doing too much too fast and needs to refocus on what they were initially, a data platform that provided one place to do everything.",94,62,DataDoyle,2024-01-12 17:06:52,https://www.reddit.com/r/dataengineering/comments/19501yg/my_whole_team_hates_dlts_and_i_dont_blame_them/,0,False,False,False,False
1905tj2,Which tools are worth learning for an aspiring data engineer?,"I'm currently working as a data analyst, but in the near future I'd like to move into a more technical role and delve into data engineering. 

At the moment, I have quite good knowledge of Python and SQL as I work with these languages in my day-to-day job. I also have basic knowledge of general database concepts like normalisation/views/stored procedures etc.

But even looking at the entry-level job descriptions, I feel that I'm still a long way from getting a data engineering job, because the amount of tools required is insane.

And my question is, which of these tools are actually worth my time to learn? For example, are SQL Server tools like SSIS/SSAS important for data engineering? Or is it better to learn some cloud computing concepts? Or maybe something else?",95,41,Mokebe13,2024-01-06 18:01:08,https://www.reddit.com/r/dataengineering/comments/1905tj2/which_tools_are_worth_learning_for_an_aspiring/,0,False,False,False,False
18tjgp8,Zendesk Moves from DynamoDB to MySQL and S3 to Save over 80% in Costs,,91,11,rgancarz,2023-12-29 09:43:54,https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/,0,False,False,False,False
18x2214,Optimizing a One Billion Row Challenge in with Rust and Python with Polars,"I posted this on /rust and I thought /dataengineering might find it interesting! 

I saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)

Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.


|Implementation|Time|Code/Gist Link|
|:-|:-|:-|
|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|
|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|
|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|
|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\_average\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|

Thanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!",87,29,matt78whoop,2024-01-02 22:13:42,https://www.reddit.com/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/,0,False,False,False,False
1904k5j,DBT Testing for Lazy People: dbt-testgen,"[dbt-testgen](https://github.com/kgmcquate/dbt-testgen) is an open-source DBT package (maintained by me) that generates tests for your DBT models based on real data.

Tests and data quality checks are often skipped because of the time and energy required to write them. This DBT package is designed to save you that time.

Currently supports Snowflake, Databricks, RedShift, BigQuery, Postgres, and DuckDB, with test coverage for all 6.

Check out the examples on the GitHub page: [https://github.com/kgmcquate/dbt-testgen](https://github.com/kgmcquate/dbt-testgen). I'm looking for ideas, feedback, and contributors. Thanks all :)",83,23,fuzzh3d,2024-01-06 17:07:47,https://www.reddit.com/r/dataengineering/comments/1904k5j/dbt_testing_for_lazy_people_dbttestgen/,1,False,False,False,False
18vdch8,Should I be offended? Project manager send me a code from Chatgpt,"I'm working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don't know how to react to this situation, and the code he sent is flawless, I'm at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?",77,116,Zack-s21,2023-12-31 18:05:45,https://www.reddit.com/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/,0,False,False,False,False
19aty1m,Data Engineer offer retracted after I moved cities.,"Hey everyone,

I don't know if this is the right place to post this but I finally landed a data engineering job after being a dashboard jockey for 4-5 years. It was everything I had dreamt of. I was due to start in two days and I find out that the company retracted the offer due to a massive layoff.
I moved to Toronto, Canada and  put down a deposit for a rental, and basically spent a lot of time, energy and effort.
Is there anything I can expect from the company? Should I go back to looking for data analyst roles?
Just a rant, any advice would be awesome.

Thanks!",81,17,gruntywolf,2024-01-19 21:13:48,https://www.reddit.com/r/dataengineering/comments/19aty1m/data_engineer_offer_retracted_after_i_moved_cities/,0,False,False,False,False
18xb4ug,None of what I learned is a job requirement. I am essentially skill-less.,"Hi there, here's an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I've seen recently.

""Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.""

So.. I'm about to graduate from a Master's in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.

Yes the master's teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master's. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master's was a Bachelor's or an ""Intro to Data Literacy"" course.

Where do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master's to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?

Gee. I feel like uni has absolutely not made me job ready in any way.",77,55,Zomdou,2024-01-03 05:06:32,https://www.reddit.com/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/,0,False,False,False,False
194d07l,"Data Engineer - What's the best course, certification or degree of all time?","Hello guys,

I hope you guys are well. I'm curious about your opinions. I'm a data engineer trainee. I want to learn A LOT. Not only SQL, Python, but PySpak, etc, etc.

But I'm curious: What's the best course, or certification (specialization) or degree of all time for you, that you can end the course and say: ""Wow, f\*\*\*\*\*\* hell! This was amazing! I learned so much with this!""

I want to know your opinions :)

You can also share books, share what really help you with to grow as a Data Engineer and as a professional :)

Have a good day/night",74,59,GigabyteWarrior,2024-01-11 21:28:02,https://www.reddit.com/r/dataengineering/comments/194d07l/data_engineer_whats_the_best_course_certification/,0,False,False,False,False
18zfz14,Preparing for DE Interviews at FAANG+ companies,"I will try not to dox myself but the end goal for me is to end up as a Senior DE at a large tech company. At the moment I'm ambivalent on whether this results in Data Platform Engineering or Data Analytics Engineering.

Here is my general framework for studying:

1. LC Easy/Medium (Arrays & Hashing, Two Pointers, Sliding Window, Stack, Binary Search, try to solve in 20-25 minutes with no/minimal help)
2. SQL Medium/Hard (Try to solve in 3-5 minutes with no/minimal help)
3. Data Modeling (Identify business needs using Product Sense and create a Star/Snowflake schema from this)
4. Behavioral (standard STAR answers)

I am decidedly not good at algorithmic questions, which is part of the reason why I transitioned to DE (also I think it's cooler, among other things). Is this a good framework to abide by to target dedicated DE roles at FAANG+ companies (I specifically have Meta and Amazon in mind)? Any comments or insight would be welcomed.",74,25,KingTyranitar,2024-01-05 19:59:59,https://www.reddit.com/r/dataengineering/comments/18zfz14/preparing_for_de_interviews_at_faang_companies/,0,False,False,False,False
18who2l,Ways to keep your SQL sharp with minimal effort?,"Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. 

Where would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.",70,25,muhmeinchut69,2024-01-02 05:29:55,https://www.reddit.com/r/dataengineering/comments/18who2l/ways_to_keep_your_sql_sharp_with_minimal_effort/,0,False,False,False,False
18v0dn0,Do you really need to know coding algorithms in your job as a data engineer?,"Data engineer is a type of software engineer and everywhere suggests that software engineers should  know data structures and algorithms like insertion sort, merge sort,  binary search, etc.

I feel like I'm not using any of them at all (or maybe I'm using some but don't aware that I am), but I still able to write tons of Python programs in my ETL development and they work just fine.

As my background is not actually from com sci, I feel it will take me so much time and effort to learn and not worthwhile comparing learning other stuffs and concepts in data engineering like data quality, data modeling, data warehousing, SQL mastery, unit testing, distributed data processing frameworks, streaming, machine learning, MLOps, IaC, DevOps, etc. 

&#x200B;

Happy New Year :)",65,39,soravispr,2023-12-31 05:26:52,https://www.reddit.com/r/dataengineering/comments/18v0dn0/do_you_really_need_to_know_coding_algorithms_in/,0,False,False,False,False
193xq76,Will you stop using dashboards?,"I'm hearing more and more about dashboards dying and moving to ""interactive data apps"". I wonder if this is vendor marketing fluff or if this is actually happening. Thoughts?",63,75,tamargal91,2024-01-11 09:29:34,https://www.reddit.com/r/dataengineering/comments/193xq76/will_you_stop_using_dashboards/,0,False,False,False,False
1933ach,Someone finally did it! r/learndataengineering is live!,"After seeing the same basic question again, I was like “I think it’s enough. It’s time to create a new subreddit” and when I tried to create r/learndataengineering, it said community already exists. Voila!


My nominations for questions that should go there:
- What makes a good Data Engineer?
- Do you need <tool/skill> to be a Data Engineer?
- What do you all think about <tool/skill>?
- What <project/skills> should I work on if I want to be a Data Engineer?
- Is Data Engineering a good role for me to transition to from <current role>?
- Read my blog post about DuckDB/Polars/…


Since I had to choose a flair, I went with discussion. What questions/topics do folks think belong in that subreddit?


Edit: I guess it’s an old subreddit (3+ years). Regardless, since it exists, I think we should utilize it",61,12,jawabdey,2024-01-10 08:10:34,https://www.reddit.com/r/dataengineering/comments/1933ach/someone_finally_did_it_rlearndataengineering_is/,0,False,False,False,False
1abov9g,"Something for fun, what abilities would you give this card?",,125,24,AMDataLake,2024-01-26 18:12:55,https://i.redd.it/8om179jgstec1.jpeg,0,False,False,False,False
199lx3r,How do you spend your time waiting for things to run?,"Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y'all do while waiting for something to run?",61,53,an27725,2024-01-18 09:03:31,https://www.reddit.com/r/dataengineering/comments/199lx3r/how_do_you_spend_your_time_waiting_for_things_to/,0,False,False,False,False
18z9z3x,"In your opinion, what makes for an ""elite"" data engineer?","Personally, I think that communication skills are one of the aspects at the top of the list. Additionally, a depth in database technology, distributed systems, ETL/ELT, and business use cases. But for how long will those aspects remain true, since many things evolve over time, especially rapidly?

Do you think have intermediate ML engineering knowledge, high performance computing, or any other present tangential knowledge will become table stakes? ",58,40,Tender_Figs,2024-01-05 15:50:24,https://www.reddit.com/r/dataengineering/comments/18z9z3x/in_your_opinion_what_makes_for_an_elite_data/,0,False,False,False,False
18ybkzq,Discussion: Is SQL all you really need? A perspective from an older colleague.,"Half rant, half a discussion point.

I having a conversation with my colleague the other day.  I'm quite a bit younger than them and have more ""modern"" skills in the sense I've mainly worked on cloud, use Python for automation, and, of course, SQL whereas they are very much on prem experienced.  Unfortunately, one of my colleagues, another DE, let's call them Angela, insists absolutely everything is a SQL problem.  If it can be done in SQL, it absolutely must be done in SQL.  Every piece of data, no matter the format, must be stored in a database.  Angela refuses to use anything else.

One of my colleagues asked about learning Python and how to go about it for somebody they know. I recommended for somebody trying to career change, doing a Udemy course for the basics and to get a taste of what's possible and then move into project building as soon as possible would be the advice I give.  My colleague then said thank you for the recommendation as there are so many courses out there it's really hard to pick a good one.

Angela then says very proudly, ""Ah, this is the problem with modern technologies.  See, when I learnt SQL 15 years ago, it hasn't really changed much so I've never had to learn anything new.  With modern technologies, it always changes and you could be using something completely different next year whereas I've only ever used SQL at all of my jobs"".

Rant portion: This made me feel rather uneasy because this isn't limited to just code bases.  Angela isn't a fan of CI/CD (the answer to all problems including cross server database deployments in the cloud is a SQL script).  Angela barely believes in source control (literally local development of SQL scripts is how they work despite being told by management to work differently).  From this anecdote, it's very clear that Angela sees SQL as the one true god which will outlast everything and therefore every other language is a waste of time.  What makes this doubly worse is, in my opinion, they have absolutely terrible work practices which makes it difficult for everybody else.  One of the most triggering things was that they added a lot of FK constraints to logging tables, this caused CI/CD to fail, and then they blamed CI/CD rather than kinda shitty FK assignment.

Discussion portion: It isn't a secret the opinion of SQL being the most important skill to have in the world of DE is probably the most popular one and I'm pretty sure we can all agree that CI/CD, source control, good software engineering practices etc. are non negotiable.  

How many people feel like Angela about SQL? 

I found it a really interesting perspective as my mindset has been, ""if something new and useful comes a long, guess I'll have to learn it"".  Whilst SQL is important, I don't agree with the notion that DE starts and ends with SQL, I think general purpose programming outside of relational DBs is just as important as SQL.",64,101,average_ukpf_user,2024-01-04 12:04:16,https://www.reddit.com/r/dataengineering/comments/18ybkzq/discussion_is_sql_all_you_really_need_a/,0,False,False,False,False
19dry3i,What do you think about Apache Pulsar?,"Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.

It has been an Apache top-level project since 2018 ([https://pulsar.apache.org](https://pulsar.apache.org/)). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.

Unlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.

When I first heard about the Pulsar, I thought my dreams had come true. 🌈🦄

Now I'm trying to understand why there is not much buzz in public about it:

* Is it a marketing flaw and people just didn't ever hear about it?
* Or is there something wrong or missing in Pulsar?
* Maybe it is just an overkill for most new projects?

I would greatly appreciate hearing about your experience and thoughts on Pulsar.",58,77,visortelle,2024-01-23 16:12:17,https://www.reddit.com/r/dataengineering/comments/19dry3i/what_do_you_think_about_apache_pulsar/,0,False,False,False,False
19d18sj,How often do you get bored?,"I work at a company I would consider to be top-tier. Great pay for where I live, nice work culture, great people, and good upside potential (company could sell or do IPO in the next few years). But I just feel bored. It's not challenging anymore. When I came on board 6 months ago, I rebuilt everything from scratch and now it's all working almost flawlessly. Our team is small, and we've scaled efficiently so not created more work for ourselves. I do 2-3 hours of real work every day. I have 1-2 short meetings a day if that, answer questions, fix small things that break. I like to feel like I've done something at the end of the day, and more recently the past few months, this is rare. Honestly, most days I feel guilty for not working more. I take time during the day to listen to audiobooks (while working), I work out, take lunch with friends, or similar. It's the opposite of burnout. It's unnerving to not be busy or feel like I'm adding value.  

Is anyone else's shop like this? What are your recommendations?",55,40,ntdoyfanboy,2024-01-22 17:36:51,https://www.reddit.com/r/dataengineering/comments/19d18sj/how_often_do_you_get_bored/,0,False,False,False,False
192fhgg,Why we are ignoring Julia for data engineering and going for rust??,"I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.

We all agree python has issues and major of them are
1. Its slow as hell
2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem 
3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern

Because users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors 

I agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn’t solve the packaging problem of python

Why don’t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors",59,94,__albatross,2024-01-09 14:06:39,https://www.reddit.com/r/dataengineering/comments/192fhgg/why_we_are_ignoring_julia_for_data_engineering/,0,False,False,False,False
194rp77,Best way to learn pyspark.,"Hello Everyone, I am having 5.5 years of experience in data engineering mostly worked on teradata. Currently in gcp services. Before making another switch I want to be fundamentally good at pyspark by having some hands-on experience. What will be best way I can follow to learn pyspark or and good courses that I can take if any. Please suggest.",57,22,TheErenYeager03,2024-01-12 10:11:08,https://www.reddit.com/r/dataengineering/comments/194rp77/best_way_to_learn_pyspark/,0,False,False,False,False
19aeuyr,It seems that a bunch of US work has shifted overseas to EU.,"I noticed that a lot of people from the US have trouble finding a data job in the US(and people with substantial experience). 

In the EU the situation seems fine, I get recruited at least 5 times every 3 months. Maybe I've been lurking here for too long and started to take this as the real world, and its a different thing in actuality. For me I cannot imagine someone with a lot of experience to not have a job in tech, no matter the technology or how ""old"" it is  there is always something to maintain or develop. 

What is happening in the US? Is this jobless crisis coming to the EU soon? Is it only Data Engineers that are affected or all data people(or all tech people)?",54,89,DiskoSuperStar,2024-01-19 09:00:36,https://www.reddit.com/r/dataengineering/comments/19aeuyr/it_seems_that_a_bunch_of_us_work_has_shifted/,0,False,False,False,False
18zljvi,Is there such thing as a “product data engineer” who isn’t just a slave to the reporting team?,"I’m still relatively new to the field of data engineering, and I’ve noticed some patterns about the kind of work that data engineers handle and who that work benefits.
  
I did a data engineering internship this last summer at a midsized fintech company, which was my first time seeing “real” data engineering firsthand. The work itself was pretty interesting and I liked the people I worked with, but came to the conclusion that the only benefit that my work had on the company was getting data into the hands of the reporting team. 

I came to this realization because as I was updating my resume with my new internship experience, the only way that I could really quantify my work was essentially “saved time and effort for the reporting team.” I had no clue what my impact was on the business or my work was being used. It was kind of an alarming feeling.
  
I think I’d like to ask: is this normal? Is most data engineering work geared toward just being data stewards, and getting the data to other people?
  
Is there such thing as a data engineer that works on a “customer-facing” product, where I’d be able to tangibly explain how I impacted the product or business?",57,44,NFeruch,2024-01-05 23:51:43,https://www.reddit.com/r/dataengineering/comments/18zljvi/is_there_such_thing_as_a_product_data_engineer/,0,False,False,False,False
1abfe9x,Whose job is it to add primary keys to tables?,"I’m a ETL developer. I have about 400/2500 tables that don’t have primary keys. I asked my manager why they don’t have primary keys and he said he doesn’t know. I asked if there was any documentation and there isn’t. I asked if I had to make them up for each table and he said yeah. I’m supposed to make them up? How? He said yeah it’s really annoying but it’s something we have to do…I need primary keys in the table for my code to work that migrates the data from source to target. 

Aren’t the DBA or the person who created the table supposed to do it? I’m genuinely curious because I don’t think I can sit there and go through 400 tables and make up a primary key for each table. Like I can’t mentally handle sitting there and going through each table like that. I’m considering finding a different job over this. Has anyone else done this before? It’s 400 tables for christs sake…",68,111,briogeosucks,2024-01-26 10:33:51,https://www.reddit.com/r/dataengineering/comments/1abfe9x/whose_job_is_it_to_add_primary_keys_to_tables/,0,False,False,False,False
195tn9u,"After finally getting my dream job, I was switched to other role against my will.","I was hired as a consultant and started to work as a Data Engineer on a startup. A lot of work but I really enjoyed working on it. However, they decided to fire a entire team and put me instead as the developer of that project. When I opened the files, it is just a mess. Spaghetti codes, multiple logics that just gets overwritten and no documentation at all. I discovered that the logic didn't work at all and I have been mostly finding issues with the code.

I asked the business analyst to help me understand all the logic and he doesn't know either. There is no proper documentation of the reqs of the project (the original business analyst was fired for this too), they changed the project owner since the original moved to another company and this new guy doesn't know either what to do.

Well, I suggested to spend time to try to understand the project itself but got called off by everyone since they need this done by February. It just happens that I am in a trial period in this job and it also ends in February. They are expecting me to finish this one but honestly I feel lost since there is no one to contact about the full scope of the project, I am working blinding and doing some ""screaming"" qa.

By the way, the original stack was Python Pandas, DBT, Airflow, AWS, Postgresql and they were planning to move to Snowflake, and now in this project is just plain SQL for MariaDB. The fricking test to enter the company were hard, I studied a lot of these data engineer tools to enter, even Kubernetes, Docker, Terraform and now I am just a SQL developer. Honestly, I don't mind working with SQL but because the incompetence of other people, they switched me over since they are out of people and they are no plans to hire a new team for this. I just hope that I can recover back my original role.",54,35,DataSenpai,2024-01-13 17:49:57,https://www.reddit.com/r/dataengineering/comments/195tn9u/after_finally_getting_my_dream_job_i_was_switched/,0,False,False,False,False
19ab6dx,How true is this thing of getting “blacklisted” for wrong titles on resumes?,,53,75,gotchabiash,2024-01-19 05:04:06,https://i.redd.it/h51r0yfcxbdc1.jpeg,0,False,False,False,False
198w13u,The Ultimate Guide to Unit Testing With dbt,,50,13,ivanovyordan,2024-01-17 13:01:34,https://datagibberish.com/p/unit-testing-with-dbt,0,False,False,False,False
18vubnp,Happy new year engineers and data enthusiasts!,"What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   


If you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. ",49,13,Interesting-Rub-3984,2024-01-01 10:44:10,https://www.reddit.com/r/dataengineering/comments/18vubnp/happy_new_year_engineers_and_data_enthusiasts/,0,False,False,False,False
19fkptt,Interviewing at Apple,"I've been applying to jobs in FAANG since November and I finally got a call back from Apple. Actually I got callbacks from 3 different teams  in the span of 2 weeks. 

So I've got my first interview on Monday and Im confident in my technical skills. I was wondering if anyone here who has worked for, or interviewed at, Apple had any insights or advice that could be useful. 

2 of the roles are more standard data engineering positions and 1 is a devops role (SRE).

Edit: I have 3 YOE so these are all mid level positions",53,11,bigYman,2024-01-25 21:45:04,https://www.reddit.com/r/dataengineering/comments/19fkptt/interviewing_at_apple/,0,False,False,False,False
19cxuav,Am I too fussy?,"Hi guys! seeking some advice on my data engineering career.

Long story short: in 3 years I have had 4 different jobs. I left all of them. I don't know if I am asking too much to companies or I am the problem.

Long story:

I am in my mid 20s. I left all companies due to different factors (no pay raise, bad projects, bad management...). My longest job has been 9 months (actual job). Recruiters keep sending me offers but, would jumping so much affect me in the long run?

Another question I have: why do folks stay at a bad company? I have seen tons of tech employees working at a company they don't like for years. Obviously I am not saying just leave, but look for opportunities. It really amazes me.

Those are my main points because I am starting to think that I am the problem and I should stay at a company although it doesn't have all the requirements I need...

Thoughts on this?",53,109,data_macrolide,2024-01-22 15:14:12,https://www.reddit.com/r/dataengineering/comments/19cxuav/am_i_too_fussy/,0,False,False,False,False
194q4yk,How clean is your code?,"I’ve recently joined a data engineering team after a few years as a web engineer. I am not and expert but know the basic design patterns and principles. The team I joined writes really bad code.. but to be honest I’m not sure it matters? Most of these scripts never need to be adjusted and will continue to move data. 

I’m curious what coding standards you implement in your code. Are you using object oriented designs?",48,51,Commercial-Wall8245,2024-01-12 08:22:45,https://www.reddit.com/r/dataengineering/comments/194q4yk/how_clean_is_your_code/,1,False,False,False,False
19533tp,Projects that would impress you,"As a recruiter or a hiring manager for a data engineering entry level position what projects would impress you more or make a student stand out, I know quite a bit of SQL and Python (I can solve Leetcode medium to hard in both) what would you recommend as project that would help me get into DE, thanks",48,12,Some-Landscape-4763,2024-01-12 19:13:18,https://www.reddit.com/r/dataengineering/comments/19533tp/projects_that_would_impress_you/,0,False,False,False,False
18uote4,Is modern data stack relevant for small data orgs?,"I am looking to build the data/analytics function in a non-tech/data organization. We are not a startup, I have an on-prem Oracle ERP database that has about 25 years of activity in it (\~ 500GB). We have a few cloud-based enterprise apps (CRM for instance) that are cloud-based that we use but up to now we've not tried to extract data or do any integration. I've been tasked to change that. Operational data is mostly done via Access queries on the OLTP ERP database and shared Excel files. I just started using PowerBI to build out a couple dashboards for leadership but data governance/quality is a real issue. We have one DBA and myself to work on this.

My question is ... since we haven't moved to the cloud yet (although I see that coming at some point), and I see the need for data governance and something like a data warehouse/mart (i.e. modeled data, cleaned up snapshots over time, etc.) in order for us to make headway on providing data that is actionable, is it worth looking at modern data stack tools or should I really be looking at 90's era tools and techniques? Is something like Snowflake or Databricks reasonable for small data (never going to be more than 1 TB total)? Or should I just focus on building a local Postgres warehouse with Kimball/Inmon and ignore things like data lakes, Fivetran, DBT, Snowflake, and Airflow? Is there an in-between/hybrid approach that would work?",49,57,Inevitable_Log9395,2023-12-30 20:25:11,https://www.reddit.com/r/dataengineering/comments/18uote4/is_modern_data_stack_relevant_for_small_data_orgs/,1,False,False,False,False
18svyhx,Who would emerge as the winner between Databricks and Snowflake in the race of all things Data and AI?,what are the differentiators when both of them are encroaching into each other's territories ,49,72,Ok-Tradition-3450,2023-12-28 15:04:28,https://www.reddit.com/r/dataengineering/comments/18svyhx/who_would_emerge_as_the_winner_between_databricks/,0,False,False,False,False
18ydlv0,"What do you guys think of ""Fundamentals of Data Engineering"" book?","I'm a DE with over three years of experience. Due to the heavy promotion/recommendation of the book by DE influencers, I decided to read it myself. So far, I've read over 50% of the book, and I have mixed feelings about it. As a DE with some experience, I find the book overly bloated with unnecessary technical details that are difficult to remember and discourage me to read sometimes, rather than providing an overview of specific technologies and possibly more useful use cases. Sometimes after reading a chapter, I can easily remember the general topic but not the specific details. The only time I find the book valuable is when I have a reference to a service I've used (usually aws services) and can visualize a use case for it. However, I can imagine that this wouldn't be the case for everyone, especially beginners. I also miss a more engaging narrative style in the writing. The writing style comes across more like a technical leaflet with bullet points and defined terms.

I understand that my feedback may seem a bit harsh, but my intention isn't to roast the book. I'm glad that a book on this topic was created and I encourage to try it yourself, even if the writing style wasn't appealing to me. I wanted to ask you what's your opinion on that? If you have some tips on how to approach/read this book to get the most out of it? ",46,27,thethewaza,2024-01-04 13:49:22,https://www.reddit.com/r/dataengineering/comments/18ydlv0/what_do_you_guys_think_of_fundamentals_of_data/,0,False,False,False,False
196mzxv,What's the cheapest way to host Airflow for personal projects?,"I'm looking to build up some DE personal projects in my spare time. Mostly just in the interest of learning and upskilling with no plans to profit off of anything.

I'd like to orchestrate some DAGs through Airflow, and would like to host in on AWS. At work we use MWAA which would be way overkill, and I'm curious what the cheapest hosting strategy would be for hobbyists? I'm planning to play around with the idea of running all tasks as ECS/EKS tasks, so the core infra doesn't need to support anything too intensive.",43,28,mccarthycodes,2024-01-14 18:49:58,https://www.reddit.com/r/dataengineering/comments/196mzxv/whats_the_cheapest_way_to_host_airflow_for/,0,False,False,False,False
19eio7f,Allowing report access to 35000 people,"Hello!

I work for a retail firm and we are running an incentive over 6 months. I produce the incentive reports using python code and we usually share it with a few important salespeople.

However this year, management is looking at creating something like a powerbi report and allowing access to all salespeople. The number of salespeople will be about 35,000. 

I'm experienced with python and powerbi but I've never done something that needs to be accessed by such a wide audience.

Does anyone have any experience with such a scenario where a lot of people need access to a report and how it was achieved?

Edit: I forgot to mention that the salespeople are in a hierarchical structure and one should not see details of another hierarchy ",42,45,kkchn001,2024-01-24 14:43:51,https://www.reddit.com/r/dataengineering/comments/19eio7f/allowing_report_access_to_35000_people/,0,False,False,False,False
196ntm5,Should I take this job offer as a PowerBI Developer?,"About me: 25M, < 2y experience, graduated May 2022 with degree in Mathematical Economics. Currently working in a business analytics role for a non-tech firm It's pretty simple, fairly low pay and atp quite stagnant in terms of learning/progressing, but it's a good experience

I just got offered a new job as a Power BI developer, that pays about 15k+ more. On one hand, it seems like an obvious accept, however the two main issues I have are:

* I want to learn more technical stuff beyond building dashboards. I mean, yes dashboards and visualizations are important, but I really enjoy doing actual large-scale computational science work. I find it much more intellectually stimulating and interesting.
* I don't want to stay in my current city, which the new job is in. I don't really like it here tbh, it's small and boring. Ideally, I want to go to a bigger city (nyc or SF).
* Little-to-no DE focus, apart from ""ocassional SQL queries"" they threw into the job description.

Am I being naive to turn this offer down? Any input is greatly appreciated.",41,31,OutrageousPressure6,2024-01-14 19:24:50,https://www.reddit.com/r/dataengineering/comments/196ntm5/should_i_take_this_job_offer_as_a_powerbi/,0,False,False,False,False
19121xo,Data engineering cases from real companies,"Hello,

I am looking to sharpen my skills as a Data Engineer to work in sectors I am interested in. Is there such thing as a website where Data Engineering problems from real companies are posted to be solved and/or used for practice? I am partial to the finance and gaming sectors. Thanks.",44,15,Prestigious-Dot-71,2024-01-07 20:46:33,https://www.reddit.com/r/dataengineering/comments/19121xo/data_engineering_cases_from_real_companies/,0,False,False,False,False
18ym35o,Are you using Iceberg in production?,"I’m a big believer in the data lake, even during the Snowflake hype. But it’s always been difficult to setup and use. Iceberg (as well as Hudi and Delta) made data lakes easier with built-in support for upserts and optimizations.

Databricks had been pushing Delta and I’ve seen it used in production at varying scales. But I think Iceberg is a better, more open alternative. Aside from Netflix I haven’t seen much about companies using it in production.

Are you using Iceberg in production? 
Are you considering migrating existing data lake to Iceberg?",44,63,royondata,2024-01-04 19:50:03,https://www.reddit.com/r/dataengineering/comments/18ym35o/are_you_using_iceberg_in_production/,0,False,False,False,False
19ap4ga,Data Analyst > Analytics Engineer,"Hi everyone, I’ve been a data analyst for about 4 years, primarily using SQL on a day to day basis. I’ve been cleaning raw data and creating data tables to use for analytics and dashboards. I feel like I’ve been doing both an analytics engineer and data analyst job. 

I’ve recently been laid off and I want to pivot into the analytics engineering route. Does anyone have any recommendations on how I can get started? Any additional skills I need to learn? 

Thanks in advance!",39,24,xloserr,2024-01-19 17:53:52,https://www.reddit.com/r/dataengineering/comments/19ap4ga/data_analyst_analytics_engineer/,0,False,False,False,False
18tx3o5,Analytics Engineering in 2024,,39,13,oleg_agapov,2023-12-29 20:50:52,https://dbtips.substack.com/p/analytics-engineering-in-2024,0,False,False,False,False
19en4gl,Switch career from Data Engineer to Data Architect,"What should i focus on developing my skills to move to Data Architect from a Data Engineer? Is it worth it career wise? 
Also if i could get a good resource or links to projects to learn data architecture that would be much appreciated.",37,23,bhoo1,2024-01-24 18:19:05,https://www.reddit.com/r/dataengineering/comments/19en4gl/switch_career_from_data_engineer_to_data_architect/,0,False,False,False,False
19bvq2q,Is there interested in a Data streaming 101 course based on Rust and WebAssembly?,"A number of data folks I respect has recently nudged me with an idea to create a data streaming 101 course since I have been managing an open source data streaming project and a managed cloud service for over a year now.

I have thought about it a few times in 2023, and I'd like to ask the community, if you folks would like a data streaming 101 course.

A bunch of good ones already exist. Here is how this one would be different.

The implementation and hands on labs of this data streaming course would be based on Rust and Web Assembly. It would be entirely self hosted. There would be a bit of complexity to grasp, but I would work to make it as simple as possible.

I am thinking 7 emails with the course content delivered with text, video, and supporting code in a GitHub repo.

The only investment for the course would be time. And that too not a lot. Say 2 - 3 hours to consume the content and 2 - 3 hours to implement the labs.

Does this sound interesting? Let me know in the comments.",39,34,drc1728,2024-01-21 05:10:08,https://www.reddit.com/r/dataengineering/comments/19bvq2q/is_there_interested_in_a_data_streaming_101/,0,False,False,False,False
18t52ro,DDIA,Isn’t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I’m stupid 😂,39,25,Repulsive-Ad7769,2023-12-28 21:28:10,https://www.reddit.com/r/dataengineering/comments/18t52ro/ddia/,0,False,False,False,False
198lq59,What is next after Lakehouse architecture?,"Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. 
Is there something new in this space ready to replace lakeshouses?",35,67,brokeRichieRich,2024-01-17 02:40:48,https://www.reddit.com/r/dataengineering/comments/198lq59/what_is_next_after_lakehouse_architecture/,0,False,False,False,False
195k5tz,Data Fatigue?,"I work at a smallish company but we don’t spend a lot on data team resourcing. So by default I’m the all inclusive data engineer, architect, analyst and requests come from all departments. 

Anyone here find it really challenging to source, ingest,  model, shape AND then do analysis?

I used to be analyst but had a much smaller slice of the pie and did no engineering, I was good at doing the analysis and making recommendations.  But now I get to the end of the whole process and I really struggle to analyse the data, anyone else been here or have any tips?",36,22,variance-explained,2024-01-13 09:20:32,https://www.reddit.com/r/dataengineering/comments/195k5tz/data_fatigue/,0,False,False,False,False
18ynoig,I recorded a PySpark Big Data Course (1+ Hour) and uploaded it on YouTube,"Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am adding the link below, have a great day!

[https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=8&t=74s](https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=8&t=74s)",37,6,onurbaltaci,2024-01-04 20:55:35,https://www.reddit.com/r/dataengineering/comments/18ynoig/i_recorded_a_pyspark_big_data_course_1_hour_and/,0,False,False,False,False
192f47n,"Do you feel the tension between ""data democratization"" and the business need for high quality curated data?","If so, how do you deal with it?",35,16,datarbeiter,2024-01-09 13:49:03,https://www.reddit.com/r/dataengineering/comments/192f47n/do_you_feel_the_tension_between_data/,0,False,False,False,False
19cyd9g,Where’s the boundary between “the added complexity doesnt outweigh the benefit” and “scalability?”,"I see arguments from time to time that it’s fine going straight into using spark, airflow, highly available rdbms, advanced git vc architectures, CI/CD, kubernetes, or whatever. The argument is typically that the designer expects the business to need to go this route eventually, and why design a system that you know you’ll have to redesign later on for scalability reasons?

Then there’s the other argument that these systems are completely unnecessary for small time guys, despite the fact that they offer many unique quality-of-life features that aren’t replicated elsewhere. The complexity of these systems alone seems to warrant a need for implementation, and if that need isn’t satisfied them it’s considered an unnecessary effort. Resume driven development.

So where’s the line between these perspectives? Does it depend on team size and knowledge? What else?",34,21,DuckDatum,2024-01-22 15:37:36,https://www.reddit.com/r/dataengineering/comments/19cyd9g/wheres_the_boundary_between_the_added_complexity/,0,False,False,False,False
190vs11,Meta Round 1 Technical Interview,"Howdy compadres,

I have an upcoming first round technical Meta de interview. I'm curious if anyone have any info on the general difficulty of the questions? Would stratascratch mediums cover it or should I amp it up? A lot of info on the meta swe interviews out there but not a ton on the de ones (at least for this specific stage).

I'm fairly confident I can handle most joins/aggregations etc.. but you know the deal, interviews like this make you question your skillset.",34,16,bolognaisass,2024-01-07 16:23:09,https://www.reddit.com/r/dataengineering/comments/190vs11/meta_round_1_technical_interview/,0,False,False,False,False
18wdkvp,"Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?","Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?",34,24,MasterKluch,2024-01-02 02:05:26,https://www.reddit.com/r/dataengineering/comments/18wdkvp/accepted_a_6_month_contract_to_hire_job_while_im/,0,False,False,False,False
18uesn8,I shared a Python Course (1.5 hours) on YouTube,"Hello, I shared a Python Course on YouTube. It is completely beginner friendly, I started with installation of Python and I finished the course with classes. I am leaving the link below. Happy new year!

[https://www.youtube.com/watch?v=VOdPQmm298o&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=1](https://www.youtube.com/watch?v=VOdPQmm298o&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=1)

&#x200B;",35,14,onurbaltaci,2023-12-30 12:39:30,https://www.reddit.com/r/dataengineering/comments/18uesn8/i_shared_a_python_course_15_hours_on_youtube/,0,False,False,False,False
193a5e1,Less technical + large salary increase or more technical + slight salary increase,"Hi all, first off I would like to thank this sub massively. I’m more of a lurker than a poster but it taught me everything I needed to know to transition to DE roughly 1.5 years ago.

To put it simply, I have two offers, A & B. I wondered if some of the engineers on this sub could offer some advice.

A) 1.5x salary, more seniority, outdated tech stack (think SSIS, Teradata). Role is more of an analytics engineer I’d say.

B) 1.1x salary, same seniority, great tech stack for learning (GCP, Scala, Spark). Role is much more Software Engineering with a data speciality.

Background) I’m early careers. Using the outdated stack in A), currently in a less senior analytics engineer type role. Probably overcompensated in my current role for my age/experience. Non Computer Science but engineering background/education so I pick up technical skills relatively quickly.

Goal) I’m very good with and leading people so I’d like to eventually lead a team of engineers once I’ve learnt the relevant skills to understand the difficulties and pain points engineers have. Interest lies more in the technical software side rather than analytics.

Any comments would be much appreciated!

TLDR - Should I be learning or earning in early stages of my career!",33,33,el527,2024-01-10 14:57:17,https://www.reddit.com/r/dataengineering/comments/193a5e1/less_technical_large_salary_increase_or_more/,0,False,False,False,False
197xr9y,Open-Source Observability for the Semantic Layer,,33,10,Srammmy,2024-01-16 08:07:40,https://github.com/data-drift/data-drift,0,False,False,False,False
191m6ke,"People who transitioned to DE, how did you study?","I'm a junior Data Analyst/Scientist consultant, mostly doing analytics and BI, but I also work with machine learning.


I want to move into Data Engineering. I've started learning on my own and I'm curious about how others have done it and would recommend doing it.


My plan is to learn the basics on my own, replicate 3 full projects from the web, then do my own project on something I like. 


Does this sound good? What do you think? Do you know some end to end project that you would consider worth learning from?",32,30,Deiice,2024-01-08 14:16:08,https://www.reddit.com/r/dataengineering/comments/191m6ke/people_who_transitioned_to_de_how_did_you_study/,0,False,False,False,False
1abifu5,What is your current biggest day to day pains in building pipelines?,"Is it a lack of connectors for a particular tool, is your data shuffling too much, what are the things causing you pain these days?",37,79,AMDataLake,2024-01-26 13:33:12,https://www.reddit.com/r/dataengineering/comments/1abifu5/what_is_your_current_biggest_day_to_day_pains_in/,0,False,False,False,False
19d6f86,First time connecting to an API,"Backstory - First let me say I’m a newish BI analyst and in a department of me. We are starting to develop a data culture but I need to show value. I’m using PBI and made several reports from on prem databases. 

My goal is to connect to an API and put the acquired data into a MS Access database. The API returns the data in JSON format. 

I’ve written a very simple Python program that pulls the data from the API on a daily basis and places it in a JSON file to be consumed by PBI. 

I’m not a data engineer by any stretch of the imagination but I figured this would be the right sub to ask this question. 

Is it possible for Python to pull the data from the API and insert into an Access database while also performing some ETL? Am I in way over my head if I’m very new to Python?",29,18,jebert32,2024-01-22 21:08:49,https://www.reddit.com/r/dataengineering/comments/19d6f86/first_time_connecting_to_an_api/,0,False,False,False,False
193dqrc,Are DE’s less prone to first wave layoffs?,"I’ve been at companies before in a DS role and observed round after round analysts and DS impacted. But sole DE responsible for critical infra related to billing etc was the last holdout. This, in addition to fun of learning something new, inspired me to transition to DE years ago. I can see an impending storm this year based off many corporate red flags and hints.

I’m wondering if I should go to greener pastures like city or utility type DE work. I have made myself the sole DE responsible for critical billing calculations and infra at current company but my comfort level when layoffs start isn’t very high and I maybe have 4-5 months of savings not the 1 year that is vaunted. 

The last thing I want is to be impacted and forced to get a job that I’m not excited about or find out the wait to get an offer is 9+ months for an experienced DE",34,21,Data_Dork,2024-01-10 17:28:21,https://www.reddit.com/r/dataengineering/comments/193dqrc/are_des_less_prone_to_first_wave_layoffs/,0,False,False,False,False
196erg9,A Guide to Data Lake Interview Questions,,32,0,Intelligent_Tune_392,2024-01-14 12:24:33,https://itcertificate.org/blog/data-lake-interview-questions,0,False,False,False,False
18y56lj,How hard is it to switch from pyspark to scala?,"Hi,

I've worked for a while with pyspark, did some courses, read a few books etc. I also did some courses and now feel quite comfortable with OOP in python. I haven't worked with any languages other than python/sql.

Now it is possible that I'll be moved to a project with spark+scala (batch programming, so no streaming there). How hard is it to pick it up for someone with no Java background?

Can you recommend some courses/books? I think of picking up 'Scala for the impatient'. 

If it were up to me, I'd stick with python for now and become more advanced with it, but the market in my location sucks (and so do I with my skills), so I won't have much of a choice.

Thanks

&#x200B;",32,19,Visual-Exercise8031,2024-01-04 05:19:41,https://www.reddit.com/r/dataengineering/comments/18y56lj/how_hard_is_it_to_switch_from_pyspark_to_scala/,0,False,False,False,False
18tho6r,Next-Gen ETL Tools,"Hello everyone

I really would appreciate some guidance. From what I've read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I've experience with Talend, Informatica PowerCenter, and SSIS .

But i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?

Our DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools

Edit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc",31,73,thebatman7727,2023-12-29 07:44:16,https://www.reddit.com/r/dataengineering/comments/18tho6r/nextgen_etl_tools/,0,False,False,False,False
19cld0z,My Thoughts on EcZachly/Zach Wilson's Bootcamp V.3,"I was already hovering over [Zach's Bootcamp](https://dataexpert.io/) but was a bit insecure since the price was huge and a few not many positive comments on two posts from this subreddit [here](https://www.reddit.com/r/dataengineering/comments/14ieh8u/any_feedback_on_zach_wilsons_data_engineering/) and [here](https://www.reddit.com/r/dataengineering/comments/12otxy6/interview_prep_anyone_in_zach_wilsons_data/). So I have seen the posting about a PPP discount based on the country that you live in, since Brazil's economy is kinda crap, I decided to try, if I got I would buy, otherwise maybe think a bit more and try on another opportunity. To my surprise, I was selected! Now I will give you guys my feedback for all the weeks since I got the both tracks course.

It's important to notice that I have been a Data Engineer for almost 2 years, but never worked on big tech, FAANG, etc. meh experiences, not complete garbage but nothing mind-blowing, know a bit of Scala, worked with Airflow, PySpark, Cloud, Pandas... The classic stuff.

TLDR: Was worth it? Yes. Further, I will point out a few things that made it worth it. Just the knowledge may not be worth it for you.

Week 1 - Dimensional Data Modeling

This first week, I believe Zach was extremely motivated to teach, the classes were insightful and focused on data engineering basis, there I fixated on the differences between OLTP and OLAP, and also learned about the existence of Master Data.

There he points out a lot about **how** you are delivering your data, and the importance of noticing that for each kind of consumer, you may want to prepare the data in different ways.

Learned about additivity on the dimensions, a term that I had never heard before the boot camp, and also about SCD tables, I don't know why, but never heard about this one before too.

Week 2 - Fact Data Modeling

This second week Zach was also extremely motivated, I believe these two topics are his favorite, not that he wasn't motivated on the others, but the difference between Week 1 and 2, and the rest was clear. There I also fixated on the difference between a fact and a dimension.

During this week Zach taught about techniques for fact tables deduplication, and ways to aggregate fact data into lists or binaries format to get fast analytics.

It's good to point out that Zach brings a lot of his experience to FAANG-like companies, so some cases will not apply to you probably, but it is nice to know how happens there, this extends to the whole boot camp.

Week 3 - Analytics Track - Analytical Patterns

Here Zach taught about what kind of patterns to aggregate data would suit better for each type of requirement, for example, what to use when we are looking for root causes, what to use when looking for rankings, etc.

One insightful class from this week was related to the data engineering interview process (usually on big techs), he told me about what to expect in terms of technical tests, what to pay attention to during the coding interview, tips and tricks for window functions, and there I learned also a new thing that never seen before GROUPING SETS, GROUP BY CUBE and GROUP BY ROLLUP.

Week 3 - Infrastructure Track - Flink Streaming

I hated this week, not by Zach's fault, but I didn't like streaming, I think it was good knowledge, but certainly not enough time for someone who has never seen that before. I believe that for people like me that never used or seen Flink before, I was only able to digest and understand the theoretical part, like Kappa and Lambda architecture, or the concepts of micro-batch and near real-time, etc.

During the labs, we used Flink with Kafka, I have never used both of them, but tbh, I was warned, he says on the requirements sections that for infrastructure track: ""Basic understanding of Docker, Flink, and Kafka."" So if you want to do the boot camp, try to look just a bit to understand, it will make your life easier.

I discovered that maybe I don't want to work on Uber lol

Week 4 - Analytics Track - KPIs and Experimentation

This week Zach taught about leading and lagging metrics, another concept that I have never heard before, and also [Timothy Chan](https://www.linkedin.com/in/trchan/) taught about A/B tests, experimentation, etc. Tim is a nice guy, but the content for me, was boring.

Week 4 - Infrastructure Track - Spark Batch

Here was one of the most awaited weeks, here Zach covered topics from the basics of Spark theory, so what is a plan, driver, and executor, to JOIN optimizations and tuning. We have seen differences from the caching and broadcasting, as well as Notebooks x Spark Submit. It was nice but maybe expecting something different.

Week 5 - Analytics Track - Data Quality

Here I can summarize that it was related to the importance of trust in data, and what kind of data quality checks we can use for different cases and each type of table. I used my notion annotations from this class as a cheat sheet to check if I am not missing any type of QA check. Interesting to point out to you guys that he mentioned an Airbnb framework called MIDAS, google it when you have time.

The second class was presented by a Brazilian fellow that is specialized in dbt, it was interesting, of course, have heard about dbt but never had the opportunity to try it.

Also here we learned about data design document building, and I liked it.

Week 5 - Infrastructure Track - Also Data Quality

This week wasn't anything mind-blowing, but was important, here we discussed about differences between SE testing and DE testing, why they have higher quality standards, why most organizations miss the mark

In the second part of this week, the Airflow God [Marc Lamberti](https://www.linkedin.com/in/marclamberti/) caught the reins and gave us a presentation on the theory of data contracts, best practices on data validation, and ways to enhance the data quality, followed by the technical part using Airflow.

Week 6 - Analytics Track - Visual Impact

Here we had a class where the knowledge there was insightful but not useful for me yet, he discussed challenges and what separates the senior data engineer from the staff data engineer, as a few career insights more related to professionals in higher places of the hierarchy, so not absorbed much in my POV, since I am still kinda a minion.

The theory behind Dataviz was taught here, it would be like maybe the Week 3 classes being used in real life, very insightful, for those who are looking for analytics engineering, this week is a must.

Week 6 - Infrastructure Track - Pipeline Maintenance

This one was maybe even harder to digest than the Flink one for a reason, I never had to schedule maintenance on pipelines, reduce costs, or optimize computing on pipelines yet. This kind of stuff is out of my decision power, so great content, but not applicable to me. He taught about the impact of ownership on projects, the significance of domain knowledge, and effective communication. Another example that he talks about is related to tech debt and data migration, so yeah, I have never had to deal with that, so kind of abstract for me.

I have to point out a few things about this boot camp:

1. I thought the weekly homework would be easy peasy, Udemy quiz-like. I couldn't be more wrong. They are hard and require a lot of time. If you don't mind about the certification and the mentorship program, you don't need to worry about that.
2. Zach has a discord community for those who are in his boot camp, there you can chat with your peers, Zach, and people from other boot camps, it's nice and helps keep the engagement.
3. With the boot camp, you gain access to past classes and talks from people who have been there, so you can watch for example the [Joe Reis](https://www.linkedin.com/in/josephreis/) talk that happened during V3 boot camp.
4. Weekly is a Career Development Q&A with [Sarah Floris](https://www.linkedin.com/in/sarah-floris/), we can ask questions, tips for LinkedIn, etc.
5. For those who do the homework on time, we have access to a weekly coffee chat with Zach, where we can ask questions for him. Extremely worth it, that was what motivated me the most for doing homework, I could participate in all of them, and it was nice to be on the last one, because the first one had like 80 people, and the last maybe 8, so only the warriors were there.
6. Access to other classes like LLM-related or 30-minute classes to prepare for technical interviews, like data architecture, data modeling, SQL, and DSA.
7. In the end, we have a capstone project that we developed by ourselves with a few requirements, fetching all the knowledge, it is a good idea, but this one was too much for me, the due date is on Jan 26th, but I will not be able to finish it, marriage ceremony preparation, masters and other life things are draining too much time for me to dive into that, but I would recommend doing that.

With those points above I feel that was worth it, it was intense, but I feel grateful for the knowledge. As I said before if you are already a data engineer master, that is the data modeling king, and all the topics that I mentioned you are comfortable with, or at least with most of them, maybe it will not be worth it for you, this boot camp is more suited for someone that already know something, but still need to climb the ladder, so maybe an end junior\~end mid-level range.

For the V.4 boot camp, Zach removed from the curricula the pipeline maintenance and dataviz week, but it will be available from my cohort and will be adding a dbt week and an end-to-end Machine Learning week though, to be honest, I am not a big fan of ML and didn't fall in love with dbt, so I would prefer doing my version lol, but I am sure that it will be cool too.

I am sure that on many points Zach is improving the UX of his boot camp, so things that were bad from the V.2 were better on V.3 and the V.4 will be better than mine. I conclude with if you can, do it, but be prepared to dedicate 6 weeks to that, just watching the recorded classes is a waste of an opportunity.

If you guys have any other questions about the boot camp I am glad to answer them, I know that it is not cheap and you may feel insecure, you can ask here or reach me on DM.",31,63,abbadb,2024-01-22 02:51:26,https://www.reddit.com/r/dataengineering/comments/19cld0z/my_thoughts_on_eczachlyzach_wilsons_bootcamp_v3/,0,False,False,False,False
19cfrl8,What resources did you use to learn Spark?,"I’ve been looking to compile some different spark/pyspark learning links into a repo for reference. What sites/courses/etc have you found helpful? Ideally free, open source resources. Thanks!",29,15,zachtsk,2024-01-21 22:28:11,https://www.reddit.com/r/dataengineering/comments/19cfrl8/what_resources_did_you_use_to_learn_spark/,0,False,False,False,False
193vm5s,"Moved from a small company, which uses simple cloud services to a big corporation, that uses robust frameworks for ETL, processing and validations.","Data changed from few thousand records to millions of records. Everything is well organized, tracked and validated. 

I’m struggling to learn so many frameworks, and understanding huge data with a lot of attributes for data modeling. 

Not able to handle the pressure and feeling of not belonging here, as everyone seems to know what they are doing and deploying lot of changes to production, every week. 

I have been working here for 2 months and I got a feedback that I should be faster. I’m not able to progress due to fear of getting fired. People who joined few months before seem to be performing better or meeting expectations.

Did any face these challenges while changing organisation? How did you handle it? Any suggestions?",29,16,Agile-Letterhead6262,2024-01-11 07:02:56,https://www.reddit.com/r/dataengineering/comments/193vm5s/moved_from_a_small_company_which_uses_simple/,1,False,False,False,False
193dhko,Do you think that most job posts that ask for distributed computing actually require distributed computing?,"I've seen a major uptick in job posting asking for Spark and Kafka. 

Kafka I understand why it's asked for, but aside from setting up a consumer every now and then, from what I've seen, it isn't a significant part of a lot of warehouse data pipelines. 

For Spark, most workloads for most data products can be handled without a Spark setup, or is abstracted with current database/datawarehousing solutions. 

While I understand that there are a growing amount of products and applications that require or would largely benefit from these two technologies, I imagine that there also an overwhelming amount of data pipelines that don't really need or use it. 

However it feels like 30-40% of recent job listings are asking for experience with those. 

Do you think it's because companies are trying to setup to leverage real time data AI solutions? or do you think it's mostly employers that ask for skills for the sake of putting skills?",30,33,Justanotherguy2022,2024-01-10 17:17:52,https://www.reddit.com/r/dataengineering/comments/193dhko/do_you_think_that_most_job_posts_that_ask_for/,0,False,False,False,False
18yy280,Having a very hard time as a Data Engineer,"how did you land your first proper paying data engineering role, i have worked in shadow for 2 senior data engineers in a MNC doing all their work all by myself for 700 USD/month. Despite having skill set and experience ,because i am in a third world country(Nepal) i cannot find a decent data engineering job.

Would you guys care to join in and share your experience and guide DEs like me?",29,19,Horror_Comment6061,2024-01-05 04:34:36,https://www.reddit.com/r/dataengineering/comments/18yy280/having_a_very_hard_time_as_a_data_engineer/,0,False,False,False,False
18xldbk,One Billion Row Challenge—using SQL and DuckDB 1️⃣🐝🏎️🦆,"u/gunnarmorling [launched a fun challenge](https://www.morling.dev/blog/one-billion-row-challenge/) this week: how fast can you aggregate and summarise a billion rows of data?

I'm not a Java coder (which is what the challenge is set in) but thought it'd be fun to do it in SQL with DuckDB nonetheless.

Loading the CSV in is simple enough:

    CREATE OR REPLACE TABLE measurements AS
            SELECT * FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';') LIMIT 2048;

as are the calculations:

    SELECT station_name, 
               MIN(measurement),
               AVG(measurement),
               MAX(measurement)
        FROM measurements 
        GROUP BY station_name

The funky bit comes in trying to reproduce the specified output format:

    SELECT '{' || 
                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||
                '}' AS ""1BRC""
        FROM src;

The final script looks like this, and takes about 26 seconds to run:

    ❯ /usr/bin/time -p duckdb -no-stdin -init 1brc.opt2.sql
    -- Loading resources from 1brc.opt2.sql
    
    WITH src AS (SELECT station_name,
                        MIN(measurement) AS min_measurement,
                        CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean_measurement,
                        MAX(measurement) AS max_measurement
                FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';')
                GROUP BY station_name)
        SELECT '{' ||
                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||
                '}' AS ""1BRC""
        FROM src;
    100% ▕████████████████████████████████████████████████████████████▏
    1BRC{Abha=-33.0/18.0/69.2, Abidjan=-24.4/26.0/75.4, Abéché=-21.1/29.4/77.1, Accra=-25.1/26.4/79.0, […]Zanzibar City=-23.9/26.0/77.2, Zürich=-39.0/9.3/56.0, Ürümqi=-39.6/7.4/58.1, İzmir=-32.8/17.9/67.9}Run Time (s): real 25.539 user 203.968621 sys 2.572107
    
    .quit
    real 25.58
    user 203.98
    sys 2.57

**👉 Full writeup:** [**1️⃣🐝🏎️🦆 (1BRC in SQL with DuckDB)**](https://rmoff.net/2024/01/03/1%EF%B8%8F%E2%83%A3%EF%B8%8F-1brc-in-sql-with-duckdb/)",31,2,rmoff,2024-01-03 15:01:04,https://www.reddit.com/r/dataengineering/comments/18xldbk/one_billion_row_challengeusing_sql_and_duckdb_1/,0,False,False,False,False
18tgg69,Best practices for documenting database design,"Are there resources for best ways to go about *documenting* database designs? 

I realize that one can export a database schema, add some comments to it, and use diagrams to represent it visually. But, these seem deficient in that they do not capture the semantics or rationale for the underlying fields/tables. I’m especially curious about established resources that lay this out in detail, especially ones that I can show to others in support of best practices (a textbook would be ideal, but I realize that’s a tall order for what I’m looking for).",30,20,rubikol,2023-12-29 06:28:12,https://www.reddit.com/r/dataengineering/comments/18tgg69/best_practices_for_documenting_database_design/,0,False,False,False,False
19a1pf2,How do you guys orchestrate DBT transforms?,"New to using DBT, I would love to know if there are any open source tools in the market that you guys use to schedule these jobs. If not, do you guys just go about building python scripts for the orchestration?",28,55,Lucky-Front7675,2024-01-18 21:40:52,https://www.reddit.com/r/dataengineering/comments/19a1pf2/how_do_you_guys_orchestrate_dbt_transforms/,0,False,False,False,False
191cpwm,What do you like and not like about being a data engineer?,"I was a technical/data PM that switched to the data/analytics engineering space because I got tired of talking to all teams and people ata company every day and actually enjoyed the peace and quiet of running queries, updating data models, using Looker, getting fresh (as we call it at my place lol instead of raw) to usable data for the data science team etc. What I don’t like about it, is that sometimes an ad hoc request from sales or an incompetent mba grad will be top priority for leadership, and I have to export the results in google sheets or something. Like isn’t that something for an analyst to do??

What about you?",26,13,imjusthereforPMstuff,2024-01-08 04:41:42,https://www.reddit.com/r/dataengineering/comments/191cpwm/what_do_you_like_and_not_like_about_being_a_data/,0,False,False,False,False
18yzbp8,How do I manage Apache Iceberg metadata that grows exponentially in AWS?,"I've been a developer for over a decade, but I'm new to Data Engineering. I set up a couple Iceberg tables in AWS Glue and S3. I've been replicating my production data to these tables for a couple of weeks (\~100k-300k inserts per day) and saw that our S3 storage size was exploding. After a little analysis, 99% of this storage was metadata. In the worst case, one table had only 13GB of actual data and 66TB of metadata (I emptied that bucket pretty quickly). Several other buckets had 200 MB to 2GB of data and still had 5TB to 7TB of metadata.

Is it normal for Iceberg to accumulate metadata so quickly? or is this just a factor of having so many inserts on a daily basis?

I tried running the ""OPTIMIZE table"" query in Athena, which I got from the Athena documentation, but it only scans about 2GB and takes 30 mins per run, which is way too slow to do by hand on 5TB. After a couple days of Googling and reading the docs, I still don't feel like I'm getting anywhere. How does anybody manage this stuff? Seriously, any feedback, suggestions or links are appreciated.

Thanks!

[Let's play \\""Can you spot the Iceberg tables?\\""](https://preview.redd.it/lcr99hk47kac1.png?width=1413&format=png&auto=webp&s=c9dcb18f207369eb5443b17cfc4bf29cc951bc7d)",27,12,EntrepreneurFitz,2024-01-05 05:42:01,https://www.reddit.com/r/dataengineering/comments/18yzbp8/how_do_i_manage_apache_iceberg_metadata_that/,0,False,False,False,False
18vwegq,How should I replace NaN values?,"I have a column named 'normalized-losses' in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can't do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks in advance

&#x200B;

https://preview.redd.it/iq0ty9pyut9c1.png?width=710&format=png&auto=webp&s=f29e1af711003115fe90d00b85fec080c01eb271

https://preview.redd.it/ignyqkpyut9c1.png?width=1037&format=png&auto=webp&s=51a2858f63d99edbe72dbd030f582f18a87197c7",28,10,Darktrader21,2024-01-01 13:06:44,https://www.reddit.com/r/dataengineering/comments/18vwegq/how_should_i_replace_nan_values/,0,False,False,False,False
1ablzb7,Why you're not getting hired -- Tips for those looking for a new job.,"## Context
I've seen a few posts/comments on the top of getting a new job. Talking about a tough job market, learning skills, etc. The comment I wanted to write on a few of those deserved a top level post IMO.

The market is somewhat saturated at the moment with the recent layoffs and hiring freezes/slowdowns, so you won't just get an offer or three thrown at you just because you applied a few places. Here's the simplest way to get hired.

## Differentiate yourself

A hiring manager or recruiter has to sift through tens to hundreds of resumes to fill a position; your resume needs to be have a reason why it should be chosen over your fellow applicants. **If your resume reads like every other applicants', then you're likely not going to get interviews**. The follow up question, then, is how do you differentiate yourself? Here are a few suggestions

1. Tailor your resume to the job description. At the very least, use keywords that the company is looking for. If the company isn't looking for it or it isn't relevant, take it off your resume; it's wasted space.

2. Write each bullet in your job descriptions demonstrating one of either two things (1) business impact or (2) differentiating skills. For business impact bullets, convey the outcome that your actions had on the business and not a description of what you did. For skills-related bullets, mention how you used specialized technology to solve a business problem. You're probably not being hired to be a technologist but to help drive business success.

3. Clearly demonstrate your skill sets!  Saying you can write Python is fine. Having a link to an easy to navigate Github.

4. Have side projects. This is an important carve out of #2 above. If your skill is passion for the industry or willingness to tackle unsolved problems, having a side project is the most effective way to demonstrate this and stand out from the crowd. 

5. Network. If you don't have enough experience to have differentiated job descriptions nor are you willing/able to put in time on a side project, your best bet to get interviews is to bypass the resume screening altogether. Go to events, meet people, tell them you're looking for a job, and/or offer to help them out however you can. If you can't travel, there are online meetups.",62,22,butwhhyy,2024-01-26 16:13:39,https://www.reddit.com/r/dataengineering/comments/1ablzb7/why_youre_not_getting_hired_tips_for_those/,0,False,False,False,False
194vdyx,How does your business implements their ETL pipeline (if at all)?,"I'm curious about how's the landscape out there, and what is the general maturity of ETL data pipelines. I've worked many years with old school server based GUI ETL tools like DataStage and PowerCenter, and then had to migrate to pipelines in Hive (Azure HDInsight) and blob storage/hdfs. Now our pipeline is just custom python scripts that run in parallel (threads) running queries on Google BigQuery (more of an ELT actually).  


How are you guys doing it?  


1- Talend, DataStage, PowerCenter, SSIS?  
2- Some custom solution?  
3- Dataproc/HDInsight running spark/hive/pig?  
4- Apache Beam?  
5- Something else?",27,69,rikarleite,2024-01-12 13:44:25,https://www.reddit.com/r/dataengineering/comments/194vdyx/how_does_your_business_implements_their_etl/,0,False,False,False,False
193mypp,"How do you manage uninterrupted focus time amid job failures, slack messages, emails, meetings?","Some members of my team have raised some points about not having enough focus time, due to: 

* general email
* data issues (via slack/email
* slack messages (within team or outside)
* meetings (fairly minimal but still disruptive of the flow)
* in-person questions - usually from within team (we have semi-open floorplan)

a senior data engineer mentioned he rarely gets 90 minutes  of uninterrupted development work per day. Our small team  manages about a dozen third party clients (with unclean data sometimes) plus our own internal processes and . Some weeks, it's not bad, other times there are a string of issues/interruptions. 

What are some specific techniques y'all have employed to focus?

* snooze slack
* focus time slack status (do people respect this?)
* custom slack status
* block calendar time
* how to handle in-person chatter/questions? 
   * headphones on means focus?",28,27,gman1023,2024-01-10 23:42:39,https://www.reddit.com/r/dataengineering/comments/193mypp/how_do_you_manage_uninterrupted_focus_time_amid/,0,False,False,False,False
18yc4zs,How does everyone see the future of data engineering and its demand?,"When I started not too long ago, data engineers did a little bit of everything.  They would set up the data infrastructure, write data pipelines, and talk to stakeholders.




Nowadays, all three of those job responsibilities are becoming their own specialized roles on data teams.  As time goes on, I'm imagining that these roles will continue to become even more specialized.




Also a related question, but is anyone trying to go into any specific area of data engineering now that work has become more specialized?  Will one area of data engineering be more in demand than the others?",27,5,level_126_programmer,2024-01-04 12:34:53,https://www.reddit.com/r/dataengineering/comments/18yc4zs/how_does_everyone_see_the_future_of_data/,0,False,False,False,False
18xr0wg,Does my bad query leaves a bad impact?,"Guys, I had an interview at a company which gave me an assessment. The assessment was pretty long and I had to share my solution to them. I haven't really worked with geospatial data and used a technique that left a query running for three days and it has impacted their database performance. 

Anyway the relevant person talked to me via email and he was very polite and professional. I owned my mistake and gave a couple of resolutions too.

Now I have a presentation coming up at the company. I would be presenting my solution. What do you guys think I should be prepared of? Is it a straight up rejection in their minds. Or what kind of questions should I prepare myself for. 

I would appreciate any help. I am really frustrated at this moment.",28,29,milostough,2024-01-03 19:03:27,https://www.reddit.com/r/dataengineering/comments/18xr0wg/does_my_bad_query_leaves_a_bad_impact/,0,False,False,False,False
18w1ufk,"DataFlint, a new open source Performance Monitoring for Apache Spark","Hi everyone!  
I released a new open source library and would like to get some feedback from r/dataengineering!

The library is a modern UI for apache spark. It can show you in which step your query is currently running, which steps is the longest and alert on potential performance issues

Link to the github page: [https://github.com/dataflint/spark](https://github.com/dataflint/spark)

[DataFlint Demo](https://reddit.com/link/18w1ufk/video/td74k6056v9c1/player)",28,7,menishmueli,2024-01-01 17:40:12,https://www.reddit.com/r/dataengineering/comments/18w1ufk/dataflint_a_new_open_source_performance/,0,False,False,False,False
18vyykn,Enterprise data solutions - how does your look like?,"I've been reading a ton of books about modern data management, data mesh, is modeling/dwh dead, etc., lately. Now I'd like to hear some real-life examples of how different companies/ppl architect their data warehouses/lakes/swamps/meshes and what things work or don't and why. E.g.:

**Centralized vs decentralized** \- do you have one centralized data engineering team that handles everything for the whole company or do you have a mini-team per department? What are some upsides/downsides of your setup?

**Streaming/real-time** \- Do you do batch vs stream processing or both? Is there a push for real-time analytics in your company and are you able to deliver?

**Data modeling** \- How do you approach data modeling, have you tried creating and maintaining one huge or several smaller ""generic"" data models or do you have a data model per domain or even a data model per report? Do you use star schema, snowflake or data vault or one bit table?

**Self-service** \- what do you do to maximize the ability of your consumers to self-serve?

Alternatively, if anybody knows of more public docs like the [Gitlabs handbook](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/edw/) let me know.

Thanks!",27,19,InsightInk,2024-01-01 15:28:21,https://www.reddit.com/r/dataengineering/comments/18vyykn/enterprise_data_solutions_how_does_your_look_like/,0,False,False,False,False
199n5mr,laid off and looking for next best option,"I have 15+ year exp in DW engg and 8+ yrs in cloud aws with big data tools  including spark and data bricks.  I also have few AWS certifications and cloudera certifications but they are not current now.

I recently got laid off in November and have been looking for opportunities.  seems like now a days all the buzz is ML and AI which i am lacking.  Question to fellow Engineers is it better to learn additional tools/platforms like Snowflake + aws professional level certs or jump into AI/ML learning which i got to say is not natural to me i am finding it difficult to learn.  Any advice

&#x200B;",26,49,Competitive-Fee-4006,2024-01-18 10:30:49,https://www.reddit.com/r/dataengineering/comments/199n5mr/laid_off_and_looking_for_next_best_option/,0,False,False,False,False
194m0y8,SWE Market Vs DE Market,"Hey all!

I've seen a lot of posts recently across Reddit of SWEs feeling the pain from layoffs and having trouble finding new positions out there.

As someone in constant fear of being laid off (mostly just anxiety) I see these sorts of things and get a bit nervous, however, I have seen much less of this in the Data Engineering community.

Not sure if it's just because it's a smaller community or if it's because there's still a good market for Data Engineers.

Any thoughts from the community about the market in comparison to Software Engineering?",26,22,Traditional-Ad-8670,2024-01-12 04:18:23,https://www.reddit.com/r/dataengineering/comments/194m0y8/swe_market_vs_de_market/,0,False,False,False,False
1916jyx,Will Apache Beam die out?,"Apache Beam seems to have a steeper learning curve than Spark, yet I can see some adoption. What are the advantages of it over Spark or other similar tools? Will it become popular, even more popular than Spark, or just die out?",27,37,RevolutionStill4284,2024-01-07 23:48:52,https://www.reddit.com/r/dataengineering/comments/1916jyx/will_apache_beam_die_out/,0,False,False,False,False
18t3ect,How do I document ETL/ELT pipelines?,"Hey,

We’re using a service that allows us to develop ETL pipelines and schedule jobs for automated runs. 

The current setup thoroughly transforms the data step by step, but there are multiple buckets and other locations our pipelines are dropping data into. 

As I continue development, what is a good way to begin documenting the ETL architecture, tools, and how do I go about documenting what is going o? 

How detailed do I need to be when documenting this for new joiners? For now, I will not being using sphinx so I will be using docstrings to comment in, and creating documents myself typed out. 


Thanks",24,23,None,2023-12-28 20:17:34,https://www.reddit.com/r/dataengineering/comments/18t3ect/how_do_i_document_etlelt_pipelines/,0,False,False,False,False
19cgwl4,so Where should i go after reading 'fundamentals of data engineering ' book,yes any Book or course recommendation would help me being better data enginer or any advice 🙏🏼,24,40,Eisa_t,2024-01-21 23:17:12,https://www.reddit.com/r/dataengineering/comments/19cgwl4/so_where_should_i_go_after_reading_fundamentals/,0,False,False,False,False
1922w0r,Data Architect vs Data Engineer,"Hey there, so I'm wondering if there are any major differences between the two roles and what they might look like from others with similar experience around them.  I know data engineers are more implementation and data architect is design, but I was wondering how they relate within the field and what possible career trajectory there might be for each",24,23,morkborkus,2024-01-09 01:57:54,https://www.reddit.com/r/dataengineering/comments/1922w0r/data_architect_vs_data_engineer/,0,False,False,False,False
191y5xq,Should I use Great Expectations or build it myself?,"I'm working on a data engineering project with specific demands for ensuring and validating data consistency. My database is structured into three tiers: a schema with raw data tables, a schema with intermediate tables, and a schema with tables that are 'ready to query'. Therefore, validations need to be performed between these levels, considering various values and conditions.

I attempted to create some expectations using Great Expectations (GE), but I encountered several complexities specific to my use case. For instance, I need to verify whether the sum of the revenue from orders with status equals to ""approved"" in my orders table matches the sum in the approved revenue column of my revenue table. This is a basic example for which I couldn't find any solution using GE.

My question is: should I create a simplified version of Great Expectations to address all my use cases deeply? I feel that I'm going to amount of Toil in either scenario, possibly even more if I choose to implement it using Great Expectations in its entirety.",23,31,Feisty_Albatross_893,2024-01-08 22:31:13,https://www.reddit.com/r/dataengineering/comments/191y5xq/should_i_use_great_expectations_or_build_it_myself/,0,False,False,False,False
18zq0tf,After Data Engineering,"Has anyone made a career in data engineering and it’s predecessor role names, then successfully switched to something unrelated? 

I’m currently about as senior as you can get without going in to management and have had enough. Seems a bit late at 40-something to retrain as an electrician or whatever though.",24,29,Witty_Ad1057,2024-01-06 03:16:35,https://www.reddit.com/r/dataengineering/comments/18zq0tf/after_data_engineering/,0,False,False,False,False
197621x,Building a data driven system from ground up for an organisation,"I started out as a Data Engineer at an organisation that had 0 knowledge about any data driven approaches, they had a data team but all data was being shared with CSV files on teams. I was hired and now I feel like its time I change things. I have just 1 year experience but I feel I have a solid grasp on standard practices on system design and databases. I am alone in this task and I feel doing this will definitely change my impact in the organisation. 

Right now I have completed building API's for every data source that was manually collected on CSV files. I need to think about data lakes, warehouses and ETL processes. 

They do not provide any cloud services and just have a server on-prem that runs a VM which I can SSH into. I do not know how scalable this would be but the amount of data they are collecting and processing is not intensive so I guess it would work for an MVP for now.  

I needed some guidance on how can I start working on data modelling, which frameworks to be using for batch processes, some of the best practices on how I need to start planning the system design for such a large project.

&#x200B;",23,25,IncomeTraditional995,2024-01-15 10:55:36,https://www.reddit.com/r/dataengineering/comments/197621x/building_a_data_driven_system_from_ground_up_for/,1,False,False,False,False
18vsz8v,If you have an etl that is using python to extract data from an api and land it in s3 the write to Postgres - how do you manage local / dev / qa/ prod envs ?,What’s the best way so I can’t practice writing my etl before pushing it to prod?,23,24,citizenofacceptance2,2024-01-01 09:05:17,https://www.reddit.com/r/dataengineering/comments/18vsz8v/if_you_have_an_etl_that_is_using_python_to/,0,False,False,False,False
18uth0e,"Kick the cloud, use vim-databricks to develop locally","For me personally developing on the cloud is a pain. I'm used to and love my local setup, so I wrote a quick plugin to send commands to a databricks cluster from vim: [vim-databricks](https://github.com/kentkr/vim-databricks). The implementation is light weight and currently only supports sending python scripts or lines within those scripts, but there's more to come. Check it out and I'd love to get feedback, thanks!  


&#x200B;",23,13,IntroductionAny3343,2023-12-30 23:46:53,https://www.reddit.com/r/dataengineering/comments/18uth0e/kick_the_cloud_use_vimdatabricks_to_develop/,0,False,False,False,False
19de5wg,Entry level + Fully Remote... How unrealistic?,"My goal is to get a fully-remote, entry-level job in data engineering. How unrealistic is that?

Be brutally honest... better to crush my dreams now rather than study for months only to find out later.

I have no experience in data engineering, but I do have a phd in a stem field and 4 years working as a data scientist",23,52,KimchiFitness,2024-01-23 02:52:12,https://www.reddit.com/r/dataengineering/comments/19de5wg/entry_level_fully_remote_how_unrealistic/,0,False,False,False,False
19brn6q,Thinking of Bailing on My New Job - Need Some Advice,"So, I hopped on board with this new company last month, and turns out they're on a bit of a staff exodus spree. Come next month, it's just gonna be me and two junior engineers left on the engineering team -  not just data engineering but the whole engineering team!! 😬

Thinking of pulling the plug and bouncing to a new gig. As a data engineer, I'll be dealing with numerous data-related questions, but the challenge lies in not having someone to ask why the data is structured in a particular way.

Any of you been in a situation like this? What did you do, and what should I be considering before making my exit?",23,8,Kindly-Screen-2557,2024-01-21 01:28:49,https://www.reddit.com/r/dataengineering/comments/19brn6q/thinking_of_bailing_on_my_new_job_need_some_advice/,1,False,False,False,False
192he2z,DBT test and documentation generation,"All of us **want to move fast**, and while tests and documentation may feel like they slow you down, we firmly believe that the confidence they provide far outweighs their cost in the long term. Yet lots of people we speak to using dbt don't use them.

We built a quick tool that infers tests/documents for you. 

What are people's **thoughts on a tool that infers tests and docs and helps you add them to your schema definition**? 

&#x200B;

[Schema inference](https://reddit.com/link/192he2z/video/mv0k6icgofbc1/player)",22,12,bk1007,2024-01-09 15:34:41,https://www.reddit.com/r/dataengineering/comments/192he2z/dbt_test_and_documentation_generation/,1,False,False,False,False
18xqr9b,I need a mentor,"Previously, I was a mechanical engineer where I was working in a factory setting. However, I did a bootcamp and landed a data engineering role about 8 months ago and I’m excited but struggling. I’m new to the corporate world (politics) and software engineering and want someone to talk to/ have conversations with stay on top of software trends, become more confident in my role, and overall feel good about this career. If interested, please let me know and we can discuss rates.",23,43,geekyabs,2024-01-03 18:53:03,https://www.reddit.com/r/dataengineering/comments/18xqr9b/i_need_a_mentor/,0,False,False,False,False
18vws8a,Looking for the best data engineering course from basic to advanced,"Hey,

I'm on the lookout for some cool data engineering courses to level up my skills and hopefully snag a great job. Any tips or suggestions? Thanks a bunch!",22,27,brainvale,2024-01-01 13:29:44,https://www.reddit.com/r/dataengineering/comments/18vws8a/looking_for_the_best_data_engineering_course_from/,0,False,False,False,False
18vb0gt,Why can’t I just rely on AWS native services instead of running Databricks clusters on AWS for running the analogous workloads?,What’s the differentiator/ draw from an architectural standpoint apart from the obvious cost savings facet?,23,13,Ok-Tradition-3450,2023-12-31 16:16:44,https://www.reddit.com/r/dataengineering/comments/18vb0gt/why_cant_i_just_rely_on_aws_native_services/,0,False,False,False,False
19faswq,What are high in demand skills related to data engineering?,"Hey folks, hope you're all doing well. i got asked this question amd not sure how to respond. So thought of spurcing some ideas here.",25,11,ephemeral404,2024-01-25 14:38:47,https://www.reddit.com/r/dataengineering/comments/19faswq/what_are_high_in_demand_skills_related_to_data/,0,False,False,False,False
19cukvp,"How do you manage the amount of data ""assets"" in your business?","I've been wondering recently just how much of a problem this really is.

To run a data engineering department, you have, in some capacity, all of the following:  
\- source data (API's, files, external DB tables)  
\- storage (blobs, databases, lakehouses, more files)  
\- pipelines (code, no-code, low-code  
\- dashboards (customer facing, internal, observability) 

All of these exist across different codebases/repositories, tools, teams, software.

How the hell do you manage all of it?

Sure, you can say ""documentation"", but even in the best case, you have a Confluence that has the documentation which explains everything, a README in the Git repo, but it still does little to explain everything that you own and how it all links together, short of a few diagrams that are likely outdated.

Does anybody do this in what they would consider a ""good"" way? Or even any way at all that isn't just documentation?

We're looking into OpenMetadata as a tool to help with some of this on a more granular level, but it still doesn't answer it at the scale I'm talking about.",20,14,theDro54,2024-01-22 12:30:27,https://www.reddit.com/r/dataengineering/comments/19cukvp/how_do_you_manage_the_amount_of_data_assets_in/,0,False,False,False,False
199wtkc,How Do You Get Teams to Actually Use Your Data Platform?,"I've been trying to figure out how to get other teams in my company to use our data platform more. So far, I've made some dashboards and told folks about them, but it's not really doing the trick.

I'm wondering what's worked for you. Have you had any luck with things like office hours, better documentation, or just really awesome dashboards? I'm all ears for any tips or tricks you've got.",20,19,joekarlsson,2024-01-18 18:22:17,https://www.reddit.com/r/dataengineering/comments/199wtkc/how_do_you_get_teams_to_actually_use_your_data/,0,False,False,False,False
197f0xe,Would a software company hire junior data engineers while not having a data team?,"I landed an internship as a data engineer in a small software company, I'm not being heavily supervised due to everyone being so busy all the time and the fact that it's a small team, but I'm mainly learning how to use Airbyte (connect and build custom sources using Python). It's a 3 months full-time internship and I may or may not be offered the position (in their words: ""if I excel as an intern""). I don't feel very hopeful being hired as a data person when there are no senior data engineers in the company... but I really want the position, and I want to prove that I'm capable. I'm trying to explore documentations of tools I'm using and might use (Airbyte, dbt, some API's) when I'm not assigned a specific task, but I don't feel like it's enough, any advice?",23,9,Whatinthetabuleh,2024-01-15 17:55:53,https://www.reddit.com/r/dataengineering/comments/197f0xe/would_a_software_company_hire_junior_data/,0,False,False,False,False
18zvkyp,Integration tests - is that a thing in Data Engineering ?,"I come from a backend development background, where we mocked external systems, built and booted up upstream and downstream internal services from code, and tested some cases keeping the service at hand in mind. The best thing was that it was all part of CI/CD, so if anything breaks I get early feedback before my PR is merged.

I know theoretically this can be replicated in DE:  with mocked data sources, in memory spark cluster that might run within a GH action, notebooks that run in sequence to eventually dump some presentation layer data in a DWH schema and call it done. Okay may be I expanded this a bit to end to end automated tests, but still.

Has anyone done something similar, or its just an overkill ?",19,15,hamzah102,2024-01-06 08:38:55,https://www.reddit.com/r/dataengineering/comments/18zvkyp/integration_tests_is_that_a_thing_in_data/,0,False,False,False,False
18woe6i,Why is Airbyte so buggy?,"I just installed Airbyte locally on my machine and it seems to be inconsistent in performance as compared to it's counterpart, airflow. Is this a common thing, or is my PC not good enough for this?",21,18,_areebpasha,2024-01-02 12:33:07,https://www.reddit.com/r/dataengineering/comments/18woe6i/why_is_airbyte_so_buggy/,0,False,False,False,False
18v2vpm,"Is it possible to set up git for versioning schema changes to Postgres, with GitHub Actions for CI/CD ?","This sounds a bit impossible because versioning schema changes sounds like keeping an sql file that builds the database schema from scratch. However, you don’t `alter` a database the same way that you `create` one. So there’s a translation layer there that I’m not sure would be easy to overcome for automation.

Is this sort of pipeline possible? Has it already been built?",20,14,DuckDatum,2023-12-31 07:58:14,https://www.reddit.com/r/dataengineering/comments/18v2vpm/is_it_possible_to_set_up_git_for_versioning/,0,False,False,False,False
1aba1zn,Is SSIS still big in Industry?,"I'm a Data Analyst with focus on Power BI and llittle experience of SSIS. I recently quit my job and started looking for another job in Power BI development.
I got two recruitment calls and both of them are willing to interview me but for SSIS development.

Should I go ahead and give the interview for SSIS or wait for better opportunity in Azure Data Engineering?",19,30,internet_baba,2024-01-26 04:41:06,https://www.reddit.com/r/dataengineering/comments/1aba1zn/is_ssis_still_big_in_industry/,0,False,False,False,False
19bznpt,Is working with Low code ETL tools worth it career wise? (Snaplogic),"I'm a grad student with 2 YOE working with Snaplogic. I'm trying to get some job opportunity with coding, so i can grad with coding experience, I've always believed that it could open doors in future and make me more employable. 
But now a recruiter reached me out to work in a Snaplogic project for a foreign financial institution that would pay me 3 times what i currently earn. 

Is going deeper into such a tool as Snaplogic going to limit my options in future? 

Should i keep focusing on getting a job that requires deeper tech skills?

Is low code a dead end? Will i be stuck in low code forever if i keep working on it?",20,30,meioaesmo,2024-01-21 09:25:44,https://www.reddit.com/r/dataengineering/comments/19bznpt/is_working_with_low_code_etl_tools_worth_it/,0,False,False,False,False
18zdo9c,Is Microsoft Integration Services (SSIS) still being used?,"Hi Engineers, quite a few years have passed and recently I am in need to use SSIS for a personal project. But when I see all the data engineering software landscape it seems like we're completely in a new era (DBT, Kafka, Databricks, Trino, Presto, Spark etc).

**-Which are the current days use cases and if it has long future'?**  
**-Is it becoming a classic outdated tool?**  
**-Which are the tools considered SSIS replacement?**",20,44,Prestigious_Flow_465,2024-01-05 18:23:23,https://www.reddit.com/r/dataengineering/comments/18zdo9c/is_microsoft_integration_services_ssis_still/,0,False,False,False,False
198p5hw,My new org thinks databricks DLT can do everything,"So, I've been using databricks for some time now. I looked into DLT when it first came out and didn't like it. It had a lot of missing features/limitations. 

I've since moved to a new org that wants to implement databricks. The sales reps have already turned them on to DLT and I feel pretty uneasy about it.

When loading data (files in cloud storage) to an append only bronze table, why use DLT? Why not just use autoloader? 

I was watching a demo wherein a SCD2 was able to be created with a simple declaration (from a table stream). You had to tell it the id and the sequence, but what if you had multiple IDs that define uniqueness? And multiple dates to use to figure out the order? Does it support that? 


Anyway, how far has DLT come? I'd like to keep a config that I can loop through to spin up jobs and run in parallel. Is that even compatible with DLT? I really don't want to be manually creating ""pipelines"" for every single table..",20,14,idiotlog,2024-01-17 05:38:09,https://www.reddit.com/r/dataengineering/comments/198p5hw/my_new_org_thinks_databricks_dlt_can_do_everything/,0,False,False,False,False
196bfau,Tips: Started to learn Data Engineering!,"Just started my academic course where i would be learning how to perform ETL from multiple postgresql OLTP's to a postgresql OLAP using talend and AWS cloudstack for data engineering where im expected to do a individual project as my finals. My prof has told us that the coursework would be hectic with a lot of in class lab sessions. Would really appreciate any tips or suggestions that can help me through my academic course and to build a professional career as a data engineer.

Currently I know python as a programming language and intermediate mysql.",19,14,iT0X1Ni,2024-01-14 08:40:09,https://www.reddit.com/r/dataengineering/comments/196bfau/tips_started_to_learn_data_engineering/,0,False,False,False,False
18vbprm,Rethinking Data Architecture: What's Your Ideal Setup for Standard Many-to-Many relationships model?,"Hello everyone,

I'm currently evaluating different data storage and transformation strategies for a complex data set and would appreciate your insights or recommendations.

### Current Setup and Challenge

1. **Data Collection**: We extract data from sources such as AWS IAM service.  
This data includes entities such as users, groups, and policies, which inherently have many-to-many relationships.
2. **Data Storage**: The extracted data is initially loaded into an S3 bucket in a JSON format.
3. **Data Transformation**: Currently, we perform data transformation in-memory. This process involves handling the complex many-to-many relationships and preparing the data for final storage.
4. **Final Storage**: The transformed data is then loaded into a PostgreSQL database.

Managing these complex relationships is becoming increasingly challenging, especially as our data volume grows (can be 1M+ records for every entity).  
We are considering whether a traditional RDBMS like PostgreSQL is the best approach or if we should pivot to a Data Warehouse solution or even explore other database types like GraphDBs or Lakehouse solutions such as Databricks.

### Data Modeling in PostgreSQL

For PostgreSQL, our model involves tables for users, groups, and policies, along with bridge tables for the many-to-many relationships:

* **Tables**: Users, Groups, Policies.
* **Bridge Tables**: Users\_Groups (connecting users to groups), Groups\_Policies (connecting groups to policies), Users\_Policies (connecting users to policies)

#### Example PostgreSQL Query

A typical query we use to find all policies of a specific user (including those obtained through groups) is:

    SELECT DISTINCT p.policy_name FROM users u 
    LEFT JOIN users_groups ug ON u.user_id = ug.user_id 
    LEFT JOIN groups_policies gp ON ug.group_id = gp.group_id 
    LEFT JOIN users_policies up ON ug.group_id = up.group_id 
    JOIN policies p ON gp.permission_id = p.policy_id OR up.policy_id = p.policy_id WHERE u.user_name = 'XYZ';

### Considering Data Warehouse (e.g., BigQuery)

In contrast, a Data Warehouse approach like BigQuery would involve a denormalized fact table, potentially simplifying queries:

* **Fact Table**: User\_Group\_Policy\_Facts (consolidating user, group, and permission data).
* **Dimension Tables**: Users, Groups, Policies.

#### Example Data Warehouse Query

To find all policy IDs associated with a specific user (both directly and through groups):

    SELECT DISTINCT f.policy_id FROM User_Group_Policy_Facts f 
    WHERE f.user_id = [User ID] OR (
                                    f.group_id IS NOT NULL AND
                                    f.group_id IN (SELECT group_id 
                                                    FROM User_Group_Policy_Facts
                                                    WHERE user_id = [User ID]));

### 

### Seeking Suggestions On

* **Database Selection**: Considering alternatives like Data Warehouses (Snowflake, BigQuery), GraphDBs, or Lakehouse solutions (Databricks) for our data storage and querying needs. Key parameters for our database selection include:
   * Query Performance
   * Pricing Model
   * On-Premise capabilities
   * Scalability
   * Streaming Data Support (not mandatory in our case, but it is a consideration)
   * …
* **Data Transformation**: Exploring more efficient transformation processes, possibly using DBT or transforming data on S3 (with Spark) or directly in the database.
* **Handling Complex Relationships**: Advice on managing many-to-many relationships effectively, especially in a scalable and performant manner.
* **Scalability and Performance**: Best practices or recommendations for architectural changes to improve scalability and query efficiency.  


I'm particularly interested in hearing about experiences with similar data models and the trade-offs you've encountered in different database environments.

Thank you for your time and insights!",20,17,ewenField,2023-12-31 16:49:06,https://www.reddit.com/r/dataengineering/comments/18vbprm/rethinking_data_architecture_whats_your_ideal/,0,False,False,False,False
18uozj4,Tips for new DE,"After a long application and interview process i finally landed a DE position after being a DA on the operations side for 2 years with a mechanical engineering background. I will be starting in the new year and this new company will be much more code heavy and utilize AWS, Docker, and github much more than i currently have used. 

What are some resources i can use to familiarize myself with these new tools since i don’t have a CS background, how does a production environment compare to personal projects, and what are some good questions to ask my manager as i am onboarding to facilitate my transition?

Any other general tips are also appreciated!",19,11,2teknical,2023-12-30 20:32:44,https://www.reddit.com/r/dataengineering/comments/18uozj4/tips_for_new_de/,0,False,False,False,False
18uewuf,What to prep for DE DS&A interviews,"Recently laid off senior DE with 7 YOE. I was most recently working at a big tech company but was not asked DS&A questions during the interview when I got that job (which was 3.5 years at this point). I have some interviews lined up for January, and i’ve been solving Neetcode 150 in preparation. Part of me feels like i’m wasting my time because some of the questions feel very irrelevant to the DE role and I wonder if I’d be better off prepping for other things like SQL, data architecture, big data tools etc. I should mention I am applying to technical DE roles that involve writing distributed pipelines in Beam/Spark. Any tips on if I should continue spending my time on Neetcode or pivot instead?",18,6,mango_sorbet13,2023-12-30 12:46:25,https://www.reddit.com/r/dataengineering/comments/18uewuf/what_to_prep_for_de_dsa_interviews/,0,False,False,False,False
198fqk7,Small data team beginning data modeling,"Hi, I could use some thoughts and advice.

I am at a company with 200 employees and just a few data people. Will add more data people soon. We use Snowflake, planning on using DBT as well.

Currently we are querying a combination of event tables and transactional database tables and have no comprehensive strategy for data modeling. Lots of random views created by various engineers.

* What data modeling approaches have you tried and found most useful? What strikes the right balance of being easiest to maintain and keeping a single source of truth?
* What is your opinion and experience with the Unified Star Schema approach?
* Data Vault?
* Pure Inmon?
* if you use Kimball star schema, how do you keep the dim and fact tables in a useful multi fact warehouse instead of a bunch of disparate data marts?
* Do you have any success stories you can share with me? I want to know what it's ke once data is successfully modeled and cleaned.

Sorry if this isn't the most coherent question. I don't know what I don't know.",17,14,Agreeable_Coast_4859,2024-01-16 22:18:03,https://www.reddit.com/r/dataengineering/comments/198fqk7/small_data_team_beginning_data_modeling/,0,False,False,False,False
1919pch,What problems do you face with the documentation software in your company?,"Most of the companies I have seen use Confluence or Google Docs to document their releases or how to setup codebase, software architecture or how things work in general.  
One of the problems I personally face is that these documentations never get updated or are just too cumbersome to search and read.

What are the problems you face? What software does your company use for this purpose? Would you or your company be willing to pay for software that solves these problems?",18,17,brequinn89,2024-01-08 02:10:09,https://www.reddit.com/r/dataengineering/comments/1919pch/what_problems_do_you_face_with_the_documentation/,0,False,False,False,False
18yo31h,Breaking Down IT Salaries: Job Market Report for Germany and Switzerland!,"Over the past 2 months, we've delved deep into the preferences of jobseekers and salaries in Germany (DE) and Switzerland (CH).

The results of over 6'300 salary data points and 12'500 survey answers are collected in the Transparent IT Job Market Reports. If you are interested in the findings, you can find direct links below (no paywalls, no gatekeeping, just raw PDFs):

[https://static.swissdevjobs.ch/market-reports/IT-Market-Report-2023-SwissDevJobs.pdf](https://static.swissdevjobs.ch/market-reports/IT-Market-Report-2023-SwissDevJobs.pdf)

[https://static.germantechjobs.de/market-reports/IT-Market-Report-2023-GermanTechJobs.pdf](https://static.germantechjobs.de/market-reports/IT-Market-Report-2023-GermanTechJobs.pdf)",17,0,One-Durian2205,2024-01-04 21:12:15,https://www.reddit.com/r/dataengineering/comments/18yo31h/breaking_down_it_salaries_job_market_report_for/,0,False,False,False,False
18um0uk,Informatica(latest cloud) with DataBricks vs Pyspark with DataBricks for modern Data stack,"What i am infering (not sure is ) the difference is with informatica there is a vendor lockin while with spark its not . . is it the main diffeernce .  
But I am not able to believe if its correct guess . . because i am seeing lot of big shot presentation on informatica +databricks [https://www.databricks.com/partners/informatica](https://www.databricks.com/partners/informatica)and lot of investment in informatica to support in modern cloud .So what is the use case of Informatica/talend +Databricks . . vs Pyspark +Databricks

***PLEASE NOTE***  : I am not referring to the old legacy ETL only tool functionality of informatica but comparing the latest clould one with DataBricks .  


Please note that i agree that Old ETL only informatica /talend had significant scalability issue , IMy qn is not wrt the old etl tool when data lake was never a concept .  


My qn is wrt latest  informatica /talend clould inegration with DataBricks which they  marketing themseves as highly scalable for big data . So is the quesion in that context   
 **Informatica(latest cloud) with DataBricks vs Pyspark with DataBricks**    
",17,10,Data5kull,2023-12-30 18:24:54,https://www.reddit.com/r/dataengineering/comments/18um0uk/informaticalatest_cloud_with_databricks_vs/,0,False,False,False,False
18sz7ta,Why can’t I pass context between tasks in Airflow/Dagster?,"The biggest blockade to me learning an orchestration framework is the aversion that they have to “statefulness.”
  
I’m not asking HOW do pass “state” between tasks (I know that it’s XComs in Airflow and IOManagers in Dagster), but WHY does it force us to do it in a roundabout way?
  
Most everything that I’ve ever coded has been written in a pretty stateful, object-oriented manner, so switching to this roundabout, functional approach has been difficult to grasp. What’s the intuition behind this design pattern?",18,25,NFeruch,2023-12-28 17:22:09,https://www.reddit.com/r/dataengineering/comments/18sz7ta/why_cant_i_pass_context_between_tasks_in/,0,False,False,False,False
199dfy5,How can I get into data engineering?,"Hi. I’m a mechanical engineer working currently as a mechanical designer. I do HVAC/plumbing system design and analysis. My company is a consulting firm and everyone here has architectural engineering degrees, so I feel out of place. It’s also a really small office and I don’t like it nor do I feel comfortable here. I’ve been working there for 7 months. It is my first job out of college and I’ve realized I want to do something more concentrated in data/analysis. My favorite classes as an engineering student were linear algebra, differential equations, statistics etc. Are there online certifications/programs out there that will help expand my skills and are worth the money and time so I can get a new job? I’d like to avoid going back to school because I don’t want more debt.",18,10,Serious-Look1597,2024-01-18 01:08:07,https://www.reddit.com/r/dataengineering/comments/199dfy5/how_can_i_get_into_data_engineering/,0,False,False,False,False
1909k7c,"In your opinion, are there different types/classes of data engineers?","It’s understood that data engineering is a subset of software engineering, but as time goes on, it seems there are more distinctions forming around different types?

For example, Starbucks was hiring for a senior data engineer who would focus more on semantics, ontologies, metadata, cataloging, and master data management. It didn’t mention building pipelines, but was expected to have previous experience doing so.

Then you have data engineers who are effectively data-flavored forms of devops, and focus more on platform engineering. 

Likewise, business focused front-end data engineers, who were previously labeled business intelligence engineers, are now commonly referred to as analytics engineers.

Lastly, what about data engineers who aren’t necessarily creating new pipelines, but optimizing existing ones? Or data engineers who work in shops who have opted to buy (Fivetran, ADF, paid Airbyte) instead of build?

Just curious is all.",17,18,Tender_Figs,2024-01-06 20:40:04,https://www.reddit.com/r/dataengineering/comments/1909k7c/in_your_opinion_are_there_different_typesclasses/,0,False,False,False,False
19ekul8,Job Search Struggles,"Anyone else having a real tough time landing a new position in the current market? I have just over 3 yoe, and the best I've gotten is either a deferral (passed interview process but they don't have any new projects to bill me on yet, don't ask) or a denial after final round. Seems the market is oversaturated with all these layoffs, plus companies are able to be very picky right now. They said tech jobs are abundant, but I've been searching for almost a year with no written offer! Luckily I do have a job, but I've been trying to switch companies for quite a while now.",16,20,emjaycu3,2024-01-24 16:19:09,https://www.reddit.com/r/dataengineering/comments/19ekul8/job_search_struggles/,0,False,False,False,False
19djant,ETL pipeline from SQL Server to Postgres,"Im currently working on an ETL pipeline to move tables from SQL Server on a server to Postgres on another server, and then do operations on those tables in Postgres on a daily schedule. Some of those tables contain geospatial data.

I currently implemented the pipeline through python scripts using pandas and sqlalchemy

For tables with spatial data I use geopandas and geosqlalchemy

The process is functional so far but it takes too long to move large tables 

How can I speed up moving tables from SQL Server to Postgres? What kind of tools/ideas would be better to implement?",18,36,bolt_runner,2024-01-23 07:53:08,https://www.reddit.com/r/dataengineering/comments/19djant/etl_pipeline_from_sql_server_to_postgres/,1,False,False,False,False
19au44k,How BI tools query data so efficient,"TL;DR: I'm trying to understand how the hell cross-filtering, drill downs in dashboards that shows large amounts of data are so responisve in Power BI, Tableau etc. From querying data perspective that looks like multiple, computing heavy querries running at the same time.


Hi,
Recently I started to think about BI tools data backend, that means what are they doing to filter and calculate large amounts of data so quickly.

Power BI after data import creates its own dataset, which i guess is pretty optimized but then it is still uploaded to cloud, web app still have to somehow query that dataset. Considering cross filtering and drill downs in big dataset it looks like every click that filters dashboard have to query that big dataset, thats a lot of calculation and I doubt that visuals are somehow cached. My guess was that maybe PBI does some sort of olap cube in backend which calculates every possible measure that dashboard can show during data refresh but i guess that I'm wrong.

On the other hand we have tools like Looker or Tableau which as far as im aware doesnt create own dataset. In my perspective they just directly querying databases, additional laser which does...what? Is it some sort of magic that optimizes data stream so efficiently compared to just sql query or is it some sort of cache under each created dashboard i'm struggling to find answers.

Why it bothers me so much? Currently Im working in startup, db is postges 1.3 TB size of data with about 20 mln new records Daily. To visualize I'm using metabase which query postges directly via pure sql statements. Both are self hosted, postgres is of course tuned and inexed.I tried to do precalculated materialized views to speed up querries but Daily refresh started to be pretty painfull.

Because of those struggles I try to understand how BI tools and maybe move from metabase to Looker but I'm wondering if it will help. Or maybe postgres hit its limits and its time to move to some cloud based data warehouse.

What do you guys think? Does anybody have insight how BI Tools work?

Any opinion will be much appreciated,
Cheers.",17,29,Adisab12,2024-01-19 21:20:59,https://www.reddit.com/r/dataengineering/comments/19au44k/how_bi_tools_query_data_so_efficient/,1,False,False,False,False
193c2jn,Real Time Pipeline Architectures on Cloud,"Data Engineering Streaming Pipeline Architectures from my experiences working in multiple companies leveraging AWS, GCP and open source services. 

A templated approach for building a pipeline, solving two common types of patterns:
- Event Sourcing
- Change Data Capture


Read here: https://www.junaideffendi.com/blog/5-real-time-pipeline-architecture/


Let me know what pipelines have you worked on.",17,6,mjfnd,2024-01-10 16:19:50,https://www.reddit.com/r/dataengineering/comments/193c2jn/real_time_pipeline_architectures_on_cloud/,1,False,False,False,False
19371i1,Overwhelmed + Impostor Syndrome in DE position,"Hi pals 

So, last month I started a position as a data engineer at this big company and it's something really new to me. 

For context, I have about 1 year of experience and at my last job I mostly worked in azure data factory, mantaining an ETL fmk with that + sql server + databricks (a lot of sql, a bit of pyspark). I also did some monitoring reports in powerbi. 

The company is transitioning a lot of erp data into the cloud, and there's a big project involving different countries but at the same time it still relies on outsourcing for developing features for this other platform so there's some silos there and I feel so overwhelmed cause I was given the task to develop a feature but I feel like i have't even grasped the whole architecture and really wasn't aware of the dimension of it all... I'm also working on a language I haven't worked in a while so that's also affecting my impostor syndrome.

It's been a month and I feel like I have made very small progress, any advice on how to deal with this? Any of you been in a similar situation before?",16,12,taromoo,2024-01-10 12:20:58,https://www.reddit.com/r/dataengineering/comments/19371i1/overwhelmed_impostor_syndrome_in_de_position/,0,False,False,False,False
191s99i,Is the data team a data producer or a consumer?,"I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.

I believe we are more consumers than producers of data.

\- You depend on the data the developers write in PostgreSQL.  
\- You depend on the data salespeople input in Salesforce.  
\- You depend on the data accountants add to Xero.

If these people produce low-quality data, there’s almost no chance for you to build high-quality data products.

Trying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.

You need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.

I'm not saying data engineers are not producers for analysts, but I think we are primarily consumers.

What's your opinion?",18,18,ivanovyordan,2024-01-08 18:35:51,https://www.reddit.com/r/dataengineering/comments/191s99i/is_the_data_team_a_data_producer_or_a_consumer/,0,False,False,False,False
19f7eju,How did you get into streaming jobs?,"I’m basically a ETL/dwh dev working in SQL, from here, how can I start working in real time streaming and spark roles?",15,16,Jealous-Bat-7812,2024-01-25 11:39:44,https://www.reddit.com/r/dataengineering/comments/19f7eju/how_did_you_get_into_streaming_jobs/,1,False,False,False,False
1986tw8,Data Engineering Interview,"I nedd help!..So i just got a message from an hiring manager on linkedin who told me to prepare for a 1 hr technical interview(intermediate role) in 2 days after sending my resume. This is my first interview ever and i really dont know how to prepare for this, i looked into the company and i figured they are a data consultant agency which makes the preparation difficult since they will be having a lot of different client using different tools. I really don't want to mess this up, i would appreciate any advice on this..Thank you",16,11,logicdata,2024-01-16 16:23:17,https://www.reddit.com/r/dataengineering/comments/1986tw8/data_engineering_interview/,0,False,False,False,False
197lqqx,Credential Sharing Stories,"So someone I know shared company aws credentials in a zip file (many txt files etc) via email to their colleagues. When they were just meant to share the link of the share point/or what ever it was.  It’s now under investigation and classed as a security breach. 

Is this normal or are some people just stupid?

Have any of you had similar experiences like this before? Do people share code via email?",14,8,Administrative_Ad768,2024-01-15 22:16:07,https://www.reddit.com/r/dataengineering/comments/197lqqx/credential_sharing_stories/,0,False,False,False,False
1957xt1,Feeling like data orchestrators mostly waste compute resources,"Curious what kinda of utilization times folks have across various data orchestrators (airflow, prefect, dagster, etc.). All the big systems require some kind of always-on infrastructure to handle scheduling and provide API access, but in most cases require very little of that capacity most of the time.

We run a hybrid airflow+prefect platform at the office, but our 90% of our workloads run daily and 10% run hourly, with most jobs taking 5-15 minutes to finish. So we have a huge spike at midnight for an hour or so, then a few minutes of work each hour for the rest of the day. Besides, those servers are only responsible to manage compute that's being run on other services (like a k8s cluster), so even when the orchestrator's ""busy"", it has fairly little CPU burden and is mostly just polling external services every few seconds. Net result, our orchestrators use maybe \~5% of their capacity on any given day, with almost all of that being during the first hour of each day. Still costs an arm and a leg (MWAA, Astronomer, Prefect cloud, they all cost a lot even if you're doing very little with them), and some of those components still have to be scaled for peak usage and can't easily auto-scale.

Anyone else have this frustration? Any common solutions?",15,14,archeprototypical2,2024-01-12 22:35:46,https://www.reddit.com/r/dataengineering/comments/1957xt1/feeling_like_data_orchestrators_mostly_waste/,0,False,False,False,False
1932tvf,How's data engineering for a senior storage guy?,"Hi,
I'm a data storage guy (22 years in IT. age 48). I've done some bash and Python scripting. Started playing with Pandas for some data analysis. How easy/tough would it be to get into data engineering? How's the ageism in data engineering? Are they ok with senior folks?",15,17,pritesh_ugrankar,2024-01-10 07:38:39,https://www.reddit.com/r/dataengineering/comments/1932tvf/hows_data_engineering_for_a_senior_storage_guy/,0,False,False,False,False
18za481,"Integrating Airbyte, Prefect & dbt 👇","Dive into a hands-on data engineering project this weekend! This [GitHub repo](https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_prefect_bigquery) showcases the combined powers of Airbyte, Prefect, and dbt in a practical setting.

In under an hour, you can establish a complete data stack ready to manage e-commerce sample data. The project uses BigQuery, but you can adapt it to any other data platform with slight modifications. Just follow the instructions in the README.

This project is designed to be straightforward yet adaptable, perfect for professionals with limited time and those eager to learn.

I look forward to your thoughts and suggestions for improvement!

Disclaimer: I’m part of the Airbyte team, and it’s my personal interest to help fellow engineers experiment and learn

&#x200B;

[Prefect DAG](https://preview.redd.it/58whu832dnac1.png?width=2388&format=png&auto=webp&s=6c8a1dae20c2f3638c6818e9dbdad3e14735f4b6)",15,2,thabarrera,2024-01-05 15:56:29,https://www.reddit.com/r/dataengineering/comments/18za481/integrating_airbyte_prefect_dbt/,0,False,False,False,False
18x6y0i,What is your reverse ETL tool of choice?,"We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?",14,61,axlee,2024-01-03 01:42:00,https://www.reddit.com/r/dataengineering/comments/18x6y0i/what_is_your_reverse_etl_tool_of_choice/,0,False,False,False,False
18v20x7,Data engineering vs software engineering,"I've been working on data engineering role even though my job title is software engineer. 
I have expertise in spark scala , AWS and terraform.
I know my way around spark , spark optimisations (not just writing joins using dataframe), I also have lots of good exposure to AWS services and terraform.

Recently I've been feeling little bored and routine work in my role. I've never worked on web related projects.

And I'm afraid if I keep working in this role , I might not be able to switch to web dev in later stages of my career.

I have a hard time deciding if I want to be in data engineering or move to web dev.
I've been seeing online regarding SDEs trying to switch to data engineering and data science, especially in faang companies 

I'm really confused about which path to take.
Any advice would be immensely appreciated.

I'd want to know which path would have the most growth and scope and innovation.

Which role would be better in the long run and how difficult is to switch from one domain to another.

I've been seeing sde roles paying better than data engineering.


I brought it up with my manager and he said you could not get working with this huge amount of data nor infrastructure anywhere but you can always learn web developement on your own. And data engineering is close to AI ML than sde.

Please Help me understand the pros and cons in terms of pay, growth and value.

Which path would be helpful career and financial wise.

Note: I'm having 2.5 years experience and I feel if I waste anymore time it'll be too late to switch domains as I will no longer be considered entry level. I've been promoted for my work last year and my manager says I'm on the fastest track to my next promotion.",16,21,FreshAnalysis1139,2023-12-31 07:02:23,https://www.reddit.com/r/dataengineering/comments/18v20x7/data_engineering_vs_software_engineering/,0,False,False,False,False
19fld6g,Best Practices for Python Collaboration Between Multiple Data Engineers,"Hi guys,

I was just hired on for my first data engineer role at a public health insurance company.  They want to start using Python for data engineering and I was wondering if anyone had any resources on best practices, or was available to talk, about how to set up a modern environment where multiple individuals can collaborate on the same Python project remotely.  They are using Azure DevOps for source control and Control-M to schedule jobs.  The specifics of what to think about beyond this, I don't know.  I've used github for solo projects here and there, but don't have much experience collaborating on python projects with multiple people.  I know there are things like Docker for package management and CI/CD, but how all these things fit together I don't know.

Thanks for your insights!",13,10,i_am_baldilocks,2024-01-25 22:14:12,https://www.reddit.com/r/dataengineering/comments/19fld6g/best_practices_for_python_collaboration_between/,0,False,False,False,False
19dp67h,Am I romanticizing DE?,"Summary:

* B.S. in Microbiology & Chemistry 
* 10 years of work experience - clinical research, teaching, & sales
* No coding experience 
* Strong soft skills
* Currently working fully remote

Debating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it's positive or negative.  

Am I turning this career path into a ""feel good"" dream or is it feasible? 

Much love. ",14,26,ItsWetInPortland,2024-01-23 14:05:53,https://www.reddit.com/r/dataengineering/comments/19dp67h/am_i_romanticizing_de/,0,False,False,False,False
197ym19,Future of Big Data Systems by Spark creator Matei Zaharia,,15,0,dnulcon,2024-01-16 09:07:25,https://youtu.be/aRk6Lk6L5gA?si=9iKT8jej7jKLdy0K,0,False,False,False,False
194wbtw,Work habits in DE,"Sorry if this is a basic or stupid question, i guess it applies to many software engineering situations (compiling code, deploying, testing, etc) but in DE there's some times where I need to debug, a 10 min spark job on EMR as an example, and i struggle to know what i should do during those 10 minutes. Sure i could ""do something else"" in the meanwhile but i feel the context switching is what makes me feel burnt out at the end of the day. Yet i can't sit there and do nothing since if i have 12 errors, 2 hours of my day is gone. Again, basic question but i wonder what hacks i can implement to improve my efficiency and state.

Do you maybe do related reading? busywork email? (push ups? just kidding.)",14,15,Pretty_Meet2795,2024-01-12 14:27:00,https://www.reddit.com/r/dataengineering/comments/194wbtw/work_habits_in_de/,0,False,False,False,False
19085g7,Data stack for a early startup,"

We’re a small but not new startup. 20 people. Our data needs are growing.

Some context:

We’re already on AWS so doing everything on AWS would make our SOC2 easier than having to also bring big query/looker into audit scope.

We deal with phi/hipaa so permissions is important (we can try to anonymize as best as possible.

It would be nice to avoid a data analyst and have bi tool that’s simple enough people can build their own

Are we going to regret staying on aws?

Are we going to probably want a data analyst?

Any other things to consider?",15,12,grow_lab,2024-01-06 19:40:02,https://www.reddit.com/r/dataengineering/comments/19085g7/data_stack_for_a_early_startup/,1,False,False,False,False
18usv34,Migrating from Big Query To Databricks,"I’ve been in the data space for quite a few years. I’ve recently been tasked to migrate from Big Query to Databricks.

Any gotchas, migration paths, advice etc?

Please no people who are gona explain why their preferred vendor is better. It’s annoying and not relevant",14,31,ThrowRA91010101323,2023-12-30 23:20:07,https://www.reddit.com/r/dataengineering/comments/18usv34/migrating_from_big_query_to_databricks/,0,False,False,False,False
18ujut9,Seeking Feedback for a New PySpark Learning Tool,"Hey DataEngineering community!

Quick heads up—I'm not a storytelling pro and my word game is a bit meh. Got a little assist from ChatGPT for this post. 😅

I'm diving into the data field and noticed something missing – a tool dedicated to PySpark practice in a practical setting. So, I'm brainstorming a new tool and would love your thoughts.

**The Concept:**

*Imagine a website tailored for data engineers, offering concise PySpark case studies from easy to challenging. Here's the twist – users won't practice on the site itself like Kaggle. Instead, they'll get detailed problem statements and corresponding datasets. They'll use their preferred environment to solve the challenges.*

**Key Features:**

**Daily mini-case studies:** Short, focused challenges covering everything from data cleaning to analysis, all curated from easy to mind-bending.

**Corresponding datasets:** Each case study comes with its own unique, messy-as-life dataset, generated on the fly. Get your hands dirty with realistic data, not those squeaky-clean textbook examples.

**Level-up your profile:** Conquer challenges, climb the leaderboard, and become a PySpark sensei! Track your progress, see where you shine, and where you might need a bit more training.

**Stuck? No sweat!** Get hints and tips along the way to guide you through the trickiest parts. But remember, the real reward is figuring it out yourself!

**What I'm Asking:**

I need your insights! Does this sound beneficial? Could it help newcomers like me? What features are essential, and do you see any potential challenges?

I'm eager to hear your thoughts! This project might be a crazy idea from a data newbie, but with your help, I believe it could turn into something truly valuable for our whole community. 

**Tech Talk:**

Looking for advice on the tech side. Thinking of Streamlit, Python, and maybe GPT-3.5 for content.  But keen to hear your take on this idea—what tech stack do you think require to materialize the idea.

P.S. Even if you're newer to PySpark than a baby otter, your voice matters! Share your thoughts, experiences, and suggestions.",14,7,DataWhizJunior,2023-12-30 16:50:28,https://www.reddit.com/r/dataengineering/comments/18ujut9/seeking_feedback_for_a_new_pyspark_learning_tool/,0,False,False,False,False
18tr905,What to know before my first job,"I got hired as a data engineer and will likely start in 2 weeks. I have my theory ok, some personal projects and a certification, but I want to know how can I prepare for the day to day work. Bootcamps and solo projects don't fill much on what skills you will need in a real job I think. For some context, I have 4 yoe as a scientist (geologist).

Thanks!",14,12,sebakjal,2023-12-29 16:39:05,https://www.reddit.com/r/dataengineering/comments/18tr905/what_to_know_before_my_first_job/,0,False,False,False,False
1absxeh,Geoglify is now Open Source,"Geoglify is now open source! Explore tools for understanding the maritime universe in GIS space. A new tech stack, a whole new world for me, geared towards the geospatial maritime universe. Where I'll bring all ideas to life. The code is now available on GitHub. The first version is open for testing. I think everything you need for this exciting journey is there.

[https://github.com/geoglify/geoglify](https://github.com/geoglify/geoglify)",51,5,leoneljdias,2024-01-26 21:05:38,https://i.redd.it/rtk11tk9nuec1.jpeg,0,False,False,False,False
19e4sc5,How to efficiently maintain a current state of dataset from an API when records can be removed?,"Hello,

I’m trying to improve a system that maintains a local copy of a dataset retrieved via API. The dataset can fluctuate by having records added, removed, or changed. It gets synchronized hourly via the results of an ETL job moving API data into the database.

At the moment, the pipeline queries API for the full dataset, normalizes the data, and compares against the database data in memory. Python code getting both the API data and database data in the same place, normalizing the API data, and doing in memory comparisons. Missing records get added to the database. Gone records get removed. Changed records get updated.

I’m thinking it would be better to just truncate the whole dataset every hour and reload all the data, regardless of whether it’s changed or not. This sounds less expensive.

Curious if there’s an even more efficient approach given records can be deleted- we don’t know if the set of records will be the same set after each update.",12,17,DuckDatum,2024-01-24 01:18:45,https://www.reddit.com/r/dataengineering/comments/19e4sc5/how_to_efficiently_maintain_a_current_state_of/,0,False,False,False,False
19ct24n,University Subreddit Data Dashboard,"Github link: [https://github.com/Zzdragon66/university-reddit-data-dashboard](https://github.com/Zzdragon66/university-reddit-data-dashboard)

* Any Suggestions are welcome. If you find this project useful, consider giving it a star on GitHub. This helps me know there's interest and supports the project's visibility.
* GPU on GCP right now is hard to get, so terraform may fail on the project initialization. You may change the docker command in DAG and \`main.tf\` to run the deep learning docker image without nvidia-gpu
* There may still some bugs. I will test and fix them as soon as possible.

# University Reddit Data Dashboard

The University Reddit Data Dashboard provides a comprehensive view of key statistics from the university's subreddit, encompassing both posts and comments over the past week. It features an in-depth analysis of sentiments expressed in these posts, comments, and by the authors themselves, all tracked and evaluated over the same seven-day period.

## Features

The project is entirely hosted on the Google Cloud Platform and is ***horizontal scalable***. The scraping workload is evenly distributed across the computer engines(VM). Data manipulation is done through the Spark cluster(Google dataproc), where by increasing the worker node, the workload will be distributed across and finished more quickly.

## Project Structure

https://preview.redd.it/4t4tagdp2zdc1.jpg?width=1651&format=pjpg&auto=webp&s=12f4e0f6dd1456191d349ed10d2200ca80c5df86

## Examples

The following [dashboard](https://lookerstudio.google.com/reporting/97414aef-54dc-4fc8-8bf5-054f0ac75d2c) is generated with following parameters: 1 VM for airflow, 2 VMs for scraping, 1 VM with Nvidia-T4 GPU, Spark cluster(2 worker node 1 manager node), 10 universities in California.

## Example Dashboard

https://preview.redd.it/zd9ykrnq2zdc1.png?width=2886&format=png&auto=webp&s=0d11ccac6550a059fd1f4c10b1433ae327792096

## Example DAG

https://preview.redd.it/qcyo7njr2zdc1.png?width=2932&format=png&auto=webp&s=f2e6d7fbbaebbc5a69ffb0725a86704486bd708e

## Tools

1. Python
   1. PyTorch
   2. Google Cloud Client Library
   3. Huggingface
2. Spark(*Data manipulation*)
3. Apache Airflow(*Data orchestration*)
   1. Dynamic DAG generation
   2. Xcom
   3. Variables
   4. TaskGroup
4. Google Cloud Platform
   1. Computer Engine(*VM & Deep learning*)
   2. Dataproc (*Spark*)
   3. Bigquery (*SQL*)
   4. Cloud Storage (*Data Storage*)
   5. Looker Studio (*Data visualization*)
   6. VPC Network and Firewall Rules
5. Terraform(*Cloud Infrastructure Management*)
6. Docker(*containerization*) and Dockerhub(*Distribute container images*)
7. SQL(*Data Manipulation*)
8. Makefile",13,6,AffectionateEmu8146,2024-01-22 10:57:15,https://www.reddit.com/r/dataengineering/comments/19ct24n/university_subreddit_data_dashboard/,1,False,False,False,False
196pq5t,Should I accept a Senior Data Analyst offer even though I want to pursue data engineering?,"Hi guys,

I'm currently a data analyst with over 2 years of experience who's been trying to break into a data engineering role. However, I've been receiving many data analyst opportunities. 

So, one company I did an interview with have been vague about the role, so I proceeded and did two more interviews and then they told me the position is senior data analyst. Honestly, I really want to leave my current job but also don't want to work as a data analyst anymore.

I'm not sure if I can express that I want to work as a data engineer to them, so they might be open to it.

What do you think?",13,16,DivergentAlien,2024-01-14 20:45:19,https://www.reddit.com/r/dataengineering/comments/196pq5t/should_i_accept_a_senior_data_analyst_offer_even/,0,False,False,False,False
196hliq,How does a Data Warehouse Architect approach a business problem and convert it into technical design,"Hi to all Seniors and experienced one in the industry. 
I have a question how does you face a business problem and convert it into technical design. Like for various software Engineering post they follow norms of system design like scalability, load balancing, reliability. 
What's the norm or basic protocol followed while designing a data warehouse. 

I m ETL developer, fresh into industry. And I want to make it norm, to approach a problem with big picture in subconscious mind. Currently I had to faced a lot of rework due to modification in DWH architecture. But are various approach you follow. Do you have any great blogs!? Right now I m trying to learn from ""Design data intensive applications"" But its more like solving software issues.

Like what common do you face and how you solved it. I'm great into blogs, books more than video. video is very tiring for me. Also I am looking forward enter real time processing market. So from that perspective I want to aware of scenarios normally faced in those situations

Any suggestions will be appreciated by heart.",14,8,asud_w_asud,2024-01-14 14:54:30,https://www.reddit.com/r/dataengineering/comments/196hliq/how_does_a_data_warehouse_architect_approach_a/,1,False,False,False,False
194iz0f,What’s the best approach for replicating all data from an SQL Server instance to a Postgres instance?,"I pay for a hosted SQL Server because a 3rd party makes data available for analytics only through this hosted service. I get SSL access to this database for a cost. The database gets refreshed nightly with my account data, so basically it’s an OLAP that gets updated every 24hrs.

At the same time, I incur egress fees for data leaving this storage. This is unfortunate because analytics often use the same data over and over again. So I am effectively paying to pull out the same data several times over. What a waste of money when the data only changes once every 24hrs.

I want to replicate the data once a data and just run analytics on my replication. However, I have an existing database (Postgres) that I use for similar analytics with data I garner locally. It would be ideal if I could just move the daily SQL Server data into my Postgres instance,

What are the best tools for achieving such replication when you aren’t an admin of the database to be replicated? Is my best bet building a CSV from a SELECT statement for every table, then importing to Postgres? Is there a better way?",14,13,DuckDatum,2024-01-12 01:44:28,https://www.reddit.com/r/dataengineering/comments/194iz0f/whats_the_best_approach_for_replicating_all_data/,1,False,False,False,False
19dzhdr,DE hard-skills (using Pareto principle),"Hello, everyone!

I'd like to move to the West in the near future working now more like in 2d world region. To do this, I think I need to master the appropriate stack (and understand the diff - as-is vs. to-be-done). I tried to make it reading this subreddit (thank you guys!) but want to calibrate it.

My own is (biased ofc + couldn't sort):  
1. Airflow (dagster or similar, airflow just preferable in my country; tool of orchestration, it's clear)  
2. Postgres (relational db, very widespread, also no comment)  
3. Spark (processing large data)  
4. Analytical storage (dunno what to include here since stack varies greatly - on-prem (GP/CH) are used mostly here but not cloud)  
5. Streaming (?)  
.. and other stuff on roughly basic level (docker, superset/grafana, k8s, ELK/EFK, CI/CD etc.)

I think mine may be cringy so could you share your own list and elaborate if possible (from top to bottom in terms of value)?

Follow-up: when engineer can put ""experience with cloud"" on their resume? I have luck to interact with cloud on my current place, but I do simple stuff from first chapters of aws (compatible) docs. I think this is not even nearly enough.

Thank you! If I missed similar discussion here share link please.",13,6,isk14yo,2024-01-23 21:26:10,https://www.reddit.com/r/dataengineering/comments/19dzhdr/de_hardskills_using_pareto_principle/,0,False,False,False,False
19c4l5o,I am moving from Java Backend role (5 YOE) to Data Engineering and I am super nervous about it.,"So I have been interviewing for this company who were looking for data engineers or folks who are eager to learn data engineering skills. Now I always loved data related skills. I used to love studying about RDBMS, NoSQL and other basic areas back in college. But after college I got hired as a Java dev and that's where I stayed for 5 years.

When I saw this job posting recently about the DE role, I could sense that this is something I would love to do. Reached out to HR and they also said that they will help me upskill in DE related technologies if I crack the interviews. Now the interviews were very thorough in coding, system design and deep in Java and Multi-threading but thankfully I cracked it and got the damn job! I still can't believe it lol. I am really excited but also quite nervous because I don't want to fuck this up. I am afraid because I am going in with 0 relevant experience and probably won't be able to make much positive contribution to org anytime soon.

What would you guys suggest to someone who's just getting started in this area? What are some things to keep in mind and how should I approach upskilling myself?",12,8,ShaliniMalhotra9512,2024-01-21 14:30:10,https://www.reddit.com/r/dataengineering/comments/19c4l5o/i_am_moving_from_java_backend_role_5_yoe_to_data/,0,False,False,False,False
19b94kv,Linters and formatters for Python and SQL,"Hi techies!

I started using ruff and sqlfluff for my repos. It’s truly amazing 🤩… I really recommend them. A must. 

I was wondering if you are developing with these plus another similar tools and why. 

In my case, my repos are coded with python for orchestration (airflow), python for processing (spark) and a lot of sql. 

Great to hear your experience. 

Have a great weekend everyone here 😎",12,7,A-Global-Citizen,2024-01-20 10:43:35,https://www.reddit.com/r/dataengineering/comments/19b94kv/linters_and_formatters_for_python_and_sql/,0,False,False,False,False
19832la,PDF Table Extraction,"Hi everyone,  


I have a list of PDFs from which I need to extract table data in automated way. I need one specific table or some important data points from that table. PDFs are from different sources, so the table structures are different from one another. I also need to locate the table in PDF because they appear in different pages every year. I was wondering what would be the most robust way of trying to extract the tables in this case?  


Things I have experimented:  


1. 3rd party Python packages (pdfplumber, tabula): results were not good enough, these packages couldn't extract tables neatly in consistent manner. They were dividing values/labels into chunks and etc.
2. openAI gpt-4 chat completions endpoint: very much inconsistent. It is difficult both to locate table in the PDF and extract table or specific data points.
3. openAI gpt-4 vision API endpoint: I take snapshots of PDF pages and try to extract data using vision endpoint, but because the resolution is not high it makes mistakes.  


I need as much Automation as possible for this task. That's why I am even trying to locate the table in PDF in automated way. Do any of you have experience with similar task? Does it even make sense to make an effort on this? If so, what would be the most optimal solution?  


Sample PDF table which I am trying to extract (let's say I need Total revenue & expense for 2023):  


https://preview.redd.it/iqa9fdmw1tcc1.png?width=1480&format=png&auto=webp&s=6f18977d4c2a77971b4a62970887c4de3971aac7

&#x200B;",12,23,Traditional_Cod_9001,2024-01-16 13:37:46,https://www.reddit.com/r/dataengineering/comments/19832la/pdf_table_extraction/,0,False,False,False,False
196zpve,"From a Data Warehouse perspective, what does a ‘single customer view’ mean to you?","Given the business wants to see a ‘single customer view’, what would this mean to you? Would your mind be drawn to the technical OBT as a solution, or do you think of it more abstractly as a collection of tables and views which together provide a holistic view of the customer?",12,23,nydasco,2024-01-15 04:19:25,https://www.reddit.com/r/dataengineering/comments/196zpve/from_a_data_warehouse_perspective_what_does_a/,0,False,False,False,False
196ta8z,Taking the right steps to becoming a Data Engineer,"This is my first post to this community and I've looked into some questions to see if I can find my answer. Although some posts gave me an idea, I'm still unsure how to take the right approach.

I've completed my bachelors in CS and now doing my masters in data science. Over the years I've familiarised myself with basic languages like Java, C++, Python, SQL and frameworks like MEVN. Since these were required to be learned for my academic purposes, I've never made the time to actually learn anything much related to Data Engineering until I started my Data Science. Over the past few months, I've familiarised myself with concepts like ETL Pipelines, Data Warehouse and Data Lakes and all that from IBM documentation and other resources. Then I went onto my go-to learning hub i.e YouTube to learn more about them. But the hands-on approach was missing. Then I went onto Udemy to buy [this course](https://www.udemy.com/course/aws-data-engineer/learn/lecture/40393098?start=15#content) but now I'm skeptical of I'm taking the right approach or not.

- Should I start with something else? 
- How should I teach myself from the practical standpoint because I tend to learn better through working with and on something instead of just reading notes.
- Where do I find resources that can help me to learn it better for getting into jobs with adequate knowledge?

I've been fascinated by the work of Data Engineers when they explain the high level stuff, but I've yet to experience that unless I take the right approach.",11,8,Rafsan1720,2024-01-14 23:11:53,https://www.reddit.com/r/dataengineering/comments/196ta8z/taking_the_right_steps_to_becoming_a_data_engineer/,0,False,False,False,False
195txyv,Using Databricks for Data Science/ML and Snowflake for Data Warehousing,"I've noticed people say on some old Reddit posts that certain companies use Databricks for their Bronze and Silver data layers, and then transfer this data into Snowflake for the Gold layer.

In such scenarios, as Data Engineers, we often need to reconnect to Snowflake from Databricks to retrieve data—sometimes a significant amount, depending on the table sizes and number of tables. This step is crucial when we have substantial data from sources not integrated into the data warehouse, as it allows us to enrich this data with warehouse data to create specialized datasets for data scientists' ML models.

Considering the use of both platforms, wouldn't it be more logical to fully establish the data warehouse in Snowflake and only transfer data into Databricks when necessary for creating these specialized, enriched datasets for data science and ML models?

I’m not familiar with the cost implications of these options, but I assume the latter approach might be more practical and efficient, especially for companies whose data warehouse teams have limited proficiency in Python.",12,23,khaili109,2024-01-13 18:02:48,https://www.reddit.com/r/dataengineering/comments/195txyv/using_databricks_for_data_scienceml_and_snowflake/,0,False,False,False,False
194gh3f,Data Warehouses vs Data Lakes,,11,5,danipudani,2024-01-11 23:50:49,https://youtu.be/xbtK43WlkMs?si=RR_Kn6e_4DuIbf9u,0,False,False,False,False
193eced,Why should I use Great Expectations if I already have tests in DBT?,"I hear Great Expectations (GX) banded about a lot on forum posts and from Medium articles etc.
 

I understand that it's a data quality tool, but so far I don't really understand why I would use it over just writing tests for my DBT models? I think maybe some of the examples I've seen are just too trivial to do GX justice?


To give you an idea of my current ""data engineering"" role (because as we know it's a vast space) I'm using Meltano to connect a bunch of app databases and third party sources to a warehouse then using DBT to create models for either analytics or dashboards. More of the time I'm doing backend and infra/kubernetes work.


I'd love to hear your thoughts and use cases!",10,9,trhyst,2024-01-10 17:52:19,https://www.reddit.com/r/dataengineering/comments/193eced/why_should_i_use_great_expectations_if_i_already/,0,False,False,False,False
1903dvu,Google's Professional Data Engineer Certification exam,"Hello! I'm going to take the exam soon. Any recommendations? Will getting the Udemy exam samples still worth it? Or are there other materials i should use?

Thank you!",11,17,No_Ice_5035,2024-01-06 16:15:34,https://www.reddit.com/r/dataengineering/comments/1903dvu/googles_professional_data_engineer_certification/,0,False,False,False,False
18zckja,Job Security,"Catch up to speed: Summer of 2023 Friend of mine told me to look into DE work as a career switch since I am already doing excel monkey work as an DA. Next 6 months I teach my self SQL, beginner Python and pass 2 Microsoft azure DE certs.

Currently applying to DE jobs and most of the jobs I am seeing seem to be like from consulting/contracting gigs. Are these legit in terms of job security? 

I would be leaving my current industry which is defense (think Lockheed or Boeing)  which I guess felt so “secure” because of all the govt fund we receive.

TLDR: does anybody here work DE for a consulting/contracting gig, how’s the job security? 

Bonus question : if you don’t work in consulting/contracting, what industry are you in and how’s the job security? Thanks",11,15,PoloParachutes,2024-01-05 17:38:09,https://www.reddit.com/r/dataengineering/comments/18zckja/job_security/,0,False,False,False,False
18ye6ve,Video: How to Setup a Nessie/Dremio Lakehouse on your Laptop in under 10 Minutes,,13,1,AMDataLake,2024-01-04 14:16:21,https://v.redd.it/w1z2r1i4mfac1,0,False,False,False,False
18x3ygn,Python over Stored Procedure in DE?,"I`m used to work with SP whenever I am etl`ing from database to database. I only use python when I need to work with csv, an pai, Json.

But I think I rear about using python and learning python (for begginers mainly) too much.

Does It makes Sense to work with python instead of PS on a database to database ETL? We are talking about stg to dw workloads for example. Is ir fazer? Secure? Or anything that advocates pro python?",12,23,DesperateBus362,2024-01-02 23:32:15,https://www.reddit.com/r/dataengineering/comments/18x3ygn/python_over_stored_procedure_in_de/,0,False,False,False,False
19ezf7p,Where does Prefect stand in terms of maturity?,"With Prefect self hosted, would you put production pipelines on it? How about if they were mission critical, would you trust Prefect with the job?

Would you invest the effort into making your prefect workers highly scalable compute monsters, or just stick with Airflow?

If not, what usually indicates that a platform is ready for this kind of workload?",12,3,DuckDatum,2024-01-25 03:10:24,https://www.reddit.com/r/dataengineering/comments/19ezf7p/where_does_prefect_stand_in_terms_of_maturity/,1,False,False,False,False
19ef60x,Mac book pro or gaming laptop for programming ?,Hey guys ! Just want your opinion on which one to buy. I work as both a data scientist as well as a data engineer and develop deep learning models as well host databases. Which laptop would you guys recommend. Price is not much of an issue but your opinions would help a lot 🙏🏻.,11,56,Automatic_Will_5137,2024-01-24 11:40:44,https://www.reddit.com/r/dataengineering/comments/19ef60x/mac_book_pro_or_gaming_laptop_for_programming/,0,False,False,False,False
19ckixd,need to implement code standards and do code reviews,"I am looking to learn to implement coding standards and eventually do code reviews. This is something new in the organization.

I don’t know where to begin. I need help in getting started and eventually understand the optimal/gold standard to reach. I understand reaching the optimal level would be a journey but want to have that in mind.

Can you pls guide on books/blogs anything on how to begin and what all happens in this ?",12,7,educationruinedme1,2024-01-22 02:08:44,https://www.reddit.com/r/dataengineering/comments/19ckixd/need_to_implement_code_standards_and_do_code/,1,False,False,False,False
19atnb7,Friday thread: what's your best I-did-the-analysis-and-then-management-ignored-it story?,"I'll start: I was once consulting for a company trying to land a multimillion dollar contract. The contract hinged on the company being able to save their customer a certain amount of money, based on projections and analysis from the customer's internal operational data.

When I did the analysis, the projected savings came up short, but my client was desperate to land the contract, so they pulled in a different analyst (somebody working full-time for the company.) His projections used slightly different methods and assumptions, but they still came out pretty close to mine.

So the CEO made the poor guy work overnight to do the analysis again, from scratch. This time, the numbers hit the target for projected savings, so that's what went in the pitch slides for the customer.

The crazy thing was that the contract was written so that my client would only get paid if they \*actually\* saved the customer money, so fudging things up front would only land the sale---it was still possible for the company to make no profit (or even lose money) on the deal.",12,7,abegong,2024-01-19 21:01:08,https://www.reddit.com/r/dataengineering/comments/19atnb7/friday_thread_whats_your_best/,0,False,False,False,False
19akgnq,BigQuery Column level lineage using ZetaSQL,"A long time ago, I built a BigQuery SQL parser for column lineage using ZetaSQL... and it's only fair to make it available and ready to be used:

[https://github.com/borjavb/bq-lineage-tool](https://github.com/borjavb/bq-lineage-tool)

It pretty much covers the whole BigQuery syntax, and tested over more than 5000 SQL queries:

* It's schema aware. This means that a query like SELECT \* FROM table will generate a DAG with all the output columns of table, and not just a single node with a \* symbol.
* It prunes unused columns. This means that for a query like WITH base AS (SELECT \* FROM table) SELECT aColumn FROM base  the output DAG will only contain the column aColumnand not the whole input table.
* UNNEST-based joins, Structs, CTEs,  analytical functions...

Excuses first: I'm not super proud of the code and some technical/data modelling decisions, but, oh well, life.

Hope you can find it useful!",11,0,borjavb,2024-01-19 14:34:41,https://www.reddit.com/r/dataengineering/comments/19akgnq/bigquery_column_level_lineage_using_zetasql/,1,False,False,False,False
19aag3y,Interview prep with career gap,"I was laid off along with my entire team exactly a year ago while I was 9 months pregnant. I didn’t get the chance to study and do interviews with a newborn. Now i have a 11 month career gap in my resume. I have a total of 8.5 years of work exp mainly in AWS, snowflake, airflow. Also have done couple of years in the BI Domain with looker/ Quicksight. 
To those who are actively interviewing, how is the job market for experienced candidates? I am doing few Leetcode problems everyday but haven’t started applying yet.",10,2,trfgjd,2024-01-19 04:24:06,https://www.reddit.com/r/dataengineering/comments/19aag3y/interview_prep_with_career_gap/,0,False,False,False,False
199t56b,Addressing lack of cloud experience,"Hi everyone, 
Back again with another post. I recently had a bad interview experience where lack of cloud experience went against me. The fact that I had recently passed the google cloud professional data engineer exam didn't seem to have an impact, the recruiter said they needed someone with on hands experience on cloud. I am just tired of getting constantly rejected, it seems like every recruiter has a check list they are just trying to check everything off from. What can i do? Open to criticism.",11,18,afnan_shahid92,2024-01-18 15:48:35,https://www.reddit.com/r/dataengineering/comments/199t56b/addressing_lack_of_cloud_experience/,0,False,False,False,False
1971hmc,Interview prep help - Python,"Hello everyone,  
After browsing through the community posts, it seems like [neetcode.io](https://neetcode.io) is highly recommended for practising Python for Data Engineering (DE) interviews. I work as a Data Engineer at a small startup, and haven't studied for or practised LeetCode or Data Structures and Algorithms (DSA). Hence, I'm reaching out for guidance.  
In the [neetcode.io](https://neetcode.io) platform, the practice section allows you group questions by topic. I assume that some of these topics are more geared towards Software Development Engineer (SDE) interviews, and not all may be equally relevant to DE interviews.  
Could you share your priority list on which topics I should prioritise based on their relevance for DE interviews?  
A bit more context: My interview prep is not MAANG specific, the goal is to get good at basic problem solving that comes up in Python rounds.

  
The topics are:

1. Arrays and Hashing
2. Two Pointers
3. Sliding Window
4. Stack
5. Binary Search
6. Linked List
7. Trees
8. Tries
9. Heap / Priority Queue
10. Backtracking
11. Graphs
12. Advanced Graphs
13. 1-D dynamic programming
14. 2-D dynamic programming
15. Greedy
16. Intervals
17. Math & Geometry
18. Bit Manipulation

Thank you for the help!!",12,6,table_data,2024-01-15 05:55:32,https://www.reddit.com/r/dataengineering/comments/1971hmc/interview_prep_help_python/,0,False,False,False,False
1922n89,DuckDB as a data warehouse for multiple users?,"Have heard great things and wanted to give it a shot. Currently have a Postgres database handling data aggregations and ETL. I wanted to pick a separate technology for the warehouse so that down the road migrating to something like Snowflake would be less painful (no shortcuts, lazy coding, etc) and to keep end users from accidentally stalling out the resources of the ETL pipelines. Snowflake and anything similar would be overkill for my current data volume, and trying to avoid cloud for simplifying security environment.

I was curious how DuckDB could work in this scenario with multiple end users. The data in Postgres will be properly transformed into gold level and star schema before landing in the warehouse. I’m really wanting the warehouse to be a ready to consume analytics layer rather than a mess of transformations and staging tables (already using DBT in ETL pipelines).

Would I surface a common .db file on a local network for end users with read only access? Or is there another configuration to consider/another tool completely that would be better suited for the job.

Edit: thanks for all the replies. It seems like local data warehouse solutions meant for multiple users is in short supply. May use the Postgres columnar extension instead and use DuckDb as a backbone for superduperdb which seems more in spirit with the “embeddedness”",11,27,minormisgnomer,2024-01-09 01:46:42,https://www.reddit.com/r/dataengineering/comments/1922n89/duckdb_as_a_data_warehouse_for_multiple_users/,0,False,False,False,False
18xiu4l,Fastest Way to Read Excel in Python,,12,0,be_haki,2024-01-03 12:59:15,https://hakibenita.com/fast-excel-python,0,False,False,False,False
18vr5ja,Data Reconciliation in PySpark SQL,"Hello everyone. Just wanted some suggestions on the approach of how to do a data reconciliation between two tables.

Consider two tables as A and B. The table A contains 300 million records and table B also contains close to 300 million records. Now I want to do a data reconciliation in such a manner that it would find an exact match, a partial match and a no match from table A to B and vice versa. 

Since the data volume is so huge, I'm currently finding delta of A between yesterday vs today's record and then reconciling it with Table B (and vice versa). But I want to do a complete Reconciliation on a daily basis and for that I'm looking for a plan over here. I've a buffer time of 8-9 hours for completion of the job.

Any suggestions would be appreciated!!

Update: logic for exact match, partial match and no match.

So when it comes to comparison logic on high level, for an IP, if source, destination,port, and protocols matches exactly then that is an exact match. The logic of partial match is if any one of them does not matches keeping other fields matching then it's a partial match. And if none of them is matching then it's no match.
",11,17,AdQueasy6234,2024-01-01 06:54:26,https://www.reddit.com/r/dataengineering/comments/18vr5ja/data_reconciliation_in_pyspark_sql/,0,False,False,False,False
18tw6um,How to best sync files between Sharepoint and s3,"I am trying to let people upload excel files into Sharepoint that will be put into s3 and later into redshift. However having some difficulty. A few questions. 

&#x200B;

Do I want to use Lists or Libraries? 

How do I handle the creation, updates, and deletes? 

What happens if someone renames a file? Does a new file get created in s3 alongside the old one or does it actually replace it (seems unlikely?)

&#x200B;

Im basically thinking of creating 3 separate flows one for creates, one fore modifies, one for deletes. But Im not even sure if all 3 of those cases are supported. Any advice from someone who has done a similar thing would be very appreciated!",12,8,third_dude,2023-12-29 20:11:31,https://www.reddit.com/r/dataengineering/comments/18tw6um/how_to_best_sync_files_between_sharepoint_and_s3/,1,False,False,False,False
19f9s1r,"Why are foreign keys ""facts"" rather than ""dims""?","Filtering, grouping etc by foreign key is extremely common, which really suggests to me that they make more sense as dimensions for a particular object rather than 'facts', which generally have unique, high-info (a la shanon) data - that don't refer to other objects or tables.

Is there any good reason that in star/snowflake data warehouses foreign keys get put into fact tables instead? It seems illogical - but I'm probably just missing some crucial intuition here.",11,20,PangeanPrawn,2024-01-25 13:50:28,https://www.reddit.com/r/dataengineering/comments/19f9s1r/why_are_foreign_keys_facts_rather_than_dims/,0,False,False,False,False
19aui8e,When to unit test and when not to?,"I've read the main posts about unit testing on r/dataengineering. However, I am still confused about it.

How do I judge whether something requires a unit test or can be skipped?

The following is my train of thought and I can be completely off but yes trying to learn it more thru reddit. 

e.g I am curating a dataset with simple pandas transformations:

1. I read the dataset from the DL and rename the columns in a function in python e.g *read\_blabla\_and\_rename():* should i mock the data and test the renames? (i do it, but i find it silly)
2. Creating a function to add a fixed column *\_add\_hi\_column()* `df[""hi""] = ""bye""` => do I test this? because i see no purpose.
3. function to replace values where X and Y or Z is applicable => i surely test this because it contains actual logic that can go wrong.
4. Do i test a function that is just doing a left/inner merge?

....

I am also trying to follow a *as small as possible function approach,* it might be highly seen from a SWE POV but is it also a good thing to do for data cleansing/transformations functions?

&#x200B;

Any additional insights are highly appreciated :) 

&#x200B;",9,10,Jazzlike-Change8493,2024-01-19 21:37:16,https://www.reddit.com/r/dataengineering/comments/19aui8e/when_to_unit_test_and_when_not_to/,0,False,False,False,False
19aee11,"Switching from DE to management ( PO, PM, SM ...)","Have you ever thought to change from Data engineering to management ? 

I'm planning to make this move after 3 years in data engineering working with Databricks , Onprem tools and only on Azure cloud , in 2021 and 2022 I was getting way more offers but in 2023 it was very tough for me to have offers / interviews due to market situation. 

Do you think it's a good or bad move considering I invested a big amount of time learning DE? ",11,15,TProfessional,2024-01-19 08:27:01,https://www.reddit.com/r/dataengineering/comments/19aee11/switching_from_de_to_management_po_pm_sm/,1,False,False,False,False
1974haj,Netflix Creates Incremental Processing Solution Using Maestro and Apache Iceberg,,10,0,rgancarz,2024-01-15 09:07:49,https://www.infoq.com/news/2024/01/netflix-incremental-processing/,0,False,False,False,False
196ms9c,Need help in Spark streaming to address to delays when processing large batches,"Hello everyone,

I need help with Spark streaming in Databricks. I'm reading data from EventHub and writing it to a Delta Lake using the medallion architecture in two steps:

* EventHub to Bronze Delta table.
* Bronze to Silver Delta table (the final table).

The EventHub to Bronze part is working fine. However, when reading from Bronze to Silver (Delta to Delta), I'm facing issues. I'm using 1-second intervals to process batches, and sometimes I get a massive number of records in one batch (100,000 to 200,000), causing out-of-memory errors and delays.

I've tried adjusting parameters like maxRecordsPerTrigger, maxFilesPerTrigger, maxbytespertrigger  
, and Spark configurations like spark.databricks.delta.autoOptimize.optimizeWrite  
, spark.databricks.delta.properties.defaults.autoOptimize.autoCompact  
, and spark.sql.optimizer.dynamicPartitionPruning  
, along with enabling backpressure in streaming, but nothing seems to be working.

If I can limit the incoming records to around 15,000-20,000, it should solve the problem. Here are my questions:

1. Is there any other solution to this issue?
2. Is the approach of going from EventHub to Bronze and then Bronze to Silver the right one?
3. If I were to replace this flow with stream analytics, would it support complex transformations?  


attaching screeshots of code of reading from eventhub and writing the data to delta lake, also adding screenshot of  capturing the unexpected  data spike while transferring from Bronze to Silver, especially when there was no corresponding spike in EventHub data at that time 

please note: trigger\_time is 1 second

https://preview.redd.it/2ltlqx6hagcc1.png?width=868&format=png&auto=webp&s=0da2f9b413f7fa0689714e21d80fde30d5b39258

https://preview.redd.it/xiow507hagcc1.png?width=1113&format=png&auto=webp&s=a154da0f2546d2030e936eece7ca690f9f281cfc

https://preview.redd.it/yv0yg27hagcc1.png?width=1204&format=png&auto=webp&s=c475dd3ad360e569488b85d204f5d93865dedc9a",10,11,HousingStriking3770,2024-01-14 18:40:52,https://www.reddit.com/r/dataengineering/comments/196ms9c/need_help_in_spark_streaming_to_address_to_delays/,0,False,False,False,False
196hvq8,Newbie here trying to learn,"Hi, I recently started a new job that is not related to data engineering, but there are a lot of things where I can apply ETL things to automate my tasks. I don't have experience as a data engineer but is my goal to switch careers and get a data engineering job eventually.

Anyways, my problem is as follows: Everyday I get a new CSV that is the same table as the day before but with new info. Every day I need to rearrange that table and 1- create a new CSV file with the daily info and 2-  ""append"" it into the monthly cleaned table. 

Those things that take a lot of time manually I can use python to automate the task, and I am in the process of achieving it. But the thing is that I get to a point in which I don't know if I should use stuff like SQLalchemy, psycopg2 or if I should just stick to pandas. I want to understand the thought process of when I should use what. Let me develop. 

For example, once I get the CSV as a DF, I filter it with pandas. But then, I need to modify the values of column F according to the values of column G. In column G I have description and each description corresponds to a group. In column F I need to specify the group. I know I can keep using pandas to also achieve this, but at the same time, this is a clear and plain simple JOIN function. 

I am reluctant to create a, let's say, PostgreSQL Database because I want to eventually be able to share my python scripts as an exe and someone like my teammate can simply execute it and don't need another program like PostgreSQL to successfully run the file. But maybe I should just do it. I don't really know. 

Also I don't understand why should I use sql at all if pandas can cover for the things that I want to achieve. 

Thanks in advance for your advice and if you can point me into maybe some useful learning books I would really appreciate it.",10,28,GFM41,2024-01-14 15:07:06,https://www.reddit.com/r/dataengineering/comments/196hvq8/newbie_here_trying_to_learn/,0,False,False,False,False
192ou1k,How are people orchestrating the T in ELT for their data warehouses?,"This is probably a stupid question to anyone with a deeper background in data engineering, but I'm new and learned dbt first using dbt cloud which makes orchestrating transformation pretty easy. 

If I have a data warehouse and I can't use dbt, how do I go about orchestrating transformation of data after landing it in raw form in the data warehouse? BigQuery seems to have Dataform and I'm guessing the other cloud providers have something similar, but what if I'm just using an on-prem database? Do I just have a bunch of scheduled stored procedures?",10,10,PureOhms,2024-01-09 20:34:53,https://www.reddit.com/r/dataengineering/comments/192ou1k/how_are_people_orchestrating_the_t_in_elt_for/,1,False,False,False,False
1927hdr,(Data) Engineering Managers - What Makes You Stay At A Company Long,"I am currently working with a large insurance company as a consultant where they have adopted data mesh. Almost every application team uses DW + dbt core. All of this is facilitated by a 10 member data platform squad. The guy heading the team is a gun - although an engineering manager engaging constantly with stakeholders, vendors, being on top of the data security game and still finds enough time to deploy some dbt models and setup some python automations. 

&#x200B;

My question is: What makes a person who is clearly able to talk both business and tech stay at a company - the company isn't a super flashy company and tends to keep in the shadows but he has been there for 15 years? Are such pivotal employees managing key business units in large enterprises constantly rewarded with monetary benefits? Do these enterprises regularly catchup with them to ask them ""Are you happy here? If not, what can we do to make it better?"" questions. Although total within their rights, they probably have a understanding how them leaving the company would impact a large enterprise and I guess here building a really strong squad during the tenure is important. 

&#x200B;",11,7,Outrageous_Apple_420,2024-01-09 05:48:16,https://www.reddit.com/r/dataengineering/comments/1927hdr/data_engineering_managers_what_makes_you_stay_at/,1,False,False,False,False
190qv2a,Best method to update SQL table from AWS S3,"Hi,

There is an application which is creating/updating/deleting processed csv files in AWS s3 bucket. I need to design a system which can update a SQL table based on data which is present in the bucket in near real time. This table will act as a source for a PowerBI dashboard.

Foe example - let's say there are 10 csv files for which I have created a RDS table. Now, if the application deletes 2 files, then the data for those files should be removed from the RDS table. If the application overwrites a file, then the data should be updated  and so on.

One of the potential solution I have in my mind is to create a lambda with trigger on the bucket which will copy data from s3 bucket to table. For deleting, I. can execute a delete query using cursor operations. However, this doesn't seems very efficient.

Another solution I was thinking is to setup a glue crawler and create a athena table. But this might not be very cost efficient since it charges per query execution.

I am not a full fledged Data Engineer. Would love to hear other's ideas.

Thanks",10,18,Wayne_dj,2024-01-07 12:12:54,https://www.reddit.com/r/dataengineering/comments/190qv2a/best_method_to_update_sql_table_from_aws_s3/,0,False,False,False,False
18wk4rt,"PyGWalker's Data Painter, a new way to interact with your data in jupyter notebook","[Clean outliers with data painter](https://reddit.com/link/18wk4rt/video/ib92wepmfz9c1/player)

Sometimes, there are some dirty data, like outliers, clusters in data that we want to remove, design a python script to clean them can be difficult especially when they are some complex patterns. With data painter, you can remove those data just within seconds.

&#x200B;

[create new feature in your data](https://reddit.com/link/18wk4rt/video/vuugpw2rfz9c1/player)

Sometimes, we observed some interested patterns or clusters under some metrics, it can be with insight if we can analysis how the cluster distribute in other metrics. Data painter allows you to annotate your data on flight and then you can directly analysis the new feature you create in other metrics.

  
Online tutorial of Data painter in PyGWalker: [https://data-painter-tutorial.pygwalker.kanaries.io/](https://data-painter-tutorial.pygwalker.kanaries.io/)  
PyGWalker's Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)",11,1,Sudden_Beginning_597,2024-01-02 07:58:03,https://www.reddit.com/r/dataengineering/comments/18wk4rt/pygwalkers_data_painter_a_new_way_to_interact/,0,False,False,False,False
19enwgo,Blind75 for data engineers?,"Hello guys, I’m preparing for interviews and want to get your inputs on a leetcode list similar to blind75 for data engineers",10,8,Jealous-Bat-7812,2024-01-24 18:49:01,https://www.reddit.com/r/dataengineering/comments/19enwgo/blind75_for_data_engineers/,0,False,False,False,False
19dtqiu,DBT for On-Prem ETL,"So, to provide some background, I currently run a very small data team in an industry that’s heavily biased towards on-prem. After facing some limitations with SSIS, we’re looking to move to a more modern ETL solution. My issue is we are currently built to run on-prem ETL while modern architecture seems to focus cloud ELT.

My current setup is as follows: 
1. Extract data from a variety of structured SQL servers (most on-prem, some accessed through VPN. All have direct SQL access)
2. Transform that data using SQL into a more usable, de-normalized format
3. Upload the data into our on-prem data warehouse for analytics. 

A few complicating factors:
1. Several of our vendors are moving to the cloud and I’ll need to export the data via API (and one vendor through snowflake)
2. Our largest data table is under 30 million rows, I don’t expect I need anything more complicated than polars/pandas and SQL
3. In the future, we’d like to be able to ingest and process high frequency (~5-15s) data loaded via 1 hour batches. I don’t need streaming yet nor do I want to implement it. 

Tools I’ve considered:
Mage*
Dagster
Airflow
Pandas/Polars*
DuckDB
DBT*
Minio*

*Current favorite for that role


My thought is, if I’m rebuilding this anyway, would it make more sense to move to an ELT architecture with DBT or maintain my ETL architecture with Python/SQL. What would that look like? Load everything raw into parquet files and process from there? Load raw data into intermediate tables on the same server as the data warehouse? If the majority of my data is already structured, should I bother with parquet? I apologize for the deluge of questions, I’m trying to wrap my head around this new world order after spending too long in the comfortable embrace of SSIS. I appreciate your help!",8,12,Lord_Lloydd,2024-01-23 17:30:25,https://www.reddit.com/r/dataengineering/comments/19dtqiu/dbt_for_onprem_etl/,0,False,False,False,False
19c4kmx,Data Engineers in the Cloud - Tell Me About Your Daily Work and Tools,"I'm reaching out for some valuable insights and advice as I embark on a journey to transition from my current Big Data role to a Data Engineering position in the Cloud. I've been a Big Data developer, primarily working with Apache Spark, Hive, and Kafka, handling data ingestion, processing, and storage, all in an on-premises HDP cluster.

While I've had my fair share of bug fixes and minor enhancements, I'm ready to take the plunge into the exciting world of Cloud Data Engineering. I've already obtained my GCP Data Engineering certification and have an upcoming Azure Data Engineering certification scheduled. However, getting interviews for Data Engineering positions has been quite a challenge, and even when I do get the chance, I'm unsure if my current experience is sufficient.

I'm reaching out to this community to learn more about your day-to-day experiences as Data Engineers in the Cloud. I'm particularly interested in knowing:

1. **Project Details**: What kind of projects are you currently working on? Whether it's building data pipelines, setting up data lakes, or any other exciting data-related tasks, I'd love to hear about them.
2. **Tooling and Technologies**: What tools and technologies are you using in your projects? Whether it's cloud platforms, ETL frameworks, data warehouses, or any other technical stack, please share your insights.

By sharing your experiences, you'll not only help me but also others  like me who are eager to make the leap into Cloud Data Engineering. My  goal is to gather this information to better understand the field and  potentially align my current experience with these insights to enhance  my resume and prepare for interviews.",10,14,Consistent_Ad5511,2024-01-21 14:29:31,https://www.reddit.com/r/dataengineering/comments/19c4kmx/data_engineers_in_the_cloud_tell_me_about_your/,0,False,False,False,False
197c7bd,What is your set up for frontend event generation/collection/enrichment?,"Hi there,

At the moment I am trying to find a solution for working with clickstream data:

* My goal is to collect page views, clicks, form submissions along with technical properties and user properties
* The volume is at least 20M events per day
* The final destination is an opensource ClickHouse database (on premise)

Just before the [news](https://snowplow.io/blog/introducing-snowplow-limited-use-license/?utm_campaign=ops.web.OS_license_update&utm_source=linkedin&utm_medium=social) from Snowplow regarding the company ""evolving from a Commercial Open Source Software (hashtag#coss) company into an enterprise software company"", the solution looked like the best option but now I am confused.

Here are my options:

* Buy the Snoplow license. Risk: paying a fortune in future (the way it happens with Mixpanel and Amplitude)
* Try using Rudderstack opensource solution (don't know much about its issues)
* Try using Divolte opensource solution (looks outdated)
* Set up events using GTM and send data from GA4 to BigQuery and then copy it to the ClickHouse. Risk: Google's unpredictable limitations, questionable customer support and possible issues with data transfer to local database
* Write our own solution. Risk: time consuming and costly

What is your set up? Do you know any other alternative? If you see any logical errors in my thoughts, please let me know as well.",9,7,Kooky_Weakness_2629,2024-01-15 16:03:37,https://www.reddit.com/r/dataengineering/comments/197c7bd/what_is_your_set_up_for_frontend_event/,0,False,False,False,False
195trbo,Shoring up resume with best practices/gaining cloud experience,"I am the sole data analyst/engineer for a smallish manufacturing company where I have practically built the whole thing from scratch: ETL from ERP/other sources using SSIS and Python, into a SQL data warehouse, and then into Power BI and SSRS for end users. 

I am starting to look at roles outside the company (for both my professional development as well as some red flags popping up at the office) but I am starting to feel that I am a bit of a cul-de-sac with my experience and don't have the hottest tools and software on my resume. 

I don't have ""formal"" data architecture/engineering knowledge (if that is a thing) so I have been trying to cobble together my knowledge from articles, youtube videos, etc, and have noticed there were several mistakes I made early on that I would have caught with best practices re: data warehouse design.

Furthermore, I have not learned any of the newer ""cutting edge"" cloud tools as I don't think I can justify the cost and the time it would require to rebuild the whole thing in the cloud, nor do we need a lot of the high performance big data tools, even though I feel it would be personally beneficial for my career to get some experience with them.

Can anyone recommend some books or something that would help me brush up on current best practices and also some suggestions how I might be able to get some experience with some of the newer tools? We are mostly Microsoft so I am taking some Udemy courses that cover the DP203 Azure Data Engineer Cert but if there are cheaper open source versions that I might be able to stick into my process somewhere that would be helpful as well.

Thanks for your help!",8,5,Midnight_Old,2024-01-13 17:54:54,https://www.reddit.com/r/dataengineering/comments/195trbo/shoring_up_resume_with_best_practicesgaining/,0,False,False,False,False
1955ps1,Thoughts on Tabular.io?,"The co-creators of Apache Iceberg have their own startup called Tabular.io

Has anyone had a demo or signed up as a customer yet? 

I’m curious what a managed iceberg implementation helps with.",8,10,miqcie,2024-01-12 21:01:49,https://www.reddit.com/r/dataengineering/comments/1955ps1/thoughts_on_tabulario/,0,False,False,False,False
1934jw1,What is the common pattern for technical assessment for data engineer interviews?,"I am transitioning from data analyst to data engineer career. Usually in the technical interview process, we are asked to perform a technical assessment(live or offline) to evaluate our tech skills. 

When I was interviewing for data analyst roles, I was usually given a dataset and the following pattern-
1. a data set and few questions to find out certain patterns or trends. 
2. A dashboard in power bi/tableau to find how I am thinking of the kpis and metrics 
3. Few questions to be done in sql from the dataset. 
4. If live coding, some basic python and sql questions 
5. A PowerPoint deck with key points and to be presented to the interview online. 

Similarly, what’s the pattern of technical assessments for a data engineer? I am mostly applying for roles which focus on ETL/ELT processes, big data processing like spark and Databricks, data warehousing etc. 

Can anyone shed any light from their previous interview experience?",8,12,NeighborhoodCold5339,2024-01-10 09:39:56,https://www.reddit.com/r/dataengineering/comments/1934jw1/what_is_the_common_pattern_for_technical/,0,False,False,False,False
192muug,How do you decide how many resources to give an Apache Spark job?,"Hi everyone!

I'm currently working in a new open source project called DataFlint .([https://github.com/dataflint/spark](https://github.com/dataflint/spark)) to do performance monitoring better in Apache Spark.

I'm working on creating a new feature for monitoring resource allocation ( i.e. how many executors do you need, and how many CPU/Memory each executor needs), so it would be easier to tune your Apache Spark resource allocation.

&#x200B;

So while making this feature I was wondering - how do you usually decide how many resources to give a job? on what metrics do you look? which configs do you play with?

&#x200B;

I released an initial version of this feature and looks like this:

https://preview.redd.it/41qbfiv6qgbc1.png?width=2936&format=png&auto=webp&s=e5e34bf888ecfa9cc7ddd8c8680729a3d3c58f45

it contains a graph of the number of executors, the min/max executors (if using dynamic allocation), which query ran, and some relevant configs.

WDYT?",9,5,menishmueli,2024-01-09 19:16:18,https://www.reddit.com/r/dataengineering/comments/192muug/how_do_you_decide_how_many_resources_to_give_an/,0,False,False,False,False
191snt1,Good datasets for personal learning,"I’m looking to experiment with various big data tooling like Databricks and Microsoft Fabric. What would be good datasets to play with to build out projects with? 

I think Kaggle is likely too small scale. I’d like to be able to flesh out ETLs all the way through to Analytics and ETLs. So looking for large scale freely available data sets that could be interesting to work with. 

Thanks!",9,6,i-kn0w-n0thing,2024-01-08 18:52:31,https://www.reddit.com/r/dataengineering/comments/191snt1/good_datasets_for_personal_learning/,0,False,False,False,False
18xves8,"Agile Development, when also doing support, and team members are siloed","We're talking about switching to Agile. 

My main concerns are:

  * Support (as opposed to planned work) is a huge amount of our time
  * We're all siloed, so do our own things 95% of the time / are not interchangeable cogs at all

 
I brought this up, and what I'm hearing  we're going to estimate a 'support' # of hours per week, then just use the remainder, per person, to assign points/plan. 

Anyone been in a place where they did that / how was it / how does that sound?",8,9,cdigioia,2024-01-03 21:59:43,https://www.reddit.com/r/dataengineering/comments/18xves8/agile_development_when_also_doing_support_and/,0,False,False,False,False
18x908p,How to work with Billions of rows of Time Series Data,"I have a table of aggregated data in Redshift which has +7 billion rows. I can’t aggregate it any further. 

They want me to run it through some python time series library and then through a data science model (which the data scientist will create) but pandas can only handle 5GB of data from my understanding. 

What are my options? The company is on a tight budget so Spark is not something they’re willing to pay for.",10,27,khaili109,2024-01-03 03:18:48,https://www.reddit.com/r/dataengineering/comments/18x908p/how_to_work_with_billions_of_rows_of_time_series/,0,False,False,False,False
18t98ob,Loading a sharded MYSQL database into Snowflake,"I have a bit of a challenge where I am taking a sharded mysql database, combining all like tables and loading them into Snowflake.  We have over 30,000 different database shards with each containing the same 200 tables.  

Currently I have a python script that loops through all shards running a select \* and appending data to 200 different csv files that are then pushed to S3.  I then use a Lambda function and the Copy Into statement to create the 200 final tables within snowflake.  

Is there a better way to do this?  I currently have to run the entire job each morning and do a rip and replace.  ",9,6,themooseCS,2023-12-29 00:27:23,https://www.reddit.com/r/dataengineering/comments/18t98ob/loading_a_sharded_mysql_database_into_snowflake/,0,False,False,False,False
18stkc3,Design patterns for coordinating data from files/API's to S3,"Hey folks,

My team is scoping some new design patterns for coordinating file/data movement to/from SFTP servers and from API's.  These eventually will get consumed by PySpark/Glue and stored as Parquet in our Data Lake, but we are exploring the best ways to coordinate all of this upstream ingestion.  We have several ideas about patterns that will fit, but I wanted to pulse the community to see if anyone has something we haven't considered or ideas that could help us refine our approaches.

Some background

&#x200B;

* Need to integrate with several things like SFTP, API's for downloading files and also API's that send JSON payloads of data.
* Have to connect to a few SASS offerings like Hubspot, but some of these are not API's that will be on a marketplace and will need to be custom integrated.
* In a regulated environment (Healthcare).
* Current setup is Terraform, AWS Glue, S3, RDS

General patterns we are weighing are:  


* Modules in Terraform: We use terraform for IAC on AWS, so modularization of infrastructure using terraform could solve our issues entirely.  However, this requires possibly writing a fair bit of terraform code to coordinate all of this.  Not really an issue, but we'd currently don't have the best practices around deploying and managing Lambdas and other AWS technologies so theres some legwork there to improve our over all patterns.  Not to mention, our general fluency on Terraform isn't amazing right now on the team.
* Framework like Chalice or Zappa: We have been exploring this possibility more broadly than this use case, so possibly deploying an app on one of these frameworks would allow us to handle the deployment of multiple AWS infrastructure items in an easier way than only leveraging Terraform modules.  Additionally, there are adaptors that translate Chalice to Terraform, so we can leverage our current deployment patterns still.
* Other options:  Theres a ton of ""SASS"" type solutions for connectors and file movement.  While we aren't super thrilled to possibly pay for another SASS solution, if its monumentally easier to implement we may consider it.  I know tools like Airbyte have an SFTP connector and some other connectors we might need, but we also have some more complicated API's we'd need to integrate with that these types of SASS solutions wont solve for.

&#x200B;

Thanks everyone! ",10,6,deepeyesmusic,2023-12-28 13:10:14,https://www.reddit.com/r/dataengineering/comments/18stkc3/design_patterns_for_coordinating_data_from/,1,False,False,False,False
19dde12,What do you do with depreciated applications in your code base?,"I recently launched a POC data pipeline and now that it’s seeing production workloads I want to refactor it. It’s like a combination of 3 data pipelines, synchronously working, and I want to turn it into 3 smaller pipelines that run asynchronously.

I figure this will result in three smaller projects that steal code from the POC and depreciation of the POC. So, what is typically done to the POC?

I use git like most.",8,7,DuckDatum,2024-01-23 02:13:46,https://www.reddit.com/r/dataengineering/comments/19dde12/what_do_you_do_with_depreciated_applications_in/,0,False,False,False,False
197e3p2,Automation; new joiners data,"I’ll try to be brief here. 

The current situation: 
If we have new hires, the HR will share the names with the IT department to create emails for them and give them access to OSS app that we’re using and there will be the basic system they will need. ex; ticketing system - HR system - slack - zoom 

The idea: 
We want to automate this process. Maybe integrating OSS with our data warehouse and set a trigger that whenever we have a new hire it will create the account automatically. 
And based on the possibility level the access permission will be different: Admin - viewr - .. etc

Is it possible? if yes, what are the tools that will help me to achieve my goal?",9,12,Fuzzy-Example-7326,2024-01-15 17:19:08,https://www.reddit.com/r/dataengineering/comments/197e3p2/automation_new_joiners_data/,1,False,False,False,False
1968ntq,In dire need of guidance,"I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. 

Aside my data science course work, I have enrolled in DataCamp's data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning 
- SQL Joins
- Introduction to Relational Databases 
- Database Design
- Intermediate Python 

I have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.",8,17,ETKojo,2024-01-14 05:44:38,https://www.reddit.com/r/dataengineering/comments/1968ntq/in_dire_need_of_guidance/,0,False,False,False,False
1956yj7,"Error Handling in Spark and Structured-Streaming, How to Avoid Stream Crashes?","I am working on a Structured-Streaming pipeline and have ran into what seems like a major issue for me which is that there is no great support for error handling. Example: if I have a dataset of rows that I am trying to process and one of those rows fails due to virtually any unexpected reason, the default behavior of Structured-Streaming is that the stream will crash. If I setup my job to restart on failure, and the error is not transient, then the stream will simply be unable to proceed beyond that point until some solution is implemented in the job code to handle it.

In my google-search for solutions to this problem, I have found a lot of very custom solutions to this. One solution is to write custom validation for each row to check for exception cases, and then conditionally handle rows differently depending on the outcome of validation, but this will only help in the cases that we can foresee, and not the unexpected.

Another solution is to handle all transformations inside of a forEachBatch, so that the whole batch transformation can be wrapped in a try/catch, of course this will result in the entire batch failing in the event of a single row failure.

I expect that many others have had to implement pipelines that encounter poison-pill messages and so it seems odd to me that there is no clear solution for this.",8,4,steve_thousand,2024-01-12 21:54:56,https://www.reddit.com/r/dataengineering/comments/1956yj7/error_handling_in_spark_and_structuredstreaming/,1,False,False,False,False
194aua2,PowerBI vs Streamlit(Python) for Interactive Dashboards?,I recently saw a post on this sub[(7) Will you stop using dashboards? : dataengineering (reddit.com)](https://www.reddit.com/r/dataengineering/comments/193xq76/will_you_stop_using_dashboards/?sort=new) where it seems like dashboarding tools are not as useful for pushing back to databases. Are there any tools out there that do help to build dashboards and write back to databases? Any frameworks specific to python?,7,13,_areebpasha,2024-01-11 19:58:41,https://www.reddit.com/r/dataengineering/comments/194aua2/powerbi_vs_streamlitpython_for_interactive/,0,False,False,False,False
192epn6,Data Fabric - cost and viability in a large org with shrinking budget and limited usage of MS stack,"I work as a Director of Data Engineering managing a data warehouse in a large org with many complex data silos and and non MS vendor delivered software solutions. We also support our BI solutions including Cognos, Tableau, and PowerBI.  Our ERP and Warehouse solution was vendor delivered on top of Oracle databases with pre packaged ODI jobs and Cognos models.  We're sunsetting ODI and Cognos and vendor delivered data models in favor of liquibase > Meltano > DBT > Openmetadata > Tableau with custom data models.  Our head of infrastructure is a ""Microsoft can solve everything"" guy and we have an o365 contract with the PowerBI tier just below premium which was rolled out without DE/BI input several years ago. We're now trying to put PowerBI governance in place which is ruffling the feathers of people who got used to the ""wild west"" approach taken by the infrastructure team early on (no limits and no central IT admin or change management on Gateways, workspaces, data sets, sharing, etc) .  The head of infrastructure is now saying we should look into Data Fabric as a solution to our PowerBI issues.  We're dealing with a shrinking budget due to big changes in our market that will likely be permanent.  That was a long way to get to my question which is about cost and vendor lock in with Data Fabric.  My understanding is Data Fabric is 1. Expensive and 2. a holistic solution that would require converting the entire data to BI pipeline to an MS stack to get full value.  Has anyone done a conversion like this?  What was the ballpark cost in a large org? Has anyone tried to move away from this stack and been stuck due to vendor lock in?",9,7,FrebTheRat,2024-01-09 13:28:44,https://www.reddit.com/r/dataengineering/comments/192epn6/data_fabric_cost_and_viability_in_a_large_org/,0,False,False,False,False
192b61u,Is anyone using the DBT semantic layer?,"It seems like DBT is really trying to push and develop the DBT semantic layer. Does anyone here have experience with it? Is it worthwhile using metrics?  
Their google sheets integration looks quite interesting but the ROI on developing this capability is unclear for me.",9,10,casematta,2024-01-09 09:51:55,https://www.reddit.com/r/dataengineering/comments/192b61u/is_anyone_using_the_dbt_semantic_layer/,1,False,False,False,False
191rwwx,How do you make it clear on your resume what area of data engineering you work in?,"Data engineering can mean a lot of things depending on what company you work at. At all of the companies I've worked at, all data engineering disciplines had the title of data engineer.





For example, I see data engineering as falling under the following areas: software engineer - data, data pipeline development, ML engineering, database admin.",8,5,level_126_programmer,2024-01-08 18:21:51,https://www.reddit.com/r/dataengineering/comments/191rwwx/how_do_you_make_it_clear_on_your_resume_what_area/,0,False,False,False,False
18xxzni,Source Control,"What are people currently using for SC of their DBs (Table Defs, Stored Procs etc) looking for some in depth answers as inspiration for a project.",8,7,QuBoyd,2024-01-03 23:43:37,https://www.reddit.com/r/dataengineering/comments/18xxzni/source_control/,1,False,False,False,False
19f1jmu,How do you handle schema changes? How to keep an alert systems to detect schema changes in downstream platform,"We have a datalake, and various source teams ingest data into our S3 datalake. One of the main issues we are dealing with is that our schema contracts are maintained in documents, and these documents are getting outdated over time due to frequent schema changes from the source side. We would like to create a schema management registry and an alerting mechanism that will notify us of any changes in the schema from upstream.We use AWS and python technologies",8,14,Puzzleheaded_1910,2024-01-25 05:04:06,https://www.reddit.com/r/dataengineering/comments/19f1jmu/how_do_you_handle_schema_changes_how_to_keep_an/,1,False,False,False,False
19e88iq,Any way to make an INSERT statement fail if a subquery returns NULL? (Postgres),"I have a table set up with 3 foreign keys. In the case of one of those foreign keys, the lack of a value (NULL) is considered important information.

It works out something like this:

```
CREATE TABLE foo (
    id SERIAL,
    name TEXT UNIQUE
);

CREATE TABLE bar (
    id SERIAL
    name TEXT UNIQUE
);

CREATE TABLE baz (
    id SERIAL
    name TEXT UNIQUE
);

CREATE TABLE foobarbaz (
    id SERIAL
    foo_id FK NOT NULL
    bar_id FK NOT NULL
    baz_id FK
    value INT

    CONSTRAINT uniq_distnct_baz UNIQUE NULL NOT DISTINCT baz_id
);
```

The thing is, does this make it impossible to reliably insert rows using subqueries?

```
INSERT INTO foobarbaz (foo_id, bar_id, baz_id, value)
VALUES (
    (SELECT id FROM foo WHERE name = “pea”),
    (SELECT id FROM bar WHERE name = “nut”)
     (SELECT id FROM baz WHERE name = “fan”),
    100
);
```

In this case, the existence of the subquery for `name = “fan”` on `baz` indicates that the user **knows** a record should exist for this. If the record does not exist, the INSERT should fail. If there was not supposed to be a value there, the INSERT would use NULL in place of that subquery,

However, it’s possible that the condition `name = “fan”` on `baz` doesn’t exist. This would return a NULL value, and that would be acceptable by the tables constraints so long as a similar record doesn’t already exist. 

This makes the INSERT statement inherently risky, as you must first confirm that the record you seek actually exists.

Is there a way to leverage the database to take care of this automatically? Make the INSERT fail if it should. 

Is the only way a VIEW with an INSTEAD OF INSERT trigger?",7,6,DuckDatum,2024-01-24 04:08:57,https://www.reddit.com/r/dataengineering/comments/19e88iq/any_way_to_make_an_insert_statement_fail_if_a/,1,False,False,False,False
19dsy6j,Maybe bombed this interview question? Asked about data validation and accuracy,"I had a phone screen yesterday for a data analytics engineer role. 

I was asked how do I monitor the data pipelines and ensure its accuracy. My response was, I enjoy working with the end user and am really great about getting constant feedback. I said how in my current role, as a Product Engineer, i spend a lot of time with users and going through user data/feedback to determine the success of a feature. 

Now that I'm thinking about it -- they may have been asking me what tools I use.

Earlier, I described a FastAPI poller I built that detected any new data from an AWS EC2 where I dumped everything. Then it took the new data, transformed it in into the ""pretty"" staging structures then updated the appropriate (separate) EC2 tables. In this case, I use pydantic models to ensure that the data is structured correctly. Any issues I can see in the logs. 

Now that time has passed I think they were asking about testing (in dbt) and monitoring tools. 

Is it worth following-up and clarifying?",8,14,No_Egg1537,2024-01-23 16:54:08,https://www.reddit.com/r/dataengineering/comments/19dsy6j/maybe_bombed_this_interview_question_asked_about/,1,False,False,False,False
19cvtuu,Advice on data infrastructure,"Hello! I'm trying to set up a new infrastructure for data in my organization. I'm not a data engineer, but I'm trying to understand the problem and look for the best solutions. I wanted to ask you for some advice:

I need to extract data from a source (using Airbyte), place this data in a warehouse (using Clickhouse), transform this data to generate better visualizations (dbt) and eventually visualize this data (Metabase).

I wanted to know if this makes sense. My focus is open source tools, which I can deploy locally and manage, hence the tools mentioned.

After that, I would also like to be able to perform some actions on this data. For example, using Retool or some tool that allows me to perform actions to insert data into my data. So I don't know exactly how to proceed.

I can connect Retool directly to my warehouse and perform actions with the data there, inserting new rows that represent other information linked to some table (for example, a users table), but I understand that this would not be the best way to proceed.

I know that there are ""Reverse ETLs"", which took the data from the warehouse and placed it elsewhere, for example in Retool itself. But, how (preferably using some tool) can I insert new lines of information into my data using this architecture?

I thought that Reverse ETL could connect to Postgres, replicate the data that I think is relevant from the warehouse, then Retool connects to that Postgres and eventually performs data insertions in that database, then Airbyte ingests the data again, doing rewrite of the old data. That makes sense?

Any information, advice or help would be great!

https://preview.redd.it/qhmeb6g1wzdc1.png?width=938&format=png&auto=webp&s=b497632952d22c447aa79da1dc842c4a967d338f",8,16,Doveliver2,2024-01-22 13:39:04,https://www.reddit.com/r/dataengineering/comments/19cvtuu/advice_on_data_infrastructure/,1,False,False,False,False
19cpb97,The best way to reduce AWS EMR costs.,"Hi, guys,

We believe the best way to reduce costs is to measure them first, so i got three questions.

1. Do u need a tool that provide u realtime(or hourly) AWS EMR costs at the task\_name level ?
2. If its a SaaS tool, are u willing to pay for it ?
3. How much? Assuming monthly bill.

So, whats your anwsers?  :)

&#x200B;

https://preview.redd.it/y6e0pb721ydc1.jpg?width=1200&format=pjpg&auto=webp&s=8e41af35453dcc8f7c682a456328a24d482b4d81",7,23,No_Structure3465,2024-01-22 06:29:33,https://www.reddit.com/r/dataengineering/comments/19cpb97/the_best_way_to_reduce_aws_emr_costs/,0,False,False,False,False
19c3vs0,Build cloud Architecture Diagrams in 1 Minute (This Tool is Crazy Fast!),"&#x200B;

[Diagram as Code](https://i.redd.it/lnjhr11ctsdc1.gif)

>Creating architectural diagrams with traditional graphical tools can be a hassle. Drag-and-drop interfaces are frustrating, achieving perfect alignment is a challenge, and versioning becomes a nightmare. Diagram as Code offers a solution, allowing diagrams to be written with coding precision, versioned like code, and generated effortlessly. This approach simplifies diagramming tasks and brings order to the chaos.

How to create Diagram using code: [Youtube Video](https://www.youtube.com/watch?v=jCy3jUgN3GQ&t=4s)",7,1,BigNo3623,2024-01-21 13:53:37,https://www.reddit.com/r/dataengineering/comments/19c3vs0/build_cloud_architecture_diagrams_in_1_minute/,0,False,False,False,False
19b324v,How are you handling AI Architecture,"I’m posting this here as my data engineering teams are responsible for putting into production the work done by the Data Science team. If this is you, were you also involved in the discussions around the architecture?

Examples: do you support any of the vector databases, or have you settled on a standard that must be used (similar to settling on DynamoDB for all microservices)? Do you use a single database for multiple RAG systems (can you even do that?) or do each have their own db?

Keen to understand where businesses are at on this front.",7,8,nydasco,2024-01-20 04:13:37,https://www.reddit.com/r/dataengineering/comments/19b324v/how_are_you_handling_ai_architecture/,0,False,False,False,False
197cijh,Best resources to get hands-on experience with dimensional modelling and building a data mart?,"Hi,

The title says it all. I am looking for resources to expand my knowledge on dimensional modelling. However, instead of just reading a book, I am actually looking for resources that allow for gaining some hands-on experience.

Thank you in advance!",7,4,DarthDatar-4058,2024-01-15 16:16:01,https://www.reddit.com/r/dataengineering/comments/197cijh/best_resources_to_get_handson_experience_with/,1,False,False,False,False
19512hz,Video: Setting Up Apache Superset on your Laptop with Docker,,8,1,AMDataLake,2024-01-12 17:48:46,https://www.youtube.com/watch?v=604i8vaukZs,1,False,False,False,False
194yo0s,Advice on small organization's ELT implementation?,"We are just starting to plan out the data infrastructure but this is what it looks like so far:

&#x200B;

* **Data sources**: SQL database, exported flat files that are stored in Google Cloud storage, a few APIs
* Python to extract and load data into bigquery 
* Python code with unit tests, logging, modules, alerts, etc containerized with docker and pushed to  an artifact registry 
* The docker image is then deployed by cloud run jobs to run the container on a schedule to start extracting and loading data
* dbt is then used for transformations + all the other goodies it comes with 

The velocity and volume of our data is very minimal and is meant for internal BI/ML use. While I want to keep scalability, maintenance, and best practices in mind, the normal constraints of big data engineering jobs are not the main concern- mostly implementing the most cost-effective, minimal solution that follows best practices. ",7,3,muneriver,2024-01-12 16:09:03,https://www.reddit.com/r/dataengineering/comments/194yo0s/advice_on_small_organizations_elt_implementation/,1,False,False,False,False
191irxm,Core Principles of Scikit Learn - Gael Varoquaux creator of Scikit Learn,,7,0,danipudani,2024-01-08 11:05:32,https://youtu.be/NvqcJrCh5KE?si=mKWWU1biapj7UPmS,1,False,False,False,False
1911c54,Why Probabilistic Linkage is More Accurate than Fuzzy Matching For Data Deduplication,,7,2,RobinL,2024-01-07 20:17:40,https://www.robinlinacre.com/fellegi_sunter_accuracy/,0,False,False,False,False
18w0y5n,Monthly General Discussion - Jan 2024,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",7,30,AutoModerator,2024-01-01 17:00:35,https://www.reddit.com/r/dataengineering/comments/18w0y5n/monthly_general_discussion_jan_2024/,0,False,False,False,True
18slkpq,"Building your Sausage Machine for Data Products 🌭: Less Tech, More Strategy",,6,1,growth_man,2023-12-28 04:59:32,https://moderndata101.substack.com/p/building-your-sausage-machine-for,0,False,False,False,False
19etrv3,Resume Recommendations for Job Seekers,"I am a data engineering manager and I am currently hiring for a data engineer. 

For those out there submitting their resumes for jobs: please understand that you don't have to list out every single thing you've ever done or technology you have used. 

Lately, I've been opening resumes and it's just like a wall of text of technologies used and every little thing you have claimed to do in the past 5 years. It's like you're throwing pasta at a wall hoping something will stick.

Doing this is the fastest way to have me gloss over and click ""no, do not proceed"".

When I open a resume I want to see that you have actually read the job post and have tailored your resume to address those needs. 

I can't have someone submit a resume saying they made vba macros for automating excell workbooks or that you were a security architecture consultant that oversaw the implementation of archer when I'm looking for someone who has worked with python, gitlab, jira, confluence, gcp {composer and with luck datflow and apache beam} (yes this is all mentioned in the job post). I don't need to know you know how to use MATLAB or Tableau to create dashboards. I need to know you know how to write SQL statements beyond ""select * from"" . Know how to write SQL? Awesome! what performance problems have you encountered and how did you optimize that query? I need to know (for my non senior position) that you know how to at least write a quick Python program to consume a csv, add a derived column to it and with luck you know how to load that payload to postgresql or even big query.

My bar is not very high for a non senior position but you have to at least look like you showed a little effort in customizing your resume to fit my needs laid out in the job post. 

Hell, I'll decide to set up an interview with someone even if they just know the basics (python and pandas/polars) as long as I can get a feel, from your resume, that you hit on the top 3 things I hire for; trustworthiness, aptitude and willingness to learn on top of the basics from a tech perspective. 

If you have all that but you don't know how to ""git push orign main"" or how to set up a DAG or shard some postgresql tables or understand how to build out a proper data model or how to implement jinja templating into your data pipeline or build and deploy terraform or even how to debug broken beam code... That's fine. We can teach you, you will learn and eventually you will become a senior. But, you have to give me something to bite my teeth into or I just can't move you forward. Sorry.",5,18,Imaginary-Ad2828,2024-01-24 22:48:01,https://www.reddit.com/r/dataengineering/comments/19etrv3/resume_recommendations_for_job_seekers/,0,False,False,False,False
19dtn2u,How to do design a near real-time data job,"I need to create a python app that will output data e.g. every 5s (ideally should be parametrizable to e.g. 10s or 1 day). Input is realtime data stream from RabbitMQ/Kafka. Every 5s app should look into the queue and take all data between defined start and end time - i.e. every run instance should be idempotent as it is defined by static start/end. Job should have some retrying logic in case something fails. App should run in on-premise Kubernetes cluster.

I thought of 3 possible solutions:

a) custom python app with some scheduler such as Airflow that could schedule the app in cluster every 5s

b) custom python app that would run in an infinite loop as standalone pod in the cluster and every 5s or so would do the job

c) use some existing solution that would solve both issues (data logic and scheduling) for me

The issue with a) is we need a robust scheduler that could handle all the job spawns (each 5s, each 10s etc.)

The issue with b) is there is less transparency in case app crashes and also since its only one app in case it crashes all runs that should run after the crash won't run until the app is fixed (scheduler would schedule all the following runs despite previous fails)

The issue with c) is the only potential solution is Prefect but it has memory leak so when we run it for inifity it will eventually run out of memory.

What would be the best approach to implement this?",6,6,romanzdk,2024-01-23 17:26:33,https://www.reddit.com/r/dataengineering/comments/19dtn2u/how_to_do_design_a_near_realtime_data_job/,1,False,False,False,False
19con24,Doubts on being a Data Engineer,"Hello Guys,
I’m working as a Data Engineer at mid size company. I mostly work on Snowflake, and Tableau. But, I’m trying to convince my manager to assign a ETL project so that I can get some exposure to Aws and Snowpark.
In parallel, I want to expand my expertise as well, so here are my queries: 
Firstly, I want to learn spark, how can I start learning, to learn it quicker?
Secondly, the reason I chose a career in data is because I didn’t enjoy DSA, I’m bad at and no motivation to learn as well. So, is it the end of road for me to achieve a data role at FAANG?
Thirdly, how do you guys manage time after work to do side projects when learning new skills?
Finally, Azure or Aws? I found azure to be too much drag and drop. And, more tool based. What do you guys recommend?",7,5,GulabiGovind,2024-01-22 05:48:13,https://www.reddit.com/r/dataengineering/comments/19con24/doubts_on_being_a_data_engineer/,0,False,False,False,False
19bi0pn,How do you guys prefer to Unit test/automate test ADF pipelines/databricks notebooks?,"Hi guys

I've been googling lots of different unit tests/automated testing for ADF pipelines and databricks notebooks (ran in ADF) and I've got a basic idea lined up, but it's got me curious what other professionals in the industry use for testing and what's generally considered best practice for you guys.",5,6,IG-55,2024-01-20 18:17:49,https://www.reddit.com/r/dataengineering/comments/19bi0pn/how_do_you_guys_prefer_to_unit_testautomate_test/,0,False,False,False,False
1994mwj,Data Pipeline for begunners,"   
I've read a lot about data pipelines, I'd like to learn how to build them but I'm not sure what tools I should use. What do I need to know/learn in order to complete this task successfully?    
All I find is mostly generalized information nothing concrete and specific, could someone with experience help?   
Thanks.",6,10,Mundane-Research9306,2024-01-17 19:02:54,https://www.reddit.com/r/dataengineering/comments/1994mwj/data_pipeline_for_begunners/,0,False,False,False,False
1978bub,DevOps course for Data Engineer,"Hello everyone,

Do you guys know of any DevOps courses on Udemy or Coursera that are suitable for Data Engineers looking to learn how to deploy Data Architectures and set up CI/CD?

&#x200B;",5,3,katoo2706,2024-01-15 13:08:19,https://www.reddit.com/r/dataengineering/comments/1978bub/devops_course_for_data_engineer/,0,False,False,False,False
196nl41,Using LLMs to draw simple insight from tabular data: a discussion,"Hey hey all, data/analytics engineer here,

I got a lenghty topic, hopefully worthy of a lenghty discussion. I'm looking into LLM-s and how they could be used for assisting in understanding reports.

I would like your opinion on the following idea I am exploring.

**Business Problem**

Our company got loads of reports generated, more than humanely possible to read (that's another discussion to have, but many of these are legally required, etc.). It would be really cool that for a given report, there would be a few sentences highlighting things.

A Report's (Reader's) Digest if you will. Things like

* ""It seems that compared to 2023 Q1, in 2023 Q2 our sales in department x has increased by 6.4% percent which is unusually high.""
* ""Our social network reach has been constantly falling on Facebook since 2023 February.""
* ""The average customer rating has been dropping for 5 consecutive days.""

**Understanding limits**

I get that LLM-s work with string data, so i'm guessing this isn't a straightforward thing to do. I'm also thinking that currently LLM-s won't figure out insights on their own without guidance, thus questions should be agreed upon with business. (Altough self-propelled insight finding would be a holy grail.) So with this in mind:

* probably business questions should be agreed upon
* probably business shouldn't ask their questions directly from an LLM agent
* LLMs are not great at understanding tabular data so this is not an obvious path ahead

**Brainstorming ideas**

Architecture-wise, I would be using Databricks (something we actively work with), loading a report in as a delta table, then use a [LLM serving endpoint](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/llm-optimized-model-serving). This is fairly new stuff coming from them, it's still in public preview, but it can leverage LLama-2, keeps the data on databricks side so it seems secure. (GDPR says hello.)

I found of course Pandas AI, that could be useful in combination with databricks somehow? I also found this [fairly recent paper](https://arxiv.org/pdf/2401.04398v1.pdf), which talks about ""Evolving chain for table reasoning"" on L[LamaIndex linkedin page](https://www.linkedin.com/posts/llamaindex_chain-of-table-use-llms-to-understand-activity-7151983658596229120-fOo3?utm_source=share&utm_medium=member_desktop) that proposes a chain-of-table querying that looks interesting. It seems something closer to what LangChain is doing. Some [medium.com](https://medium.com) articles i found: [1](https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476) (this is about documents), [2](https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d) (this is about tabular data)

**A maybe functioning idea**

Okay so let's see if the following makes sense, it is very highlever and very brainstormy:

1. Agree on with business what are the core questions they want to know from a report.
2. A LLama-2 model is setup in Databricks with serving endpoint
   1. EDIT: so probably not Llama-2, but something along this line: [https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d](https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d) or PandasAI working with a databricks-stored llama2 (afaik that is supported)
3. A job runs daily:
   1. A report is ingested some magical data engineering-y way (my fellow data engineers know the pain)
   2. Prompt engineering: do some setup with the LLM agent (no clue yet what could be needed, but it's a good guess that this is required - to prevent hallucations, etc).
   3. Prompts from business are loaded from some config file
   4. Results are stored in a separate delta table with following schema:
      1. Some primary key
      2. Some report identifier
      3. Business unit or stakeholder identifier
      4. Prompt
      5. Answer
      6. Timestamp
   5. A SQL Warehouse endpoint is available and can be queried (by PowerBI for example that gets the latest answers.)

**Problems and limitations**

How do we check if the answers are correct? Monitoring results is very blackbox-y at best. Ideally endusers needs to be trained on the purpose of the Report's Digest is to highlight stuff, but they have to verify it themselves. Getting feedback from endusers can be also complicated

**Conclusion in an ideal world**

Business managers like to look at PowerBI reports, and with this new feature they get an immediate highlight of the day, the top 3 questions they always wanna know about comes with immedaite answers. ""We have an increased count of customer service tickets compared to previous week"". The manager looks at the graph and it seems to verify this statement.

Engineer team walks into the sunset with cool music in background

&#x200B;",6,8,Labanc_,2024-01-14 19:14:50,https://www.reddit.com/r/dataengineering/comments/196nl41/using_llms_to_draw_simple_insight_from_tabular/,0,False,False,False,False
195vi3m,Reading/writing directly to Iceberg table from web app?,"At work we have dozens of use cases where scientists will track experiments and metadata in Excel files and share them through email or Microsoft Teams messages. I don't think that it will be easy to completely 180 their current processes, and because they still feel comfortable interacting with their data in editable tables, I thought that I could systemically upgrade their workflows to simple Streamlit apps with editable dataframes ([https://blog.streamlit.io/editable-dataframes-are-here/](https://blog.streamlit.io/editable-dataframes-are-here/)); with the hopes of:

1. Validating data at the source
2. Creating single source of truth for data
3. Enabling downstream automation and data sharing

Specifically with #3, a goal would be to facilitate downstream automation and data sharing in the companies data lake. But technically I'm not sure how this could/should be structured? My initial idea was to have the backend data source be an Iceberg table and have the application make inserts, updates, and deletes directly to the Iceberg table. This would in theory update the data lake in real-time and prevent the need for any extra complexity with databases, orchestration, etc.

But I've never used Iceberg before so I may be making a lot of assumptions that might not actually work... Would this be a valid use case and are there any technical/data issues with this architecture?

&#x200B;",6,9,mccarthycodes,2024-01-13 19:09:54,https://www.reddit.com/r/dataengineering/comments/195vi3m/readingwriting_directly_to_iceberg_table_from_web/,0,False,False,False,False
1955990,Is Databricks a niche enterprise platform?,"I might be shortsighted about this topic and I wouldn't have any problem in admitting it. However, I've never talked to a DE that has worked with Databricks, ever. I've worked in mid-sized companies and Databricks has never been a topic discussed.  
Most positions I see don't ask for Databricks knowledge or experience, at least in Brazil, where I'm from, or Portugal, where I'm looking some opportunities recently. Looking at their website, it seems that only very large companies use their services. 

From a management point of view, why would you use another platform instead of using the cloud that your company already uses? Wouldn't it be cheaper and easier to negotiate some discounts (like reserved instances) and keep everything in 'one stack'?

I want to emphasize that I'm not saying the Databricks is useless or bad. I only wants to understand what companies use it and why.  
",7,43,Rude_effect_74,2024-01-12 20:42:52,https://www.reddit.com/r/dataengineering/comments/1955990/is_databricks_a_niche_enterprise_platform/,0,False,False,False,False
1945xlx,A Technical Dive into PostgreSQL's replication mechanisms,"I'm the author of this guide to logical replication in Postgres. I break down some of the internal components of Postgres to make CDC easier to understand. I hope you find it informative and learn something new!  


[https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql](https://airbyte.com/blog/a-guide-to-logical-replication-and-cdc-in-postgresql)",7,0,Chemical-Treat6596,2024-01-11 16:37:54,https://www.reddit.com/r/dataengineering/comments/1945xlx/a_technical_dive_into_postgresqls_replication/,1,False,False,False,False
192sgnv,Airflow User Survey Infographic,"I recently helped launch the Airflow 2023 User Survey, and thought some of you may find the results interesting, especially those that use Airflow :)

I even had a cool infographic made. Included a sneak peek below, but for the full thing- [check it out here](https://airflow.apache.org/survey/)

Hope this is useful!

&#x200B;

https://preview.redd.it/hciz2m67whbc1.png?width=3168&format=png&auto=webp&s=9a7db044ca109cd37398102034f0e54d39ae365a

&#x200B;",6,2,BrianaGraceOkyere,2024-01-09 22:59:39,https://www.reddit.com/r/dataengineering/comments/192sgnv/airflow_user_survey_infographic/,1,False,False,False,False
190hyka,The Parquet We Could Have,,5,0,mwlon,2024-01-07 03:02:56,https://graphallthethings.com/posts/the-parquet-we-could-have,0,False,False,False,False
190f5v2,Where to find Good Projects for Github profile in Data Analyst or Analytics.,"I have completed my masters in computer science, Currently aiming to get a full time job as Data Analyst or Data Analytics roll. I am looking forward to build my Git Hub Profile with projects that will be helpful for my interview and showcase the skills required for the respected roles.  
I have started watching some youtube projects but i feel everyone will be doing the same and just copy pasting for their GitHub profile. I am ready to buy courses around $40 to $100 and learn the skills with really good projects.

Thanks in advance if any one helps me with the links or courses for building my Git Hub profile.",6,9,No_Caregiver5020,2024-01-07 00:45:14,https://www.reddit.com/r/dataengineering/comments/190f5v2/where_to_find_good_projects_for_github_profile_in/,0,False,False,False,False
18ydzpl,What is your strategy for reloading large datmarts?,"Let’s say you have a datamart, it’s probably not large per se but it is built from multiple trillion row datasets, so rebuilding it from the start is not something you want to do everyday. The normal behavior of the ETL is that you are refreshing a month of data. You made a calculation change and now you need to backfill that logic for the whole table. 

Im newish to a team and the way the DE’s I work with handle this is to push two versions of the script to prod. One of them handles the one time reload. This seems clunky. 

The way I have handled this in the past is to pass a date parameter from airflow into the script. Then I would use airflows backfill command to run the one time job. The problem being that no one has access to the command line, or will run scripts on our behalf.

Another pattern I have seen is to have a table which lists reload date periods for each table. Workable but then you need some mechanism to update those dates. 

I’m curious what other patterns people use? 

My environment is in AWS and we use Airflow  and spark.",6,5,SeaworthinessDue3355,2024-01-04 14:07:15,https://www.reddit.com/r/dataengineering/comments/18ydzpl/what_is_your_strategy_for_reloading_large_datmarts/,0,False,False,False,False
18wyvdt,New to DE concepts. My company analytics team was working on setting up DWH,"Some of the high level meeting discussed these buzz words.

Spark
Azure
Iceberg
Starrocks

In understand infrastructure is Azure and computational is done by Spark.

What does StarRocks and Iceberg do in this setup. Can some one please explain how all these concepts fit in architecture or pipeline",6,7,rajekum512,2024-01-02 20:07:29,https://www.reddit.com/r/dataengineering/comments/18wyvdt/new_to_de_concepts_my_company_analytics_team_was/,1,False,False,False,False
18vg9oj,Unity Catalog Opinions?,"Anyone using Unity Catalog extensively at their org? Looking for honest reviews on performance, ease of use, and whether the value add is worth having the additional overhead of yet another tool.

I’ve been skeptical of some goals Databricks has claimed in the past to be open and compatible with a variety of open source technologies. However, with the announcement of [Unity Lakehouse Federation](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog) and the [Open Apache Hive Metastore API](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api), I’m starting to see that they are pretty serious about this. 

We’ve got a few Postgres databases that have been used as both ODS and historically as data warehouse but also have a BigQuery instance where we’ve put larger datasets, for reference. Direct query performance of Postgres has been good but we usually find BigQuery lacking. Also have yet to work in Databricks at all and honestly not a huge fan of their transformation framework.",6,13,Express-Comb8675,2023-12-31 20:23:53,https://www.reddit.com/r/dataengineering/comments/18vg9oj/unity_catalog_opinions/,0,False,False,False,False
18uv35j,Data team in identity crisis post-acquisition. How should data engineering work in biotech?,"I've worked in the same data team for the past 5 years, and up until a year ago we were *the* data team for a \~100 person startup; we designed, built, and maintained all data pipelines, databases, and related applications. We were acquired last year, and our company is now at 6,000 people. As part of this acquisition, our data team was transitioned from the business into IT as part of the data platform team. The issue here is that while the platform team is responsible for building the core tools needed to ingest, transform, and store data, our sub team is now *only* responsible for designing and implementing data use cases, *and* completed data products (data pipelines, databases, etc.) are given back to the business to maintain. This is a now also a service that business units need to use their own cost centers to finance...

I see so many issues with this new organization:

1. Our team now technically has no ownership. We're in the platform team in name only, in reality we're end users for the actual platform team. Because everything we build needs to be handed back to the business (wet lab scientists) for maintenance, we don't technically own any pipeline, database, or models either.
2. The business (wet lab scientists) don't want to be data engineers, and no one wants to maintain these pipelines. We've had pushbacks every time we've tried to transfer a completed project back to the business, and the business has elevated the issue up the C-level.
3. The stupidest thing is the financial part, where if any group from the business wants to work with us, they need to use their own cost center to fund us. At this point we're basically internal contractors, and I have no clue why this decision was made. No other groups in the company operate like this, and all I see this doing is putting a wall between the business projects and us. 

I'm at a loss of what to do. I've tried to work in this new mode for the past year, and in that time all of our existing projects dried up and we basically spend the year doing busywork on a single data product. It's honestly embarrassing and demotivating. I no-longer see our team's importance in the company so I can't imagine how an outsider can...

Right now I *think* that a change needs to be made because there are fundamental issues. I think either:

1. Our team fully joins the platform team and is involved in actual infrastructure planning, implementation, and maintenance.
2. Our team provides long term maintenance for all data products we create; if this can't be done as part of IT we should transition back to the business.

I'm interested in any advice here because I only have experience in startups, and maybe this is common in larger companies? I am in a position where I can influence change, but no one internally seems to have a good idea what they should be doing :)",5,8,mccarthycodes,2023-12-31 00:59:17,https://www.reddit.com/r/dataengineering/comments/18uv35j/data_team_in_identity_crisis_postacquisition_how/,0,False,False,False,False
18uffjq,Professional Certificate in Data Engineering Course on EdX,"I’m a near-complete beginner looking to start a career path in data engineering. I say “near” because I took the CS50x course earlier this year, but because of school and some other commitments, I was only able to get to week 8 out of 10. I’m now looking to explore other areas, and I’d like to know if anyone has had any experience with this course, if it’s worth it as a beginner, or if there are better courses I could take. Thank you!",7,8,Mayh_24,2023-12-30 13:16:06,https://www.reddit.com/r/dataengineering/comments/18uffjq/professional_certificate_in_data_engineering/,1,False,False,False,False
19fig4i,Health Informatics/Interop Woes,"Does anyone else working in this area notice that HL7 standards are never ever ever ever adhered to in a meaningful way? Like forget dealing with date formats. I'm talking medications buried in the text section of a non standard component, in a ccda progress note... when there is clearly a medication section that this would fit nicely in...



It's quite stressful knowing that this sort of neglegence can impact extremely vulnerable people.",4,12,Zacho40,2024-01-25 20:06:27,https://www.reddit.com/r/dataengineering/comments/19fig4i/health_informaticsinterop_woes/,0,False,False,False,False
19fcu2s,Performance review of a search engine running on AWS Lambda and S3,,6,0,massus,2024-01-25 16:10:05,https://quickwit.io/blog/quickwit-lambda-search-performance,0,False,False,False,False
19eoqz0,Orchestrating a Trigger based ETL Pipeline in GCP,"We have an ETL pipeline in a GCP and Snowflake technology stack that is currently being refactored from manual into an automated process for ETL along with data validations and executing python based models. The source data is mostly API calls and CSV files. We have the following options to trigger this process in each phase from extraction and load with validations. Any suggestions which could be the best tool to orchestrate/trigger out of the following?

* Google Cloud Functions
* Google Pub/Sub
* Cloud Scheduler
* Snowflake Tasks
* Informatica ICS
* UC4

**Note**: Cloud Composer/Airflow is NOT an option due to compliance concerns.

Thank you in advance!",5,9,JoseyWales10,2024-01-24 19:22:36,https://www.reddit.com/r/dataengineering/comments/19eoqz0/orchestrating_a_trigger_based_etl_pipeline_in_gcp/,1,False,False,False,False
19eh507,Autosys vs Airflow,"I’m trying to introduce Airflow into my teams tech stack and getting a lot of resistance from data architecture as they are of the opinion that our existing Autosys setup can by and large do the same thing as Airflow. 

I disagree for a number of reasons:

- airflow has more dynamic and complex options for triggering jobs 
- airflow has sensors and hooks into other platforms to trigger jobs based on outside variables
- airflow has a better UI for monitoring jobs


Although python is clearly an advantage, that won’t fly in the decisioning. 

What else can I suggest?",5,9,GlasgowGunner,2024-01-24 13:30:58,https://www.reddit.com/r/dataengineering/comments/19eh507/autosys_vs_airflow/,0,False,False,False,False
19eg9yk,"Apache Iceberg Metadata tables, how have you used them?","If you’ve worked with Apache Iceberg, there are metadata tables to assess a tables files, snapshots, partitions, etc.

Have you used them?

What have been the most useful ways to use them for you?",5,0,AMDataLake,2024-01-24 12:45:49,https://www.reddit.com/r/dataengineering/comments/19eg9yk/apache_iceberg_metadata_tables_how_have_you_used/,0,False,False,False,False
19dt9fn,How to model and save these two data source.,"In a manufactoring project I have two sensors:

1. Sensor 1: temperature data sampled at 10Hz continously.
2. Sensor 2: 3-axis accelerometer data sampled at 6kHz in a window of 10s every 10m. In other words, every 10m I have a windows of 10s containing 10\*6k=60000 records. Every record has a timestamp, a value for axis x, y, and z. 60000x4 table.

On sensors 2 data:

The ideas is to perform, at some stage, a ""data engineering"" phase where the ""raw data"" from sensors 2 mentionted before are processed in order to output some informative and less-dimensional data. For instance, letting the inputs be:

* Window 1 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).
* Window 2 of 10s, sampled at 6kHz, every 10m has 60000x4 data (timestamp, x, y, z).
* ...
* Window M: ...

The output would be:

* MxN table/matrix (windows\_id, timestamp\_start\_window, feature1, feature2, ..., feature N-2).

Where N is the number of synthetic features created (e.g. mean x, median y, max z, min z, etc..) plus a timestamp (for instance the start of the window) and the windows ID and M is the number of windows.

If I want to save these two data raw sources (inputs) into a file system or database, and also the synthetic data (outputs), how would you save them in order to be flexible and efficient with later data analysis? The analysis will be based on time-series algorithm in order to dedect patterns and anomaly detections.

Note, the two sensors are an example of different sources with different requirements but the use case is not ""that simple"". I would like to discuss the design of modeling and storing/extraction of these time-series with easiness, scaling, and efficiency in mind.

&#x200B;

&#x200B;",5,3,Plenty-Button8465,2024-01-23 17:07:26,https://www.reddit.com/r/dataengineering/comments/19dt9fn/how_to_model_and_save_these_two_data_source/,1,False,False,False,False
19d44ul,What questions to ask when deciding on partitioning strategy?,"Partitioning isn't always a win, like any optimization it's a mix balancing trade-offs in favor of the type of workloads that higher priority and seen more often.  


When determining how large data set should be partitioned, what questions do you ask yourself?  


When determining how large a data set should be partitioned, what questions do you ask yourself?  
",4,10,AMDataLake,2024-01-22 19:34:51,https://www.reddit.com/r/dataengineering/comments/19d44ul/what_questions_to_ask_when_deciding_on/,0,False,False,False,False
19csc3q,Big Data build on On-Premise,"My boss asked me to build a system that can quickly search and retrieve data. Right now I just process text data. It can be  thousands of T of data.

The data source is  giant  and come from vary resources and format (txt, csv, xlsx, json, ...) (the data from the same resource can be different format and data structure (fields) too).   Data between sources is linked and has overlapping fields but can take different values.  I mean, maybe for the same linked point sample, the data  received from each resource can be different, and wrong.  I cannot elaborate on the details of the project due to  security issue.

Right now, i have Minio as data lake for store raw data, PostgreSQL to store processed data from raw data and using Airflow for running the processses. Right now I  encountered performance problems when upsert data to postgre.

Is there any way to improve upsert performance? Or any other way to store and query data?  
Do you have any suggestions and Tech-stack  for this?

Thanks you for your help!",5,11,Midori-Yuu,2024-01-22 10:05:14,https://www.reddit.com/r/dataengineering/comments/19csc3q/big_data_build_on_onpremise/,1,False,False,False,False
19cnj0r,Pandas - Delta - MinIO,"Hi guys, i am using Pandas to read and write Deltalake table from minIO as dataframe to perform some data processing. Everytime i need to download all delta folder from minIO bucket to local machine, read it as dataframe, perform processing, save it as delta folder and push back to minIO bucket. Then i have to delete the temporary folder in local machine. Is there a way to connect directly from pandas to minIO deltalake like Spark connector? Do you know any document or sample code about using pandas or polars connector instead of Spark for processing data in cluster? Thanks for reading",5,9,resrrdttrt,2024-01-22 04:45:58,https://www.reddit.com/r/dataengineering/comments/19cnj0r/pandas_delta_minio/,0,False,False,False,False
19chr2m,Would a data engineer with experience in machine learning have a place in the semiconductor industry?,"Hi guys, what's up?

A few years ago, I decided to change careers to work with semiconductors, so I went back to university and am now studying engineering. As I'm older, I couldn't stop working because of my family. Thinking about having experience in technology, I ended up getting involved in data engineering and machine learning. I already have 3 years of professional experience in the field and some relevant certificates.

However, at university, I've been involved in FPGA research, RTL design etc. I've already taken some Verilog and SystemsVerilog courses from Cadence Design Systems, which has a branch here in the city where I live.

Do you think that my experience with data engineering and machine learning, combined with what I've been doing at university, would make it easier for me to enter the semiconductor field? Or would it be knowledge thrown away?",4,1,EversonElias,2024-01-21 23:55:03,https://www.reddit.com/r/dataengineering/comments/19chr2m/would_a_data_engineer_with_experience_in_machine/,0,False,False,False,False
19bqqbe,Career advice: More interesting job for less money?,"Hello everyone, I need some career advice, please: 

I'm currently employed as a Data Analyst for an IT consulting company. However, my recent project involves tasks unrelated to data analytics, focusing more on process mapping and ERP adjustments. Despite expressing my concerns to my manager, they've mentioned that reassignment to a different project is unlikely until 2025.

I've received an offer from a small AIaaS company to work as a Data Analyst within the Data Engineering team, and the role seems very exciting. My ultimate goal is to transition into a Data Engineering role by the end of the year.

The dilemma is that my current job offers a great salary, extra vacation days, and access to diverse future projects. On the other hand, the startup offer involves a roughly 30% salary cut and no additional vacation days. While I can manage the pay cut since I live with my parents, it's a significant downgrade.

So, what would you advise? Should I stick with my current job, even though I'm not enjoying it at all, but enjoying the perks while independently learning Data Engineering? Or should I accept the startup offer, taking a pay cut but gaining experience under the Data Engineering team?

Thanks in advance for your perspectives. I truly appreciate it.",5,8,_Lavender_Brown_,2024-01-21 00:43:54,https://www.reddit.com/r/dataengineering/comments/19bqqbe/career_advice_more_interesting_job_for_less_money/,0,False,False,False,False
19a4k8g,DBT Cloud RBAC - Best Practices,"Hi all,

At our company, we have chosen to use dbt Cloud (most of our developers prefer the cloud IDE was the decision point).

However, in terms of provisioning dbt Cloud infrastructure and RBAC, we have come across a few challenges in regards to RBAC primarily. We are wanting to use a service user with the Project Creator permission set, in order to automate the deployment of our infrastructure (which is primarily dbt Cloud projects). However, this service user can also promote other users to any existing dbt Cloud group, and this group could be the group for Account Admins. This is currently a limitation for us, as we cannot allow a circumstance where a service user can promote any user to Account Admin.

  
I am hence seeking to check with those of you who have automated their dbt Cloud infra deployment, and what their RBAC setup looked like?

&#x200B;

Thanks.  
",5,2,TinkermanN7,2024-01-18 23:40:42,https://www.reddit.com/r/dataengineering/comments/19a4k8g/dbt_cloud_rbac_best_practices/,1,False,False,False,False
199yd9j,Need help with Databricks Spark streaming.,"I read data from Event Hub and write it to a Delta tables after applying a few transformations. 

The transformations involve multiple 'withColumn' and two 'explode' functions (one for processing the body message received in the EventHub event and another one for internal transformations).

I use 'foreach' batches to load the data.

1. If I write directly to delta table, the results are poor, with delays ranging from minutes to hours.

2. If I follow Medallion architecture,

* EventHub to Bronze Delta table. (copy the event as it as)
* Bronze to Silver Delta table (final table: all the transformation are happening in this layer)

yields significantly better performance. The delay is almost negligible, around 3-4 seconds, despite the additional processing layer.

I'm curious about the specific reasons behind the enhanced performance in the second approach, despite the data passing through two layers of processing.

**Please note:** I use same cluster for both the processes.

[1](https://preview.redd.it/ahll6yvu19dc1.png?width=1023&format=png&auto=webp&s=ef5131d2729b4e28b822970ba486ca861c0bd000)

&#x200B;

[2](https://preview.redd.it/1mhgfg7w19dc1.png?width=1023&format=png&auto=webp&s=94f05f97d26b81fd09dbfffb1dfd086a6fd6fdf9)

&#x200B;",5,0,HousingStriking3770,2024-01-18 19:25:41,https://www.reddit.com/r/dataengineering/comments/199yd9j/need_help_with_databricks_spark_streaming/,1,False,False,False,False
199wu5k,"I have my job interview for the role of Azure data engineer. I'm really nervous. Please, any tip would be helpful.","I have two years of experience and I am looking to switch from my current job. The skills required by the company are databricks, Azure data factory, sql and Pyspark. Any tips on what to do and what not to would be appreciated.",5,5,SignalCrew739,2024-01-18 18:22:55,https://www.reddit.com/r/dataengineering/comments/199wu5k/i_have_my_job_interview_for_the_role_of_azure/,0,False,False,False,False
199nzng,Data Modeling Interview scenario questions,"I have an upcoming interview where one of the steps is to create a mock data model what should I be reading up on in preparation. And what are the key things they will wlbe looking out for and be considering when doing such an exercise?

For context I have decent amount of data experience just lacking formal data Modeling experience any tips would be appreciated thanks in advance",5,1,yanicklloyd,2024-01-18 11:25:10,https://www.reddit.com/r/dataengineering/comments/199nzng/data_modeling_interview_scenario_questions/,0,False,False,False,False
198sy24,SQL for Google Sheets with DuckDB,,5,0,dan_the_lion,2024-01-17 09:48:37,https://www.arecadata.com/sql-for-google-sheets-with-duckdb/,1,False,False,False,False
197ptmh,Programatically configuring Meltano,"I am considering using Meltano as the EL tool in an application that allows users to configure their data sources so they are replicated to a data lake. For that, it would be necessary to configure Meltano extractors and loaders programatically based on a user request. The application configuring them would not be running on the same server as Meltano.

For now, this application uses Airbyte under the hood, sending requests to the Airbyte server with their API. I'm getting more and more suspicious and frustrated with Airbyte, so I'm looking for alternatives.

Is there a way to do this with Meltano? Reading the documentation, I've seen that Meltano is configured by a CLI, or directly editing the extractors' and loaders' YAML files. There doesn't seem to be an API or anything like that (except for running jobs, where you have Airflow or Dagster integrations). Should my application edit YAML files on the server, or is there a more consistent and safer way of doing this?

(or is there a third, better alternative to effectively load data from numerous sources to a data lake, while configuring these replications programatically?)",5,9,henriquemeloo_,2024-01-16 01:10:09,https://www.reddit.com/r/dataengineering/comments/197ptmh/programatically_configuring_meltano/,0,False,False,False,False
1952j59,Does anyone actually find their data quality/anomaly detection applications useful?,"I've been asking people I know and the general consensus is that they're not that useful, but that's a biased people out there. I figure there must be people out there who find tools like Monte Carlo valuable, have you all found them generally useful?",5,7,MrMosBiggestFan,2024-01-12 18:49:31,https://www.reddit.com/r/dataengineering/comments/1952j59/does_anyone_actually_find_their_data/,1,False,False,False,False
194x2i4,Do you need university for data engineering,Do you need a university degree to be employable? If so is a bsc in statistics(with courses in programming and economics as well not just the theoretical maths)good?(This is what I'm pursuing).Also does it matter what grade you finish with or is having the degree enough? If you are from europe(as that is where I'm studying) your answers are especially relevant. Thank you!,5,19,Dizzy-Location4602,2024-01-12 15:00:39,https://www.reddit.com/r/dataengineering/comments/194x2i4/do_you_need_university_for_data_engineering/,0,False,False,False,False
194abpq,Sales to Data Engineering Transition,"I'm trying to transition from sales to data engineering. I currently sell a product in the data engineering space and it's revitalized my interest in engineering. I don't have a degree. Pretty much my only relevant professional experience would be selling a data product.

&#x200B;

I've coded on and off (mostly off lol) since the start of the pandemic. At this point I know basic python up to OOP and could probably throw together a web scraper or something basic. I'm starting to learn sql and have actually used it for some extremely basic queries at work. 

&#x200B;

I've just started the IBM Data Engineering cert hoping that this will give me a good route to actually learning the industry as a whole. After this I want to complete Google's equivalent (if necessary) as I've heard its a more in depth cert for people already working in the space. While learning I plan to simultaneously start putting together some DE projects. (Pulling data, storing it in the cloud, sql to make tables, data visualization.)

&#x200B;

Is it realistic to be aiming for a data engineering role with my background? Or should I start as an analyst? (My job is trying to get me into an analyst role in the next 6-12 months but 1 the role is very high level, think tableau and excel. No one uses programming currently. 2 by the time i get in that role, I could be well on my way to finishing both certs. 3 I believe the pay would be higher if I went the DE route, think \~80-95 vs I'm assuming at least 115-120 for a DE",4,11,ColivarTT,2024-01-11 19:36:50,https://www.reddit.com/r/dataengineering/comments/194abpq/sales_to_data_engineering_transition/,0,False,False,False,False
1940jpl,Why the Modern Data Stack sucks for data consultancies looking to productize,,6,0,sbalnojan,2024-01-11 12:26:59,https://arch.dev/blog/why-the-modern-data-stack-sucks-for-data-consultancies-looking-to-productize/,0,False,False,False,False
193uz6t,Build a (semi) automatic DWH for small midsize companies?,"…how good is it possible, and let’s say only one person in charge plus a remote support person (freelancer, consultant), in case of vacation, longer absence etc.

How low can be the monthly costs for productive 300 GB data warehouse ?

It can be cloud based or on-premise.

Do you have ideas or experiences - in midsize companies (below 250 persons) or small companies (1-25 persons)?",5,13,volvoboy-85,2024-01-11 06:22:52,https://www.reddit.com/r/dataengineering/comments/193uz6t/build_a_semi_automatic_dwh_for_small_midsize/,1,False,False,False,False
1933kce,dbt + ClickHouse?,"Hey there, I have some experience with dbt and just wanted to know if anyone knows of any use cases for integrating the two tools together for an ELT.  In my last organization dbt was used to treat our transformation pipeline like software development to make sure it had good documentation, testing, snapshotting etc.  Would it ever make sense to use dbt and clickhouse together?",5,5,TheAnglerfish1616,2024-01-10 08:30:00,https://www.reddit.com/r/dataengineering/comments/1933kce/dbt_clickhouse/,1,False,False,False,False
192sv8t,Does this tool exist?,"One of the products my company offers requires customers to submit export files from various platforms to us. From there, our current process is to transform the data in excel and import into a MySQL database. Yuck.

Each system exports similar data in the same shape, but of course, headers differ, customers use different values to mean the same thing, etc. Super simple transformations. However, it doesn’t make any sense to make an ETL for each one because these files are used only once.

What I am looking for is a tool that would allow someone who isn’t super technical to upload one of these files, select a preset target, select what each header means, make any necessary transformations, and check for errors/missing values.

I haven’t been able to find anything that does this short of building it myself. If it doesn’t, I’m thinking about building it on my own.

We have a long term solution planned for this whole process so it doesn’t make sense to build it out for the company.",5,10,phonyfakeorreal,2024-01-09 23:15:54,https://www.reddit.com/r/dataengineering/comments/192sv8t/does_this_tool_exist/,0,False,False,False,False
19253mf,Data Quality python Modules,"I have some excel files and I need to do some data quality checks using python

1) Null count and Treatment
    if a particular column has >25% , it should throw an email

2) Duplicate Treatment

3) Accepted values in Column 
   e.g State column should have only 50 states

4) Expected Datatype check for columns

5) Variance check for Numeric column compared to previous month 

etc

For now I have inplemented using pandas inbuilt functions.


I thought Great Expectations would help but I did not get how to use. seems I need to create a config file and I dont know should i create config for each Data file or not.",4,8,vainothisside,2024-01-09 03:42:39,https://www.reddit.com/r/dataengineering/comments/19253mf/data_quality_python_modules/,0,False,False,False,False
191x2nj,"Really big data set, combining pool of consumers with datedim","Hi all,

We have an interesting problem and wanted to get reddit's take on it. I am keeping it intentionally vague for privacy, but will add relevant details and can answer questions in the comments. 

We are building an application that segments consumers based on whether they have or have not done activities. Ultimately the application supports combinations of boolean logic and whether they have or have not done something to arrive at a list of IDs and a count of consumers. It also supports non-dynamic date ranges, whether they have done it in the last 30 days, 1 year, or all time. One example request would be ""Get me all the people who have done ((activity X AND activity Y) OR activity Z) in the last 30 days"", another could be ""Get me all the people who have done activity X AND NOT activity Y"". We delivered this functionality though our architecture has some issues. 

Primary issue is that the we are using event tables, and as a result we have records of consumers who HAVE participated in an activity but we don't maintain people who HAVE NOT done the activity, thus we cannot query consumers who have done an activity 0 times. To achieve the NOT logic, we exclude people who have done the activity to get the final result set. If we were to run a query that has activity X OR NOT activity Y, we would get unintended results. 

Secondary issue is that we have preset date ranges, which is useful at times but not very flexible or dynamic. Ideally a user could input a custom date range in the application to be applied to the backend queries. 

This is a very large data set. The total possible consumers is \~3B and they all participate in (some) activity. We 1) want to maintain a pool of consumers such that our NOT logic can query for records where participation count = 0 and 2) want to provide a flexible date range option (or mimic that functionality) 

First thought is to join a datedim table to our table of consumers to have one record for each combination of consumer and date. However this results in an ever increasing amount of data, for 2 years we would have \~2Trillion records and counting. Then we have to apply boolean logic to filter down the records and in some cases join subsets of records, which simply gets expensive and is computationally intensive for a user facing application. 

We are using **snowflake** for data storage and **python** for custom query generation, I am trying to think of creative ways to add a flexible date range and support potentially massive computation without requests from the application being too slow. For a user application, does it make more sense to compute dynamic date ranges on the fly after a request has been submitted, or precompute a massive dataset and filter it down after the fact? Or a mix of both? 

If anyone has any wisdom or similar experience please let me know, I am also happy to answer any questions in the comments. ",5,16,lt-96,2024-01-08 21:47:22,https://www.reddit.com/r/dataengineering/comments/191x2nj/really_big_data_set_combining_pool_of_consumers/,1,False,False,False,False
191t6ed,Employer asked me to contract after I gave my notice. Best way to calculate rate for proposal?,"I just quit without anything lined up due to extreme burnout and being pushed in a management direction on a very small team that doesn’t want or need it, when my career interests at this point are technical. 

My manager came back asking if I’d be interested in contract work to help finish out the project set to span Q1 & Q2 of this year. With my departure, they’re now planning on using in-house consultants of our 1-2 main vendors and asked if I would either assist with the technical component or managing the relationship with the consultant(s). 

Honestly, I would definitely be interested if it meant I could focus on just the scope given and didn’t have to be included in other meetings or pulled into the side quests that were pushing me over 40h a week, but trying to figure out a reasonable proposal. The main thing I care about is keeping my hours below 20 (ideally 12-16) as I try to heal this burnout. 

Is it as simple as a multiplier of current hourly rate? Or should I factor in my own experience, proprietary knowledge, education, etc? Should I propose hourly or estimate the hours and lump sum for the project? 

In case it’s relevant, my role is possibly more closely aligned with Analytics Engineering (heavy in dbt, I’m only one who knows the tool/our project well) and BI development (implementing dbt for end users), but feel this community has good insight. Anyone have experience here? Thanks in advance!",5,12,hairc-ut,2024-01-08 19:12:37,https://www.reddit.com/r/dataengineering/comments/191t6ed/employer_asked_me_to_contract_after_i_gave_my/,1,False,False,False,False
191lf3o,Event ingestion for cheap on AWS and GCP: 2 demos,"Hi folks,   


[dlt](https://dlthub.com/docs/intro) is the python library for data loading with schema evolution. Our contributors recently created 2 very interesting articles on event ingestion which I wanted to share.  
1. [Event ingestion on AWS](https://dlthub.com/docs/blog/dlt-aws-taktile-blog) with Lambda; Cost of 7.5 USD/1m events.  
2. [Event ingestion on GCP](https://dlthub.com/docs/blog/streaming-pubsub-json-gcp) via PubSub; Cost of 16usd/month for 2 workers that can process 1400 events/second.  


We are also working on doing our own similar pipeline on gcp and will share it when ready.

&#x200B;

dlt library added an early release of [data contracts](https://dlthub.com/docs/general-usage/schema-contracts) with the following options for handling bad events:  
\- evolve schema  
\- reject load package   
\- truncate columns and ingest (discard columns)  
\- discard event (discard row)  
We will next add bad data handling so if you have requirements how you would like that to work please comment [here](https://github.com/dlt-hub/dlt/issues/780).  


Thanks all! and let 2024 be a year with more open source data solutions :)",5,0,Thinker_Assignment,2024-01-08 13:38:39,https://www.reddit.com/r/dataengineering/comments/191lf3o/event_ingestion_for_cheap_on_aws_and_gcp_2_demos/,1,False,False,False,False
190pl8i,Issues in loading data from source to staging layer,"I m working on a data warehouse migration project from private database to cloud one. Basically I raw data will be uploaded on a S3 bucket and then we have to ingest it in snowflake cloud data warehouse. We are following S3>Stage>PSA>DWH>DMT layer.

While ingesting data from S3 to stage we are facing a typical issue when there is bad data. Like for example there is country_cd column. And in stage we are taking it's data type Varchar(2). But in certain file vendor are mistakenly putting whole country name. That's why my code is breaking down. To ingesting file. 

At present to we are using replace function to avoid that solution. But that is costly from time n cost perspective since raw data might contain billions of rows. 

You people have lot of experience in data engineering. Can you provide some idea or lead, how do you overcome this issue. I don't want to send corrupt file back to vendor. Doing so will cost 2-3 days to the business. After every breakdown.",5,7,asud_w_asud,2024-01-07 10:47:18,https://www.reddit.com/r/dataengineering/comments/190pl8i/issues_in_loading_data_from_source_to_staging/,0,False,False,False,False
18zmnct,What's the closest alternative to BigQuery on AWS?,"My company has started to do a cloud migration from GCP -> AWS and the data engineering team I'm part of has been very slow to move over because we'll need to refactor all of our ~50,000 lines of BigQuery specific SQL to Redshift.

We went all-in on complex data types in BQ because it was so easy to work with but Redshift seems to want everything to be normalized by nature of their feature set for complex types being so sparse.

Has anyone done a migration like this before? Is there a better alternative on AWS other than Redshift?",5,39,iwipestandin,2024-01-06 00:38:23,https://www.reddit.com/r/dataengineering/comments/18zmnct/whats_the_closest_alternative_to_bigquery_on_aws/,1,False,False,False,False
18ze6v4,AWS EMR to AWS Glue or Databricks?,"Hi everyone,

Happy new year! I have a few friends that work at enterprise companies are pretty heavy in AWS and spending quite a lot on EMR. The topic came up on whether they should be switching those workloads from EMR over to Glue. Others have mentioned that they could save a lot of money by moving the workloads from EMR to Databricks. Does anyone have a perspective for companies trying to save money and achieve better performance on whether they should move to Glue or Databricks?

I have started to go down the rabbit hole and have realized it is not as simple as I thought it would be...",5,8,NYCDataGuy,2024-01-05 18:45:11,https://www.reddit.com/r/dataengineering/comments/18ze6v4/aws_emr_to_aws_glue_or_databricks/,0,False,False,False,False
18z3cl6,Is this SQL joins cheat sheet correct?,"Hey, came across this joins cheat sheet, and it seems to me like 'album' table is not referenced in this query so how could album title be in the result. Am I missing something?

&#x200B;

https://preview.redd.it/wxtn8alvglac1.png?width=645&format=png&auto=webp&s=0b374aa7c2a7428657e255bdd85726dce3c760f7

https://preview.redd.it/oizzzwvuglac1.png?width=642&format=png&auto=webp&s=9480fffa0842d02439d7cf7fd939bf02ad40c9ce",4,15,In_Dust_We_Trust,2024-01-05 09:57:29,https://www.reddit.com/r/dataengineering/comments/18z3cl6/is_this_sql_joins_cheat_sheet_correct/,0,False,False,False,False
18xp09d,Azure DE study buddies,I am studying to switch to Azure data engineer role from a data analyst role. If anyone out there is doing the same or maybe just learning adf or python or pyspark and need someone to share the learnings with. Please reach out.,5,3,Vikinghehe,2024-01-03 17:42:56,https://www.reddit.com/r/dataengineering/comments/18xp09d/azure_de_study_buddies/,1,False,False,False,False
18xnan6,Data engineer job offer,"I have job offer for data engineer at fintech Smbc NYC location base 135k and one more offer from Walmart Dallas location 110k, which one is better regarding the company and location",5,20,No_Stand_3145,2024-01-03 16:24:04,https://www.reddit.com/r/dataengineering/comments/18xnan6/data_engineer_job_offer/,0,False,False,False,False
18xmwva,DBT + Dagster setup for an IoT startup,"How does this sound as a dwh / orchestration setup for an IoT startup with <100 tables and pipelines to manage?

I have a dbt / dagster project in a monorepo, containing all our db models, pipeline code and a Dockerfile. 

For local development, the Dockerfile is used to setup a dev container running dagster daemon + webserver. For staging / production, changes are pulled into staging / production branches and a GitHub action is triggered to run CI tests + build an image which is pushed to Artifact Registry, and then onto a cloud hosted VM (also running dagster daemon + webserver).

I would be using BigQuery as a dwh, and I was thinking each branch of the repo would have its own dataset in BigQuery. I think I could get this to work using environment variables in the dbt config files.

I'm aware that it probably makes more sense to deploy staging / production on kubernetes, but I don't want to overcomplicate things as this is already breaking a lot of new ground for me

Is there anything I've overlooked?",5,4,hennyblub,2024-01-03 16:08:00,https://www.reddit.com/r/dataengineering/comments/18xmwva/dbt_dagster_setup_for_an_iot_startup/,1,False,False,False,False
18xlejx,"Which management framework (scrum, lean, kanban, etc) does your company/team use for Data Engineering and more generally which framework do you think is the best for Data Engineering?","In my case it always happens to be ""scrum bi/weekly"" but I never felt it as perfect match due to continuous fire fighting, last minute unpredictable decision changes etc. However I never had the chance to test alternatives. What about you?",5,7,df016,2024-01-03 15:02:26,https://www.reddit.com/r/dataengineering/comments/18xlejx/which_management_framework_scrum_lean_kanban_etc/,0,False,False,False,False
18xblnf,How to maintain an active Data Engineer Community?,"Hi! I'm working in a product business, and we have about 200 members in our customer community. Most of them are Data Engineers. The main reason for having this community is to have a place for questions, ideas, and feedback about the product. Currently, I'm not satisfied with how active our community is.

So, I'm asking, what kind of things should I publish/create/do to activate our community?   
Have you ended up in some good communities that I can benchmark? ",5,1,Nikke47,2024-01-03 05:31:38,https://www.reddit.com/r/dataengineering/comments/18xblnf/how_to_maintain_an_active_data_engineer_community/,1,False,False,False,False
18wrfv0,Need Suggestions for Optimising Spark Jobs,"Hi Everybody, HNY 2024 🎉

I am a data engineer and with 3.4 years of experience having skillset in EMR, spark, Scala.

Currently I am focusing more on optimising the existing jobs in the current org.

I use basic optimisation techniques like broadcasting , persistence or using repartition and filtering. 

However could you please suggest some good resources that will help me understand better techniques of optimising spark jobs.
 
I have a basic understanding of spark UI however I don’t know where to look at when I am optimising a job. 

I would really like to know how you guys are doing optimisation an existing job and what parameters you look for when optimising a spark job.

Thanks !",5,3,swarup_i_am,2024-01-02 15:03:33,https://www.reddit.com/r/dataengineering/comments/18wrfv0/need_suggestions_for_optimising_spark_jobs/,1,False,False,False,False
18vkqlm,DB solutions for remote student project,"Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.
My questions are,
Is there a SQL software that permits this? Please suggest some names that are available for free or are open source.
Is there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)
Is there anything we can do to improve the project?",6,5,Traditional_Reason59,2024-01-01 00:14:48,https://www.reddit.com/r/dataengineering/comments/18vkqlm/db_solutions_for_remote_student_project/,1,False,False,False,False
18udgs8,Experience with Windmill for data pipelines?,"I've spent the holidays playing around with Windmill (https://windmill.dev) and so far I'm very impressed with it. I'm wondering if anyone here have tried running it in production for some time and have some experiences to share? 

It's not a tool for data pipelines specifically, but it still seems quite fit for purpose. I actually see its generality as a big advantage for us. We're a small team and we have lots of little utilities and integrations that currently exist in the form of somewhat disorganized scripts and repos. With Windmill it seems we can bring all of that together in a single platform.",5,4,andness,2023-12-30 11:14:48,https://www.reddit.com/r/dataengineering/comments/18udgs8/experience_with_windmill_for_data_pipelines/,0,False,False,False,False
18tdkfv,How do you securely manage remotely available secrets at scale in a cost effective way?,"Hello all,

I’m trying to figure out a good system for securely using and managing secrets. I’d like to hear about what everyone else does. I’ll also share my early idea for a system I think may some merit to it.

This approach leverages AWS Secrets Manager, Gitea, Age encryption protocol, and Mozilla SOPS. It attempts to provide robust and secure secrets management while minimizing cost,

**Key Components:**
1. **Infrastructure:**
   - Gitea hosted on a private cloud subnet, accessed via a bastion server.
   - AWS Secrets Manager for secure storage of encryption keys.

2. **Secrets Encryption:**
   - Age encryption protocol for encryption keys.
   - Mozilla SOPS to locally encrypt secrets in a `secrets.json` file.

3. **Remote Machine Usage**
     - Cloud machines perform `git pull` every time secrets are needed to retrieve updated secrets.
     - Fetch the new decryption key from AWS Secrets.
     - Decrypt secrets in memory using Mozilla SOPS.
     - Discard old decryption key and decrypted secrets from memory.
     - Discard decrypted secrets from memory after use

4. **Key Rotation Process:**
   - **Generate New Key**
     - Utilize Age encryption protocol to create a fresh encryption key.
     - Upload the encryption key to AWS secrets for later use

   - **Rotate Secrets**
     - Use AWS Secrets API to collect the old encryption key.
     - `git pull` all repositories that use the encryption key for their secrets
     - Decrypt all secrets with the old encryption key
     - Encrypt all secrets with Mozilla SOPS using the new key.
     - Embed AWS Secrets ID in the git push for future reference.
     - `git push` the updated `secrets.json` files to Secrets-Gitea.
     - Update AWS Secrets to include the new decryption key.

**Capabilities:**
- **Traceability:**
  - Each `secrets.json` file is associated with its encryption key through AWS Secrets ID.

- **Key Rotation:**
  - Enables regular updates for enhanced security.

- **Cost Savings:**
  - Effective use of cloud resources. Secrets management is expensive.

- **Version Control:**
  - Leverages Gitea for versioning of secrets, aiding in auditing and rollbacks.

**Considerations:**
- **Centralization Drawbacks:**
  - Potential single point of attack

Edit: perhaps with git-remote-gcrypt",5,8,DuckDatum,2023-12-29 03:53:55,https://www.reddit.com/r/dataengineering/comments/18tdkfv/how_do_you_securely_manage_remotely_available/,0,False,False,False,False
18sr2k6,Delta merge very slow for small amount of data,"Hi! 

We are implementing Delta table logic on an on-prem HDFS cluster using Spark for data processing. We append daily partitions to BRONZE layer which contain new records and updates from the previous day. We want to upsert a daily partition from BRONZE to SILVER layer, but it takes an unreasonable amount of time. If we simply append the data to SILVER, the whole ETL process is around 7 minutes (the write iself around 4 minutes) for a daily partition, which contains roughly 1 million rows. If we switch to merge write, then the runtime grows to roughly 30 minutes (read + transform only takes a few minutes, rest is the merge write step). I get that the merge operation is slower because of the join, but I wouldn't expect it to be this slow for only 1 million rows. Keep in mind that these performance tests were done with only a few daily partitions in the tables. We had the same performance when we worked with only one partition in the target table. Do you guys have any idea what we should do to optimize the performance? 

The things we have tried so far without any significant result:
- Add more resource to the Spark job
- Run OPTIMIZE command on target table
- Tried creating the delta tables with explicit and implicit partitioning
- By default, Spark does a SortMergeJoin at the MERGE step. We coerced other kinds of joins, but SortMergeJoin seemed to be the fastest one

The merge itself is fairly simple. We merge to the target table using a daily partition from BRONZE (roughly 1 million rows) on one matching ID of string type. If we have a match, we update the whole set (UPDATE SET *), otherwise INSERT *.
We will expect a relatively big amount of updates later, but right now we are working with test data which contains a couple of updates maximum per daily batch.",5,8,justadataengineer,2023-12-28 10:41:42,https://www.reddit.com/r/dataengineering/comments/18sr2k6/delta_merge_very_slow_for_small_amount_of_data/,0,False,False,False,False
1abwti3,Snowflake worksheets Git versioning made easy,Check out [Medium article](https://medium.com/@thomas.dambrin/snowflake-worksheets-versioning-made-easy-4512d8ee5cc2),4,0,thomas-d11,2024-01-26 23:52:36,https://www.reddit.com/r/dataengineering/comments/1abwti3/snowflake_worksheets_git_versioning_made_easy/,0,False,False,False,False
1abamz6,Best way to let users access private cloud resources from their home?,"Hello,

I’m curious, what’s the right way to let someone access private cloud resources from their home? I have a typical public / private subnet setup with a security group on the public that lets only office IP access anything,

Should I let them vpn into the VPC network, or have them vpn into a place with a permitted static IP?

I’m using AWS. What’s considered the easiest? What’s considered the cheapest? Any reliable open source solutions?

Thanks!",4,8,DuckDatum,2024-01-26 05:13:07,https://www.reddit.com/r/dataengineering/comments/1abamz6/best_way_to_let_users_access_private_cloud/,0,False,False,False,False
19fdjlw,Building an Analytics Division From Scratch - Seeking Wisdom :),"Found myself dumped into a pretty unique and scary position. I have the fortunate opportunity of being guy #2 in a $300M+ HaaS (selling an IoT device, then charging monthly subscription) company's newly-created business intelligence/analytics division.

It's an AWS shop. I have experience building pipelines so am familiar with tools like Kinesis, Dynamo, Redshift, Lambdas, Step Funcitons, etc, and I feel like I have a good grasp on IAM with a rough grasp of networking/security. Know Python and ofc SQL.

However, I'd consider myself a junior-type guy - most of my experience is me hacking away in the console as an IC in my prior role where I was the only ""data guy"". So, despite some exposure to IaC, git/repos, CICD,  I'm really studying this stuff hard as I plan to use each in this role since there will be a larger team forming around me.

Pillars that are already in place:
- DynamoDB for all crud operations in our app
- Redshift for data warehouse
- Spectrum/external link to Stripe data via Lake formation resource share
- PowerBI for viz
- CDK for IaC (org standard, no implementation in this division yet)
- DevSecOps team. Will be able to lean on them in the beginning for bootstrapping CDK stuff 

The division is intended to be a type of CoE for reporting across every functional area of the business (marketing, sales, user activity, warehouse, etc)

Question 1: Is it really essential to have a prod & dev environment (separate AWS accounts) for deployments? Since theres no real ""app"", this sort of feels like overkill for DE, especially since many resources are stateful, like Redshift. Feel free to correct my understanding of this topic

Question 2: What resources for DE best-practices would you recommend? Would love to get it right the first time around

Question 3: Have any advice concerning messaging/ communication of strategy to the business? I will be acting as the lead, so ""selling"" the vision for the division will be part of this role. This comes more-natrually to me than writing code, but I do love to code. Part of this process will also be me defining my role. Like I said - pretty unique position I find myself in

Thanks in advance for your help",3,5,__mifflinPaper,2024-01-25 16:41:08,https://www.reddit.com/r/dataengineering/comments/19fdjlw/building_an_analytics_division_from_scratch/,0,False,False,False,False
19exl3f,Interview Prep Advice?,"Hi all,  


Quick intro: 7+ years as a data analyst (all in SQL), was out of the workforce for a bit due to rehab and then the pandemic, signed up for and completed a 3-month coding bootcamp and have been on the job hunt since for Software/Data Engineering roles.  


I have a tech interview next week for a well-known company, and I was told this is what we'd go over:   
""During this session, we will provide a data set, along with an ERD. We will act as a business stakeholder and present you with a problem we are trying to solve. The goal of this session is to evaluate your experience gathering details and understanding of requirements to be able to write a user story that could be potentially put in the backlog for the Data Engineering squad. You are free to query the data (in fact, we encourage it!), ask any clarifying questions of us you may have, and even consult your friend Google for anything you may need.""  


Does anyone have any suggestions or resources they'd advise I use to prepare for this?  It'll be late next week?  


And thank you all so much.  To clarify again, I have 7+ years of SQL experience (though I am rusty at it), and have about 6-7 months of JavaScript and Python experience.  


&#x200B;",5,4,RipNastyy,2024-01-25 01:39:06,https://www.reddit.com/r/dataengineering/comments/19exl3f/interview_prep_advice/,0,False,False,False,False
19ex1ci,NYT Interview,Has anyone interviewed with NYT before and can give some information about what their technical is like. I have one with them coming up probably in about 2-3 weeks. Recruiter said their technical is a SQL one and a python one on codepad,5,0,El_Cato_Crande,2024-01-25 01:12:45,https://www.reddit.com/r/dataengineering/comments/19ex1ci/nyt_interview/,0,False,False,False,False
19eh12q,Cash Flow in Apache Superset,"Hey guys. I have a problem with Apache Superset. I'm creating a cash flow table, I need to get the total value of one day to be the value that starts the next, how can I set up a table that understands this structure in the superset, always the total value of one day will start the other... In the power bi I can do this easily, but in the superset I have encountered difficulties... Maybe some different modeling, but with the current model I have no difficulties in the power bi

&#x200B;

&#x200B;

https://preview.redd.it/v15foa493eec1.png?width=1076&format=png&auto=webp&s=de4df643f051991efc83030459900579b362dc7a",5,0,carlosandradeds,2024-01-24 13:25:19,https://www.reddit.com/r/dataengineering/comments/19eh12q/cash_flow_in_apache_superset/,1,False,False,False,False
19e1jv5,How much does your data team rely on SaaS products?,"There is a massive growth in SaaS data tools, and I was wondering how much people rely on such product. Excluding cloud data warehouses (Snowflake/Redshift/Data bricks).

How much does your company rely on Salas tools for orchestration, data quality, data catalogs, ingestion, transformation? How big is your data team and your company's revenue? 

Which products do you use as Salas and which do you self-host? Why? 

What are the pros and cons of each?",5,4,exact-approximate,2024-01-23 22:51:32,https://www.reddit.com/r/dataengineering/comments/19e1jv5/how_much_does_your_data_team_rely_on_saas_products/,0,False,False,False,False
19e0dbt,Experiences with reverse ETL from BigQuery to Salesforce?,"We are currently using BigQuery for our analytics and want to get some data from there into Salesforce. So we looked at some tools and at first glance it seems Hightouch (which we already use for 1 or 2 small syncs between different systems) and Census would do the trick. 

So we tested Hightouch and that does exactly what we want. However we do not like the costs involved, because you pay per active sync (and destination). Our use case is something like 6-20 active syncs, but only once a day and maybe 50-100K rows total a day. So we end up paying out of our noses for barely any data. 

I also saw Skyvia and they seem to charge per record, which seems to be perfect for our use case. Does anyone here have real world experience with Skyvia? Any other alternatives I'm missing here? Curious to hear what your experiences are with similar use cases and how you solved them. ",3,8,KoeitjeNL,2024-01-23 22:02:25,https://www.reddit.com/r/dataengineering/comments/19e0dbt/experiences_with_reverse_etl_from_bigquery_to/,0,False,False,False,False
19dsv7p,Copy Data from Sharepoint to the Azure Blob storage.,"I need to move and schedule the data copy operation from the company Sharepoint to the Azure Blob Storage. I am new to the Azure and found two candidate solutions.

* Use Azure Data Factory predefined jobs.
* Use Azure Functions with Python scripts.

From what I gathered Azure Functions would be cheaper, this is not super hard to code so I wonder if there is any benefit to the ADF. Do you have any thoughts/recommendations?",4,3,bartosaq,2024-01-23 16:50:45,https://www.reddit.com/r/dataengineering/comments/19dsv7p/copy_data_from_sharepoint_to_the_azure_blob/,1,False,False,False,False
19dnoxa,"Parquet Row Group Sizing, how do you choose your sizing?","Like any other optimization, parquet row group sizing has trade-offs, when do you choose to go for more row groups or less row groups?",3,0,AMDataLake,2024-01-23 12:49:48,https://www.reddit.com/r/dataengineering/comments/19dnoxa/parquet_row_group_sizing_how_do_you_choose_your/,0,False,False,False,False
19cxvi9,Define and manage infrastructure using HCL (Terraform)? You may need a Terraform Automation and Collaboration tool,"Hey r/dataengineering,  
We have seen increased Terraform adoption amongst Data Engineers over the last 2 or so years, but have seen that most of them aren't aware of  Terraform Automation and Collaboration tools that help in CI/CD for terraform and enable RBAC, drift detection and concurrency in the process, so that Terraform can be used efficiently at scale.  


There are several open source tools that help with this (Disclosure: I am building Digger, one of the tools):  


[Digger](https://github.com/diggerhq/digger)

[Atlantis](https://github.com/runatlantis/atlantis)

[OTF](https://github.com/leg100/otf)

[Terrakube](https://github.com/AzBuilder/terrakube)  


Feel free to check them out and share feedback if you are already using any of them. Also please share any challenges you face while using Terraform as a team, it would be useful for us to learn!",5,0,utpalnadiger,2024-01-22 15:15:47,https://www.reddit.com/r/dataengineering/comments/19cxvi9/define_and_manage_infrastructure_using_hcl/,1,False,False,False,False
19carxy,Feedback on an Event Based Automation System?,"Hello,

I'm currently working on developing email notifications for impending software license expirations within our organization. I'm adopting an event-based automation approach. The architecture I've proposed would use existing instances of Prefect and Postgres, while introducing Redis and Celery as an intermediary to allow Postgres to communicate with Prefect.

The automations are handled via a Postgres `jobs` table. INSERT instigates a NOTIFY which Redis accepts as a PUBLISHER. This propagates to Celery passing along information to the Prefect API.

The job will include details about the deployment to run and the parameters to pass into the deployment. So Redis and Celery are not stuck to any single automation. Furthermore, Celery would query the Prefect API for updates on the Flow Runs status and update the `jobs` table respectively.

The linked architecture (https://imgur.com/a/H5kC6wE) is designed to prevent the creation of isolated solutions for every issue as the code base evolves and matures, avoiding a complex and convoluted long run. I’m hoping this approach to email notifications is not too complicated while offering a much higher benefit. I'd appreciate feedback on this approach.

Our capabilities are limited digitally and it’s in the organizations interest right now to improve our “data platform,” so to speak. So I don’t necessarily see this as an unnecessary effort. What’s your impression on the design?",3,6,DuckDatum,2024-01-21 19:00:54,https://www.reddit.com/r/dataengineering/comments/19carxy/feedback_on_an_event_based_automation_system/,0,False,False,False,False
19caao0,Designing a data workflow for a small company,"I work at a company with eight employees, and was recently hired as a data/business analyst. The company uses an in-house software to manage online retail sales. There is a huge amount of data available, and I need to design a workflow to capture sales data and business analytics. Currently, most of the company’s data is stored in a combination of a MySQL database and DynamoDB.

I would like to design a workflow that queries one or both of these databases to record daily statistics (i.e. sales of groups of products over time), cleans the data, and records it in a new “analytics” database. This new data would then be easier to handle for visualization, modeling, etc.

Currently I’m taking a course in AWS Lambda, because it seems like that’s the best way to go to automate my scripts. I’m looking into relational database and NoSQL models, and am leaning towards the latter but I’m unsure.

Does anyone have advice on this workflow, ideas on where to start, or resources to consult? This is my first job in the field, so I really appreciate any advice!

Tl;dr - I am designing a workflow to query a MySQL database > clean data > record statistics in a new “analytics” database > use cleaned data for visualization",5,17,Strong-Mission-118,2024-01-21 18:40:46,https://www.reddit.com/r/dataengineering/comments/19caao0/designing_a_data_workflow_for_a_small_company/,1,False,False,False,False
19c6d38,How do you prefer to deploy infrastructure?,"Kubernetes, Cloud Managed Service, etc.  


When given the option, what do you prefer?",4,4,AMDataLake,2024-01-21 15:54:01,https://www.reddit.com/r/dataengineering/comments/19c6d38/how_do_you_prefer_to_deploy_infrastructure/,0,False,False,False,False
19byhep,Anyone here working on data engineering roles using SAP tools?,"Hi all, 

Based on the current data engineering landscape, I believe most of the data engineering tools that is used by us are outside of the SAP realm. However, I would like to test and understand if this is true. 

Are there any data engineers lurking here familiar with tools like SAP Data Services, SAP HANA, SAP BW/4HANA, SAP Data Sphere and SAP Analytics Cloud. Some can argue that these are not standard data engineering tools but it still very capable tools to process and serve the data for their intended purpose. 

Also, is it desirable to get into the SAP ecosystem and work on SAP specific data projects?",4,4,scht1980,2024-01-21 08:04:43,https://www.reddit.com/r/dataengineering/comments/19byhep/anyone_here_working_on_data_engineering_roles/,1,False,False,False,False
19b9cwv,"New to AWS, need some advice","I'm a DE with 2 years of experience as (lead) data/platform engineer using mainly Azure Synapse. Currently I am asked by our data architect to create a solution using the following requirements:

-we receive an xml full load of a big dataset every day in an S3 bucket.
-we want to create iceberg tables in S3 by doing CDC on the xml data (in some fashion)
-goal is to be able to efficiently timetravel the database. 

How would you tackle this problem?",5,5,Schuurspons93,2024-01-20 11:00:05,https://www.reddit.com/r/dataengineering/comments/19b9cwv/new_to_aws_need_some_advice/,1,False,False,False,False
19aa2m8,Need suggestions for ETL/ELT tool,"I'm currently working in a company which uses MSSQL/MYSQL servers and BigQuery as the data warehouse.
And I have been assigned a task to implement some form of live streaming from the mssql/mysql server to BigQuery. Currently what we have is using the mssql change tracking feature and sending data through a custom .net service which picks up the change tracking data in batches to BigQuery using the Gcp load to table and merge.

I have done a poc on airbyte and dbt to do the same thing and was wondering if there are any other tools especially OSS ones which might be better than airbyte as it also does the same thing as our .net service but has the flexibility of connecting multiple sources without much hassle.",6,9,KilluaHatake,2024-01-19 04:04:14,https://www.reddit.com/r/dataengineering/comments/19aa2m8/need_suggestions_for_etlelt_tool/,0,False,False,False,False
19a0ybk,Real time infra,"I have a mysql database that I read the changes (CDC) with Kafka. I need to join 3 tables/topics and send a notification to a page on the front when I have a new conversion. How to make this ETL and this endpoint be consumed?

My env is in kubernetes",3,2,Dwoler,2024-01-18 21:10:17,https://www.reddit.com/r/dataengineering/comments/19a0ybk/real_time_infra/,0,False,False,False,False
199pi8s,Recommendation for Tech Stack for creating ELT process from scratch for energy infrastructure consulting firm.,"Hi everyone,

I recently started a new position at a consulting company that creates own web-based software products for companies in the energy infrastructure sector in Germany. Because each project was treated isolated regarding its data management, there are many silos of data in Excel, Postgres databases, or Python data frames, some in the Azure Cloud and some on on-premise servers.

I started my position in one of those project teams but convinced the company to start fresh with its data management and build a centralized data warehouse / data lake environment from scratch to be used by every project team.

Now I am struggling to find a decision on what our future stack should look like and if I could start with one tool that may not cover much but incrementally add more tools as soon they are necessary. So a little more context on the status quo:

* Most transformation and general ETL process of data happens in manually triggered Python scripts that may or may not utilize SQL scripts as well
* There are 6 colleagues who create data products but never cared much about data engineering, which is why I would consider them data analysts / data scientists. All of them are more familiar with Python than SQL, but also can handle both
* There are 5 developers who cared more for the backend development, but for whom data engineering was rather an addon in the process not worth of thinking more than necessary of. The focus was always how to prepare the data for that specific software feature
* Data Analytics is not yet a big thing. It is more like data products are created and the pushed to the software it was made for. Updates of these data products are mostly triggered manually. However, clients more and more want a better understanding of the construction process of their infrastructure and projects and asked more frequently for dashboards and reports
* Because much of the data has a spatial context we use Postgres with its PostGIS extension to handle this data format. It is also where my expertise as a spatial data analyst lays and I have some years of experience working with Postgres and PostGIS. Thus, I am more versatile in SQL than Python but can work with both as well
* We don't handle big data (yet) and also don't have live data feeds of sensors or such to handle. However, we have data from many sources and also need to create data products that are then used in different software systems, internal and external, and, eventually, also in BI tools.

&#x200B;

My current plan would include the following tools and technologies to build a data lake and ELT infrastructure from scratch:

* Start rebuilding the old data infrastructure in a new on-prem **Postgres database** using **dbt** to model the data (not dbt cloud).
* Even though the colleagues are more experienced with Python, I still use dbt for creating the basic models that are then used in **pandas** scripts for further data manipulation by the colleagues and then reintegrated in the data model of dbt
* After this is done use **Airflow** to schedule and orchestra the data manipulation process.
* Providing our clients with a centralized **content management system** to upload and store their Excel sheets that follow our data model requirements which we then integrate into the data lake using scheduled jobs in Airflow
* Use **Power BI** (due to our licences with Microsoft products) to provide a BI tool that leverages data for analysis purposes
* Create **Fast API** endpoints that look to our data products created in the data lake and used by software developers
* When we see that we need to scale we may have arguments for the c-level based on the experience we made with the above stack and its benefits it created to migrate to cloud and introduce services like Snowflake and Fivetran

&#x200B;

I would be really happy to see what you may think about my approach and if you have any critics about it and any suggestions for improvement.",3,10,rick854,2024-01-18 12:53:54,https://www.reddit.com/r/dataengineering/comments/199pi8s/recommendation_for_tech_stack_for_creating_elt/,0,False,False,False,False
199e5tw,Organising metabase,"Do you have good strategies around keeping metabase organized?  

What seems to happen is we get a lot of similar questions by different business units and then it’s hard to find anything.

Or the data changes- like google analytics not being supported has made a lot of charts go dead.

How do you all keep a handle on things?",5,3,bluezebra42,2024-01-18 01:41:58,https://www.reddit.com/r/dataengineering/comments/199e5tw/organising_metabase/,1,False,False,False,False
1995ho8,Database design for my software app - how to handle changing fields from API response?,"Hi there, I'm building a data-focused software app, so while I don't require the same scale that traditional data engineering requires (due to lack of actual users so far lol), I'd still like some DE input.

The app is surrounding League of Legends, and the part of the software I'm concerned with right now is getting the data from the Riot API into our database.

I have to process one player at a time, because the API call is on a per-player/per-match basis

The steps are essentially:

1. Get the match IDs that a player has played
2. Loop through these match IDs to make an API call, that returns the actual in-game data for that particular match
3. Append all this data into a pandas dataframe
   1. I know pandas is looked down upon as a transformation tool, but the total matches for each player is (at max) 1000 rows and \~200 columns, which always fits within memory
4. CAVEAT - due to the game itself changing over time, they are continually adding new columns/stats that are tracked in matches, so there are a changing # of columns returned by the API
5. Because of the caveat, I take the intersection of the columns we have in the database already, and those returned by the API. I ""manually"" add the columns we don't have in the database `ALTER TABLE ADD COLUMN`
6. I then just run a `df.to_sql()` to get all the matches into the database

I realize that this isn't the best approach, because to have a table that is continually changing can lead to troubles.

I would ideally like to store ALL data, but I really only need a subset of the data for the actual product that I'm trying to create.

# The main question I have is, how should I handle extra fields being added to the API response?",3,4,NFeruch,2024-01-17 19:36:56,https://www.reddit.com/r/dataengineering/comments/1995ho8/database_design_for_my_software_app_how_to_handle/,0,False,False,False,False
198o11q,Automate dbt cloud using Dagster,"I’m fairly new to both dagster and dbt cloud but do have some experience with both. 

I’ve successfully setup a pipeline in dagster that grabs raw data from an API, loads it to cloud storage and copies the raw data from parquet format into snowflake. 

I separately have dbt setup to take that raw data and build some data models. 

Now I simply want to trigger the dbt cloud job to run once the data pippeline job runs via dagster but I can’t for the life of me figure out the syntax. I’m able to get everything to run in one “run everything” job but this defeats the purpose of having my dbt cloud assets be dependent on the completion of the data pipeline job. Please help???

Has anyone done this before using dbt cloud (not dbt core)?",4,4,skiyogagolfbeer,2024-01-17 04:36:08,https://www.reddit.com/r/dataengineering/comments/198o11q/automate_dbt_cloud_using_dagster/,0,False,False,False,False
197zo50,Personal project: Free tier serverless GCP dlt + dbt Portugese real estate price analysis,"Hey folks,   


one of the dlt contributors wrote up how they created a dlt + dbt pipeline that they run in cloud functions to get some pricing info on real estate data, that is otherwise not available to the general public.  


You can read it on the dbt blog, sharing it [here](https://docs.getdbt.com/blog/serverless-dlt-dbt-stack)",3,4,Thinker_Assignment,2024-01-16 10:20:02,https://www.reddit.com/r/dataengineering/comments/197zo50/personal_project_free_tier_serverless_gcp_dlt_dbt/,0,False,False,False,False
196xnrt,Assistance with running python library on EMR cluster,"
Hey guys,  

Got a problem I’ve been trying to solve in my DE team. So we’re creating a custom ETL framework in-house that uses HUDI and Pyspark to perform our ETL. We are using EMR clusters to run the ETL scripts on MWAA. 

I’ve been stuck on trying to run our framework that is packaged as either a whl/egg on an EMR cluster. Essentially the driver script will import functions that exist within the whl/egg. I’ve tried some ways where you add the Python package that resides in an s3 location as a run parameter during the Spark submit run for an EMR Cluster but I get a “cannot find package” error. 

Can anyone point to any examples or sample code online that does this on a provisioned cluster? I’ve read some AWS documentation and other articles online but can’t seem to find one that instructs you on getting a custom Python package running on EMR that does not involve a pip install. 

Any help is greatly appreciated!",4,5,SignalReaction77,2024-01-15 02:35:01,https://www.reddit.com/r/dataengineering/comments/196xnrt/assistance_with_running_python_library_on_emr/,1,False,False,False,False
196ngi6,How can I move back to data engineering from a business system analyst role,"Hi ,

I have been working as bi/etl , data engineer for 15 years . Recently i took up business system analyst role. Will it be beneficial for my career in case I want to pivot back to data engineer or will it hurt my profile. The thing is Iam not sure I would be good at being business system analyst and was thinking to go back to data engineering in cloud in case things don’t work out.",4,3,InstanceSea05,2024-01-14 19:09:19,https://www.reddit.com/r/dataengineering/comments/196ngi6/how_can_i_move_back_to_data_engineering_from_a/,0,False,False,False,False
196ml6j,Data Modeling Resources,"Does anyone have any favorite resources for developing a rigorous approach to data modeling? I have a couple years of real-world experience developing data models in my work- identifying business needs, thinking about how data is collected and accessed and mapping that to a model that balances the business's priorities- but I'm looking to develop a more rigorous or formal framework or methodology for data modeling now that I think I'm primed for it.  


Does anyone have any favorite books, videos, or other resources they really like for learning data modeling?",4,2,Cheetah-Infinite,2024-01-14 18:32:23,https://www.reddit.com/r/dataengineering/comments/196ml6j/data_modeling_resources/,1,False,False,False,False
195oj1a,What are the things that we should consider when migrating data from legacy systems?,"Hello everyone,

I’m currently involved in migrating data from a legacy system to a cloud platform. What are the key considerations when initiating such operations? In other words, how do I plan for such a project from scratch?

I’m keen to learn from your experiences. How have you successfully managed projects like these? Your insights and thoughts are incredibly valuable to me.

Thank you!",4,16,adatascientistSl,2024-01-13 13:55:25,https://www.reddit.com/r/dataengineering/comments/195oj1a/what_are_the_things_that_we_should_consider_when/,0,False,False,False,False
195jn3v,How to set up a data processing script automatically,"So I'm a data scientist at a very small startup, trying to grasp some DE concepts. I have a python script that downloads data from a database, does some processing in pandas and then uploads it to another service via their SDK. I want this script to run automatically once a day.

&#x200B;

We work with Azure and unlikely to move to AWS currently, but are open to using any other tools. Could I get some direction on how to set this up? I see Airflow could be relevant but I don't understand where the computation in Airflow actually run. Do I need to connect it to Azure somehow to give it access to computing power?",4,4,PixelPixell,2024-01-13 08:42:56,https://www.reddit.com/r/dataengineering/comments/195jn3v/how_to_set_up_a_data_processing_script/,1,False,False,False,False
1955qoh,Spark Datasets vs DataFrames performance,"When I was learning about Spark with Scala I read that Datasets are a very powerful feature as they allow for compile-time type checking, as well as taking advantage of functional programming when performing transformations.

But most sources I read mentioned that they are also slower than DataFrames, although according to [this article](https://itnext.io/making-the-spark-dataframe-composition-type-safe-r-7b6fed524ec2) from 2021 the performance gap between these two is narrowing. Moving 2 years later, [another article](https://sparkbyexamples.com/spark/spark-rdd-vs-dataframe-vs-dataset/) (from November 2023) claims that Datasets are actually faster than DataFrames.

Can anyone confirm if it's true? And if so, does it make sense to only use Datasets when processing data using Spark with Scala? Or are there any areas where DataFrames would be a better choice?",4,8,Cydros1,2024-01-12 21:02:55,https://www.reddit.com/r/dataengineering/comments/1955qoh/spark_datasets_vs_dataframes_performance/,0,False,False,False,False
194o6gm,Utility of Data Science / Statistics / ML Knowledge in Data Engineering,"Hey everyone,

I'll start with some good news; I recently got my first job as a data engineer!  It's contract-to-hire at a public health insurance agency and I am very excited for it.

Before this position, I studied an MS Information Systems degree where I learned a lot about data science, ML and statistics.  In this job, it looks like I will be using none of it.  I'm trying to think if there are any ways I can use the data science knowledge I learned in grad school to grow in my data engineering career, or whether it was just lost time.  Could it come in handy if I focus on data engineering for ML in the future?  Could there be overlap where it could come in handy?  Maybe should I look for ways to apply this knowledge in my new role and not worry about how it fits into the bigger picture? 
 Let me know - right now feeling a bit sad that I'm not putting anything I learned in grad school to good use, like it was a waste of time.

Thanks in advance for your thoughts!",3,2,i_am_baldilocks,2024-01-12 06:17:03,https://www.reddit.com/r/dataengineering/comments/194o6gm/utility_of_data_science_statistics_ml_knowledge/,0,False,False,False,False
1941wzt,Data transformation; what does it really mean?,"we’re currently working on our data, and we just entered the the phase where we’re transforming our data using dbt. 

I’m trying to watch closely what my team (1 person) is doing and tbh i didn’t notice any changes!

I mean, I find it exactly replicating the data from the data source. that’s it. I can’t see the added value. 

We only re-group data into facts and dimensions, remove duplicates and rename the columns… I thought this phase will hold A LOT more. So, it’s clear that we didn’t understand how to use it properly. 

as an example, hubspot is one of our data sources and I found the transformed data not adding any value.. and we only have 7 tables from this source (?) leads, deals, quotations and contacts.

I’d appreciate if you can give examples of data transformation the people usually use and apply (I totally understand that this depends on the data/business/ business logic) but I’m here trying to understand what are we missing and why i find it useless here.",5,21,Fuzzy-Example-7326,2024-01-11 13:38:30,https://www.reddit.com/r/dataengineering/comments/1941wzt/data_transformation_what_does_it_really_mean/,1,False,False,False,False
193y54p,Data engineering project phases,"Hi! I was developing some internal documentation about phases in data engineering projects and I would love some feedback.

By now, the phases I have are:

1. Requirements gathering
2. Data architecture design
3. Time planning
3. Develop and implement
4. Testing
5. Deploy on production
6. Maintenance
7. Evaluation

Did I forget something important? Obviously, each point is explained in detail in the docs.

Thank you all in advance!",3,5,data_macrolide,2024-01-11 09:58:03,https://www.reddit.com/r/dataengineering/comments/193y54p/data_engineering_project_phases/,0,False,False,False,False
193r7as,"Do you host multiple apps on the same machine, even if they’re all containers?","Hello everyone,

At the moment, I organize my workload with a single machine per application instance. I am about to deploy an orchestration server to trigger some automations on a schedule.

I’ve got all my automations on my local machine. These aren’t applications per se, just little scripts, ETL jobs, you name it. Now, I’m trying to think of how I want to deploy workers and automations… it doesn’t really fit in with my current model, does it? I see no sense in allocating an entire machine to a single automation of this scale.

How do you guys match machine to <thing> in your job? Do you guys have a high level architecture that you follow? Would you end up creating a machine designated for hosting several automation, or design it like how I do apps?",5,4,DuckDatum,2024-01-11 02:59:48,https://www.reddit.com/r/dataengineering/comments/193r7as/do_you_host_multiple_apps_on_the_same_machine/,1,False,False,False,False
192r24x,How do your teams handle backfill with streaming data?,"let's use a simple use case of no windowing or anything like that. Kafka/Flink transformations are done and then data flows into a DB (such as Snowflake) to be queried. 

Suppose that you find out at some point that there is a bug with the streaming data transformation, and that it needs to be changed and past data needs to be re-processed. How is that handled?",4,8,harrytrumanprimate,2024-01-09 22:03:23,https://www.reddit.com/r/dataengineering/comments/192r24x/how_do_your_teams_handle_backfill_with_streaming/,0,False,False,False,False
192abwa,what graph DB to use for knowledge graphs for our LLM,"I'm working at a big fintech company and been assigned to create a knowledge graph for our LLM, which will utilize it for data ingestion.

This will be my first time fidgeting with graph DBs, which products/tools you suggest? The data that I will migrate is currently in AWS S3, AWS Redshift and MySQL DBs.",5,6,0xkiichiro,2024-01-09 08:52:38,https://www.reddit.com/r/dataengineering/comments/192abwa/what_graph_db_to_use_for_knowledge_graphs_for_our/,0,False,False,False,False
1903xvz,Is this concept possible? PBI calls R/Python to ETL?,"Hello there,

I’m in the process of building a department datamart using DataVerse. I’m coming from mostly an R/Python desktop development background. In the vein of trying to build a fairly robust and automated data pipeline, I’m seeking some help on this concept’s feasibility.

Is it possible to have PowerBI call R or Python scripts to conduct some complex transformations? Right now the process is that we use SAS to load CSVs and a hash map to aid in the transformation of the data for later loading into different lists and DataVerse. Then the data is manually loaded into tables. We cannot connect SAS directly to DataVerse or PowerBI in our environment. So right now it’s all completely manual and I want to move away from SAS.

What I’d like to do is be able to load the to-be-transformed files into a document library (SharePoint), and trigger PowerQuery to run an R or Python script in PowerBI to grab the data, transform it (using a DataVerse table or SharePoint list as a replacement for the SAS hash map). Then perhaps PowerAutomate or PowerQuery could write the new output into an existing history table in DataVerse.

Right now these are the tools we’re constrained to using and cannot branch out into other cloud tools. So I’m trying to find a reasonably automated way to build and automate this data pipeline.

Appreciate your thoughts on this :)",4,0,joyfulcartographer,2024-01-06 16:40:12,https://www.reddit.com/r/dataengineering/comments/1903xvz/is_this_concept_possible_pbi_calls_rpython_to_etl/,0,False,False,False,False
18zic2s,How does the pay-as-you-go service work in the 2B market?,"Some serverless SaaS only provides the on demand payment scheme, which says you don’t need to pay a lot of money periodically(usually once a year), just pay as you go. But on the other hand it’s very hard for customers to decide the budget as they have no idea how much they will use.

AFAIK, Snowflake, one of the biggest SaaS company, is a typical vendor who only allows the on demand payment. Some of the companies pay even millions dollars a year for them.

I’m curious about that how does the deal work when the price is likely unpredictable? For small companies they can just use a credit card to pay the bill, but this way won’t work in a big company.",5,3,leiysky123,2024-01-05 21:37:03,https://www.reddit.com/r/dataengineering/comments/18zic2s/how_does_the_payasyougo_service_work_in_the_2b/,1,False,False,False,False
18z8lgr,Dev data for reports,"Our security team has decreed that production data will no longer be permitted in dev environments (it will still be permitted in all other environments). The analysts on my team are upset because they don’t believe they can create dashboards on the tiny static, synthetic dataset that’s being proposed as a replacement, and given all the time series analysis they do I see their point (hard to compute sales in the last 30 days if your data only has timestamps from 2 years ago).

How do you handle this at your org? Do analysts build their reports solely against prod? Or have some other strategy?",4,3,demost11,2024-01-05 14:49:13,https://www.reddit.com/r/dataengineering/comments/18z8lgr/dev_data_for_reports/,1,False,False,False,False
18ybwr0,Pipeline Implementation order,"Hi guys! Hope you are having a nice day.

I want to implement a pipeline with the following:

Data Ingestion: Python scripts.   
Data Transformation: dbt.   
Data Warehouse: Snowflake.   
Orchestration: Dagster.   
Visualization: Grafana/Redash.   

I was wondering how should I proceed, meaning in what order of implementation, and also architecture-wise how to organize it (Ideally, all on prem and using docker).

I was thinking Snowflake set up, then a basic Dagster, the API calls to start storing data, then dbt to start transforming, then configure dagster for orch, and finally the Grafana connection.

I'm an experienced software engineer but I'm new to this world to be honest, and I would really appreciate your help or guidance. Thank you, and any recommendation is also welcome.",4,2,GiraffesWithBigDicks,2024-01-04 12:22:30,https://www.reddit.com/r/dataengineering/comments/18ybwr0/pipeline_implementation_order/,1,False,False,False,False
18xemop,Customer Level Unit economics,"Hey, a Fractional Head of Growth here.  
Has anyone implemented **Customer Level Unit economics**? (B2B or B2C)

Basically have an event-driven data architecture that allows you to assign cost or revenue to every action a customer might take. The goal is to identify the most profitable customers and the actions that drive profitability.

It helps answer questions like:

1. Bottom 10% of loss-making customers
2. Profitability of customers in 20s vs 30s vs 40s
3. Profitability of customers who signed up through Google vs Meta

If yes, how? Is there any resource I can refer to? I am looking to implement this at 2 companies (B2B and B2C)

Thanks!",4,2,Scary-Swing2852,2024-01-03 08:34:51,https://www.reddit.com/r/dataengineering/comments/18xemop/customer_level_unit_economics/,1,False,False,False,False
18wxzco,Need Suggestions for a MSc Research Project in Data Engineering,"Hey everyone,

I'm currently brainstorming ideas for my MSc Data Science/Comp Eng research project and I'm leaning towards a topic in Data Engineering. I'm particularly interested in areas involving pipelines, ETL, efficiency, cloud computing, and the use of Python, Spark, and SQL.

Has anyone come across any recent research or projects in these areas that could inspire a solid project idea? I'm looking for something technically challenging and innovative.

Appreciate any suggestions or pointers to recent papers, projects, or trends in this field!

Thanks!",4,1,dildan101,2024-01-02 19:32:25,https://www.reddit.com/r/dataengineering/comments/18wxzco/need_suggestions_for_a_msc_research_project_in/,1,False,False,False,False
18wq7jj,What is Incremental View Maintenance (IVM)?,,5,2,hkdelay,2024-01-02 14:06:07,https://open.substack.com/pub/hubertdulay/p/what-is-incremental-view-maintenance?r=46sqk&utm_campaign=post&utm_medium=web&showWelcome=true,1,False,False,False,False
18wg4lm,Where does terraform fit in with CI/CD?,"I’m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there’s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.

Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?

I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?

Thanks in advance!",5,6,DuckDatum,2024-01-02 04:08:05,https://www.reddit.com/r/dataengineering/comments/18wg4lm/where_does_terraform_fit_in_with_cicd/,0,False,False,False,False
18w7k0o,Individual contributor or Management career path?,"Curious people's thoughts on if they prefer individual contributor or management path when it comes to DE and DS work.  I'm 2 years into a management role and enjoy it b/c I have good employees but sometimes miss doing the actual code and work rather than overseeing it.

I think management has a higher financial ceiling but comes with a lot more work (office politics) as you climb the ladder.",5,5,Snoo-74514,2024-01-01 21:40:54,https://www.reddit.com/r/dataengineering/comments/18w7k0o/individual_contributor_or_management_career_path/,1,False,False,False,False
18v4s1q,Learn Spark via Databricks Community edition or Local Hadoop environment in Ubuntu,"Learn Spark  via Databricks Community edition or Local Hadoop environment in Ubuntu


Hi DEs
I am a data analyst with 5YOE from India and have worked on ETL  using SQL(Redshift, Oracle, MySQL). I also worked on Python mainly pandas for data crunching. 

I want to get into Data Engineering and I have to learn Spark. Should I learn and hands on Using browser based Databricks community edition or install hadoop, Hive, Spark on my Ubuntu laptop .


Does it even makes any difference learning from Pre-configured Data bricks and installing and configuring Hadoop enviromenr on my laptop?",4,16,vainothisside,2023-12-31 10:07:51,https://www.reddit.com/r/dataengineering/comments/18v4s1q/learn_spark_via_databricks_community_edition_or/,0,False,False,False,False
18v2o1m,Comparing Webhooks and Event Consumption,,5,1,Glittering_Bug105,2023-12-31 07:43:35,https://medium.com/@memphis-dev/comparing-webhooks-and-event-consumption-38225e3b5d9d,1,False,False,False,False
18t0suy,"Best way to Create MVP AWS Data Pipeline from Salesforce to Redshift, for Beginner","Hey all, I just got hired into a solar sales company, I'm one of two technical roles in the business. My career up to now has been in data analysis and data science, but I've been tasked with creating an entire data pipe to take our data from Salesforce to something automatable/queryable in a BI tool. This means I'm working way upstream of everything I know, but I was told the company had to pick between an engineer and an analyst, and chose an analyst. 

After researching I think I want to put as much as I can into one cloud service, so AWS seems good enough for that. The company is pretty small, so I don't need to do anything that scales incredibly well, once we get a real data engineer in a year or two I'll likely work with them on transitioning to something that works better. I'm currently prioritizing getting something usable but not perfect off the ground ASAP.

With my limited research, I believe the best way to do this is as follows:

Salesforce -> Amazon Appflow -> S3 -> AWS Glue for ETL -> Redshift

There will also be data coming into S3 from other sources, some of which will also go into Redshift (ex: payment data).

Is there a simpler path that I could be following to get to a warehouse entity, or is this as bare bones as it gets?

Additionally, is anyone aware of good tutorials on how to chain AWS tools together? I find great resources for individual tools but not generally how to use them in tandem. Other resources that you've liked that covers high-level data engineering would also be welcome.

Thanks in advance!",4,3,Clockworked47,2023-12-28 18:27:05,https://www.reddit.com/r/dataengineering/comments/18t0suy/best_way_to_create_mvp_aws_data_pipeline_from/,1,False,False,False,False
1abnlf1,Data analyst to data/ML engineer?,"I'm so lucky, I landed my first full time analyst job at my dream company (large healthcare system). The pay isnt great, but the benefits and work/life balance are incredible. I now realize it's a great place to work if you're settling down, not so great if you're looking to move up the ladder in your mid 20s. 

I do a mix of ETL scripting (pandas, sqlalchemy, pyodbc) and mundane accounting with Excel. 

I originally had aspirations of landing a data engineering role 1-2 yrs later, but I think I would want ML engineer roles instead. I want to go into risk analytics for health insurance/healthcare systems, there's a lot of opportunities to use advanced stats and ML there. The data I currently work with doesn't have any clinical info to determine risk, so I'm limited to data engineering tasks. I also want to be more involved in delivering business insights, rather than strictly backend data stuff. 

Any ML engineers or data engineers here to share their thoughts?",3,0,velimino,2024-01-26 17:19:45,https://www.reddit.com/r/dataengineering/comments/1abnlf1/data_analyst_to_dataml_engineer/,0,False,False,False,False
19fgexg,How to transition from GUI DE to more modern stack?,"Background: I have a degree in mechanical engineering and have 2 years under my belt as a Business Intelligence developer on a ETL team. We mostly use Informatica and SQL server. I have got a certification with Azure DE associate but no projects at work.

I’d like to transition to a role with more modern/cloud tools. My end goal is to be a ML engineer. I am considering doing GT or UIUC masters in CS. But for now, what are some tips to make you more competitive for roles with modern tooling when you only have experience with Informatica/on-prem architecture? Any tips on resume or things I can do to stand out?",3,9,putt_stuff98,2024-01-25 18:40:38,https://www.reddit.com/r/dataengineering/comments/19fgexg/how_to_transition_from_gui_de_to_more_modern_stack/,0,False,False,False,False
19fbd0n,Why would anyone pick Cloudera Data Platform over Amazon EMR?,We're moving our workloads to the public cloud and honestly I cannot think of a single advantage of using CDP over EMR. Has anyone had any experience with this? Does anyone have any good reasons to stick with the Cloudera offering? Also Cloudera sound like they're way behind on everything when it comes to the Cloud. Databricks at least promises proprietary optimizations to Spark but Cloudera offers nothing that EMR or Databricks already do better.,3,11,oontkima,2024-01-25 15:04:13,https://www.reddit.com/r/dataengineering/comments/19fbd0n/why_would_anyone_pick_cloudera_data_platform_over/,1,False,False,False,False
19faisc,Deltalake using Azure Synapse Analytics/Workspace is good?? Please advice,"
hey all.. I understand Databricks is the best choice when it comes to these Deltalake jargon.. But bear with me, in this case I'll only focus on Synapse.

Current situation, using Synapse Pipeline for Orchestration and Ingestion from source (copy activity), then data transformation using Spark Pool with pyspark.. Result is in delta table format.. Now questions:

1. I noticed in dbricks we have SQL Endpoint that able to connect PBI to adls delta table.. Similar things non-existent in Synapse? Do we really need to push data to Dedicated pool to be able to use these data in PBI? If so, what's the point to have uncosumable delta table in Synapse?

2. Have heated args with colleague about whether compute in Spark Pool (coding + sql) vs Dedicated Pool (pure SQL StoredProc styles).. I'm not a big fan of Datalake with dwh-table styles, as datalake should be an open table format which can help user with so many possiblities such as ML/AI, programming-styles development, sql-styles development, etc.. Not only limiting people with sql. Then we can build dwh on top of Datalake (like gold layer or whatever those jargons are).. Proof me if I was wrong? 

fyi we have 3 big ERP as sources, such as: Oracle & SAP.. Multiple DBs and Hundreds of tables as sources..",4,12,BumbleBeeBumbleBoo,2024-01-25 14:25:55,https://www.reddit.com/r/dataengineering/comments/19faisc/deltalake_using_azure_synapse_analyticsworkspace/,0,False,False,False,False
19erzhn,What courses or certificates are intresting to get as junior?,"I'm a junior with 1.5 years of experience, and I generally enjoy my assignments. While the work can be challenging at times, I believe it's an integral part of learning in the IT sector. We have a solid stack and strive to incorporate good practices whenever possible. Our team also utilizes some specific technologies.

I recently inquired with my supervisors about potential next steps, and they suggested gaining more experience before pursuing additional AWS certifications (I currently hold 2). Fortunately, I have the flexibility to attend relevant courses or events, with the company covering the associated costs.

For those with more experience, could you please suggest worthwhile investments of time and money that align with career growth in the data engineering field?",3,4,Warper27,2024-01-24 21:34:17,https://www.reddit.com/r/dataengineering/comments/19erzhn/what_courses_or_certificates_are_intresting_to/,0,False,False,False,False
19eqaen,tools to query s3 compatible on-prem object stores,"I'm looking for tools/framework to query s3 compatible object data stores hosted on prem. I know trino/presto can provide sql layer on top of object storage, are there any other simple and efficient tools available?",3,2,przx2,2024-01-24 20:24:44,https://www.reddit.com/r/dataengineering/comments/19eqaen/tools_to_query_s3_compatible_onprem_object_stores/,1,False,False,False,False
19edyrx,Apache Superset: Percentages and Pivot table with heatmap,"Hello everyone, i have some questions for you about Apache Superset.

  
The my first doubt is how to use percentages in the tool. I'll explain it better: basically i'm trying to values in a bar chart in percentages instead of values (the total being the sum of values).

My second doubt instead is on displaying data from a pivot table into a heatmap. Basically, since i nedd rows from the table to be values on the Y axis i need to transform my data by using a pivot table, but i'm unable to use that transformed data into my heatmap, so i'd need to use custom SQL queries, which i'd like to avoid.

Thanks",3,1,Disastrous-Tune2889,2024-01-24 10:22:23,https://www.reddit.com/r/dataengineering/comments/19edyrx/apache_superset_percentages_and_pivot_table_with/,1,False,False,False,False
19ebd3a,Which is the way-to-go database for Data Science applications,"A newbie here, so please, have mercy.

I am just trying to migrate from the amateur `pickle` and `numpy.save()` to a db-based approach for my Data Science projects. However, I have no idea which db I should start. What would be your advice?",2,5,Puzzleheaded_Egg_184,2024-01-24 07:13:29,https://www.reddit.com/r/dataengineering/comments/19ebd3a/which_is_the_waytogo_database_for_data_science/,0,False,False,False,False
19e7pf8,Must watch if you have ever been confused/frustrated with Python paths and packaging.,,4,0,mjgcfb,2024-01-24 03:40:32,https://www.youtube.com/watch?v=v6tALyc4C10,1,False,False,False,False
19e2m1m,How would you do it? Make my plan better.,"So I'm currently doing Masters in Ai and Data and I don't have a CS bachelor degree not even related to it. It's in Finance/Business. 

I have a few months till I graduate. I have had few projects using Python, I'm familiar with SQL. I know I need to get better at both of them. 

I also wanted to do the Microsoft DP-203 and the AWS Data Engineer Associate / Solution Architect. 1. To fill cv and 2. To be familiar with cloud. 

What else should I spend the next few months doing? And do I have a chance at landing my first job soon or no? Is this enough or not? 

I'm in London (UK) if that helps.",3,4,Timeframe98,2024-01-23 23:37:51,https://www.reddit.com/r/dataengineering/comments/19e2m1m/how_would_you_do_it_make_my_plan_better/,1,False,False,False,False
19dor5g,How do you decide on resource sizes?,I'm trying to figure out how to size resources for our data pipeline and warehouse. My boss wants me to calculate what each pipeline needs in terms of compute and RAM. Any ideas on how to evaluate and track these requirements? Would love to hear what's worked for you.,3,5,Kindly-Screen-2557,2024-01-23 13:45:40,https://www.reddit.com/r/dataengineering/comments/19dor5g/how_do_you_decide_on_resource_sizes/,1,False,False,False,False
19dmm6t,Data Vault or Dimensional Modelling?,"Hi,

I am working on data engineering project, and am thinking what data modelling technique to use? Can someone give a suggestion?

Thank you very much for reading and helping.",3,3,Aggressive-Nebula-44,2024-01-23 11:45:55,https://www.reddit.com/r/dataengineering/comments/19dmm6t/data_vault_or_dimensional_modelling/,0,False,False,False,False
19djcsf,New player to the game,"I recently started as a Data Analyst after a career change. As with many others, I was quickly identified as a candidate for a JDE role. Ever since (~2 months) I have been mainly working with spatial data in FME and handling pipelines and dashboards.

My organization have suggested I take some training on spatial data, but I also want to know where else I can apply myself to progress becoming a Data Engineer. Any tips on specific skills, certifications, or experiences that could help me? I'd say I have moderate ability with SQL and basic Python understanding. Should I be putting all my time in data handling / manipulation in Python? Should I be looking at any other ETL tools (we currently use FME exclusively). Should I be considering database managent?? 

Any help and advice is really appreciated!",3,1,BrittleTupperwareBox,2024-01-23 07:57:46,https://www.reddit.com/r/dataengineering/comments/19djcsf/new_player_to_the_game/,0,False,False,False,False
19cq4gk,How to structure a data pipeline repo for pyspark jupyter notebooks?,"I am planning to build a data pipeline for a new project, which would be in pyspark sagemaker notebooks Technologies used as below
Orchestration: Airlfow
Storage: S3
Final transformed tables will be created in athena.

How would you structure a git repo that's written in pyspark notebooks and with a dag folder. We are also looking to implement CI/CD in the future.

Would like to hear all your suggestions and any github repo examples would be highly appreciated.
 Thanks!",3,0,arunrajan96,2024-01-22 07:23:39,https://www.reddit.com/r/dataengineering/comments/19cq4gk/how_to_structure_a_data_pipeline_repo_for_pyspark/,1,False,False,False,False
19cpy1e,GCP Professional Data Engineer new pattern,"I've scheduled my GCP PDE exam for 30th Jan, but heard that Google has significantly changed the exam pattern in November. Has anybody appeared for the exam after the pattern change? Are there any dumps available as per the new pattern? What new topics should I focus on for the exam?",3,1,bashed_it,2024-01-22 07:11:15,https://www.reddit.com/r/dataengineering/comments/19cpy1e/gcp_professional_data_engineer_new_pattern/,1,False,False,False,False
19c3sst,How valuable are Certifications?,"I have the Databricks ML / DE Professional Certifications.

I'm currently a DE wanting to move to a better (higher paying) DE position or an MLE position. How are the certifications regarded by the companies, especially at FAANG? If they are not that well regarded, what else should I be focusing on to transition to an MLE position?",3,8,asozers,2024-01-21 13:49:25,https://www.reddit.com/r/dataengineering/comments/19c3sst/how_valuable_are_certifications/,0,False,False,False,False
19bb6ru,parallel ingestion in snowflake!?,"In on of my project, I have  a stored procedure in snowflake that is generating ingestion query of around 100 raw files into around 20 tables. Right now we are using sample Data, each one has few thousands rows. And ingestion time is around 10 minutes. 
But i m sure in production environment each file will contain millions of rows any my estimate is that will takes 30 minutes to ingest. 

Right now, I am running all ingestions queries in sequential manner, one by one. But I want to ingest Data parallely in asynchronous manner/Mutlithreading whichone is the right term, I have no idea. 
Inside snowflake, I m using python which has features to do parallel processing. But is it possible to do so in snowflake. Or any theoritical modification, you are thinking to suggest. 

From business perspective it's not necessary, since these are DWH layer and processing is of batch type. I m just exploring probable options from learning perspective. 

Thanks in advance. Any lead will be appreciated.",3,20,asud_w_asud,2024-01-20 12:56:27,https://www.reddit.com/r/dataengineering/comments/19bb6ru/parallel_ingestion_in_snowflake/,0,False,False,False,False
19b8e6v,Job title... What do you think?,"Hey all, quick question.

I've been a DE roughly 3 years and just applied to a senior DE role at a consultancy and was pleasantly surprised to get offered the role. Hurrah.

Anyway I received the contract and the title was technical project manager. I love being a data engineer and I don't want to not be close to the code or etl. In your experience if some one had technical project manager on their cv would you discount that as DE experience? Or are the roles synonymous?

Thanks",3,5,tcfcfc,2024-01-20 09:53:20,https://www.reddit.com/r/dataengineering/comments/19b8e6v/job_title_what_do_you_think/,1,False,False,False,False
19aq5eh,Origins of NumPy by its creator Travis Oliphant,,3,0,dnulcon,2024-01-19 18:35:46,https://youtu.be/nnPAAMbUWAM?si=cn3KFUIuNE0hxDSw,0,False,False,False,False
19ap18l,Piecing together sessions to analyze online user activity,,2,0,JanethL,2024-01-19 17:50:10,https://www.youtube.com/watch?v=T94zNPW-Meo,0,False,False,False,False
19aeowz,Do you think Data Engineers should have point solutions for Data Observability and Catalog OR prefer a consolidated solution which is seamlessly integrated?,"Speaking to peers, few teams have deployed data - catalog already , however from Data Observability perspective having a different tool can be cumbersome since both talk to each other.  
   
Let me know what you think?  
I am comparing: GE + Open-Metadata, decube.io (its unified), Atlan + MC",3,2,de4all,2024-01-19 08:48:45,https://www.reddit.com/r/dataengineering/comments/19aeowz/do_you_think_data_engineers_should_have_point/,1,False,False,False,False
19a7tnt,Seeking Advice to Enhance Data Engineering Pipeline,"Hello fellow data enthusiasts! 👋

 I recently joined a government organization as a data analyst on the business side, and a significant part of my role involves writing Python scripts for ETL processes. I'd love to get your insights and advice on how I can level up my data engineering pipeline, as I'm keen on enhancing efficiency and exploring new possibilities. 

**Current Setup:**

* Our primary data resides in an Oracle environment (Production Repo)
* I've developed several Python scripts to extract, transform, and load data from the production repo to our SQL Server data warehouse.
* The ETL scripts are scheduled to run daily using .bat scripts on Windows Task Scheduler, and I diligently maintain logs for transparency and troubleshooting.
* To handle data from external sources (like Vendor portals), I've implemented Selenium scripts, also scheduled for daily execution. These scripts fetch data from the portals and load it into our SQL Server data warehouse.
* For real-time data needs, I've created Power BI dashboard reports connected to our data warehouse via the company gateway. These reports refresh daily, providing up-to-date insights.

**Current Development Environment:**

* Operating on a Windows virtual machine.

**Seeking Advice:** 

In addition to my current setup, I initially aimed to enhance my data engineering pipeline further by:

* Installing Windows Subsystem for Linux (WSL) on my Windows server.
* Scheduling scripts using cron tabs from a Docker container.
* Exploring the use of PySpark for data transformations to optimize script run time.

However, I've hit a roadblock – the IT team has informed me that I can't install WSL on the Windows machine due to virtualization restrictions. Now, I'm contemplating my next steps and would love your insights? ",3,10,tragediest,2024-01-19 02:11:27,https://www.reddit.com/r/dataengineering/comments/19a7tnt/seeking_advice_to_enhance_data_engineering/,1,False,False,False,False
199wj94,Importance of Dynamic Programming and Graphs in DE Interviews,"For those who have undergone multiple interviews in their careers, how frequently were you asked questions related to Dynamic Programming and Graphs? I'm interested in understanding the prevalence of these topics in the interview process. Feel free to share your insights and experience.
Thanks!",3,1,PunctuallyExcellent,2024-01-18 18:10:39,https://www.reddit.com/r/dataengineering/comments/199wj94/importance_of_dynamic_programming_and_graphs_in/,1,False,False,False,False
199oy12,Career shift planning advice (Software Eng to DevOps/SRE to Data Eng?),"I'm a software developer in a developing country. I've been working remotely for American companies for the past 8.5 years as a software engineer, but kind of in a niche industry that over the years I've grown to really dislike - so I've been thinking of switching industries for a while now. My main work line is in PHP/Javascript and I've been slowly but steadily moving further to the back-end, trying to get more work in the CI/CD pipelines, data modeling, database management and such.

Within the companies I've worked for, I've become known for being one of only people around who understands data/databases/search in a deeper way and I got a great deal of exposure to both MySQL, Elastic stack and Redis. I do have an Elastic Engineer certification and a fair amount of knowledge of Bash/Python, Linux, DevOps, CI, containers and such, but no real on-the-job experience with distributed computing/data tools.

Early last year I got laid off and started applying for DE jobs, but in the era of ATS I didn't land a single interview in this area and ended up taking another job in the same industry (a good one, at least).

I've been around the block and I'm very confident in my ability to learn quickly on the job but I'm now 40 and I have two kids and a career, which means my time for on-the-side projects is really limited and I can't take a dip in salary to start over as a junior (though mostly because of location, my salary isn't really that high, I'm currently at 90k).

If you were in my shoes, how would you go about trying to land a DE job? Do you think routing through DevOps/SRE could be a good alternative?",3,1,Time_Simple_3250,2024-01-18 12:22:11,https://www.reddit.com/r/dataengineering/comments/199oy12/career_shift_planning_advice_software_eng_to/,1,False,False,False,False
199hke6,Unstructured data,"I feel in the next few years solutions will come out to do analysis on more unstructured data (no not just log files), I mean video, audio, text and doing sentiment analysis on it to extract data and do data quality check to train a LLM

Anyone doing this in their current data engineering job? Any tech stacks to do this that you recommend?",3,3,Guilty-Commission435,2024-01-18 04:31:03,https://www.reddit.com/r/dataengineering/comments/199hke6/unstructured_data/,1,False,False,False,False
1998e1x,SSH Tunneling for Secure Postgres Replication,"At [PeerDB](https://peerdb.io/), we recently added this new feature of SSH Tunneling to securely connect and replicate data from your Postgres Database to Data Warehouses. Sharing the blog that talks more about this feature - [https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication](https://blog.peerdb.io/ssh-tunneling-for-secure-postgres-replication) We would love to hear your feedback!  


https://i.redd.it/988cru7ik2dc1.gif",3,0,saipeerdb,2024-01-17 21:31:39,https://www.reddit.com/r/dataengineering/comments/1998e1x/ssh_tunneling_for_secure_postgres_replication/,1,False,False,False,False
1994v41,Reconsidering an Interview Opportunity After Initial Rejection: Seeking Advice on Whether to Proceed,"I applied for a DE position at a major financial services company, and after the initial interview, they chose another candidate. Two months later, the job was reposted, and somewhat impulsively, I reapplied. Now, I've been contacted for another interview. I'm uncertain about proceeding because I fear the initial panel's opinion might not have changed. Would it be advisable to call and cancel the interview?",4,10,Greckol,2024-01-17 19:12:00,https://www.reddit.com/r/dataengineering/comments/1994v41/reconsidering_an_interview_opportunity_after/,0,False,False,False,False
1992yx6,One billion row challenge - Dask vs. Spark,"I was inspired by the [unofficial Python submission to the 1BRC](https://github.com/gunnarmorling/1brc/discussions/62) and wanted to share an implementation for Dask and PySpark: [https://github.com/gunnarmorling/1brc/discussions/450](https://github.com/gunnarmorling/1brc/discussions/450)  
Dask took \~32 seconds, while Spark took \~2 minutes.

Amongst the other 1BRC Python submissions, Dask is pretty squarely in the middle. It’s faster than Python’s multiprocessing (except for the PyPy3 implementation) and slower than DuckDB and Polars. This is not too surprising given Polars and DuckDB tend to be faster than Dask on a smaller scale, especially on a single machine. I was actually pleasantly surprised to see this level of performance for Dask on a single machine for only a 13 GB dataset. This is largely due to a number of recent improvements in Dask like:  
\- Arrow strings  
\- New shuffling algorithms  
\- Query optimization  


Though many of these improvements are still under active development in the dask-expr project, Dask users can expect to see these changes in core Dask DataFrame soon.  


More details in this blog post: [https://blog.coiled.io/blog/1brc.html](https://blog.coiled.io/blog/1brc.html)",3,1,dask-jeeves,2024-01-17 17:57:22,https://www.reddit.com/r/dataengineering/comments/1992yx6/one_billion_row_challenge_dask_vs_spark/,1,False,False,False,False
1991uni,Intro to SQL Indexes,,3,0,dataengineeringdude,2024-01-17 17:14:08,https://dataengineeringcentral.substack.com/p/intro-to-sql-indexes,0,False,False,False,False
19892yb,Choicing beetween DE vs DS,"Why starting as a DE might be better than DS, considering maybe going to MLE in the future? I like the DE area and I think maybe strong skills like a DE combined with enough DS could be the key and could it be that DE will be more valued in the future as DS is today?",3,4,M4loka,2024-01-16 17:53:10,https://www.reddit.com/r/dataengineering/comments/19892yb/choicing_beetween_de_vs_ds/,0,False,False,False,False
19863jk,Network route visualization using pyvista and osmnx,"&#x200B;

[Network route visualization using pyvista and osmnx](https://preview.redd.it/q0n4jfveqtcc1.png?width=1357&format=png&auto=webp&s=ba5fa1ca2de8fece3098c795d168d5dd5a9d1343)

[Network route visualization using pyvista and osmnx](https://spatial-dev.guru/2024/01/14/network-route-visualization-using-pyvista-and-osmnx/)",3,0,iamgeoknight,2024-01-16 15:53:19,https://www.reddit.com/r/dataengineering/comments/19863jk/network_route_visualization_using_pyvista_and/,1,False,False,False,False
197sqkq,[Advice] - Computer Science Degree,"I'm currently in honey moon with de (maybe some of you been here idk) and also in the final year of my Bachelor's degree in Civil Engineering. Started working as a Data Engineer in a startup, and this experience has sparked a serious interest in computer science for me. I'm thinking about pursuing a degree in Computer Science after I graduate. From what I've looked into, it would take me about two more years to complete, which means I'd have dual degrees in Civil Engineering and Computer Science by 2026.

I seek for advice, will I be wasting my time?",3,4,gabiru97,2024-01-16 03:27:54,https://www.reddit.com/r/dataengineering/comments/197sqkq/advice_computer_science_degree/,0,False,False,False,False
197rzuz,Centralized Metrics in DWH,"We have built a data warehouse in Snowflake for whole department . There are certain key metrics that we want to produce and control the definition to maintain consistency of those numbers across the organization.
I don’t want to store only aggregated numbers, as it will lose slicing/dicing ability. I want to create a generic framework that we can use for new metrics too.
What kind of framework or approach have you guys used or recommend for such use case?
TIA",3,1,discoveringlifeat39,2024-01-16 02:51:30,https://www.reddit.com/r/dataengineering/comments/197rzuz/centralized_metrics_in_dwh/,1,False,False,False,False
1977b3s,Interview pattern for data engineers in product based companies?,"Hello, I am planning to switch in 8-12 months. Currently working in telecom based company in gcp services. I want to know interview pattern for data engineers in good product based companies like below.
Altassian
PepsiCo
Gojek
Wallmart
Intuit
BP
Same level companies.

1. No of rounds?
2. Is DSA involved?
3. Coding round on which language.

Please share your experience. It will help a lot.",4,1,TheErenYeager03,2024-01-15 12:11:06,https://www.reddit.com/r/dataengineering/comments/1977b3s/interview_pattern_for_data_engineers_in_product/,0,False,False,False,False
1974l0z,Managed Dagster Hosting,"I am curious about the ways of deploying Dagster which requires the least amount of knowledge and hassle to manage (load monitoring, version updating, etc.) the underlying infrastructure, i.e. focus on writing pipelines in Dagster. Setting up a managed K8s cluster and deploying Dagster there seems error-prone for a small unexperienced team.

Logically, Dagster Cloud can be used to get a managed version of Dagster. However, for us, its pricing model is too expensive.",3,11,WeddingIndependent30,2024-01-15 09:15:05,https://www.reddit.com/r/dataengineering/comments/1974l0z/managed_dagster_hosting/,0,False,False,False,False
19744b2,Best way to copy specific Sharepoint files using Azure?,"Hi,

I have a sharepoint site with thousands of documents in nested folders. I need to find a way to search in those directories and get only the documents that start with ""aaa\_"" (ideally also with some sort of lastModified filter but that's not mandatory).

These files need to be copied to an azure storage container, keeping the folder structure it has in Sharepoint. 

I've tried connecting ADF to sharepoint through an app registration but that gives an error, presumably due to permission. But even if I could connect I think it's only possible to copy the files using a direct path (while I need to find them dynamically).

Does anyone have an idea on how I could best do this using Azure and/or Python?

&#x200B;

Thanks for reading.",3,4,drollerfoot7,2024-01-15 08:42:59,https://www.reddit.com/r/dataengineering/comments/19744b2/best_way_to_copy_specific_sharepoint_files_using/,0,False,False,False,False
196funh,Managing Salesforce Reporting (in DBT),"My company requires reporting on information within salesforce.

They want something like # open leads, # closed leads and # converted leads on a monthly basis. My issue is that often this data is not immutable: a lead (or an opportunity for example) might be closed, but then later on reopens since the lead/opp is now interested in the product.  


This would generally change data retrospectively.  
Is the only way to do a complex query using lead and opportunity history tables and creating an events based data model?",3,5,casematta,2024-01-14 13:26:24,https://www.reddit.com/r/dataengineering/comments/196funh/managing_salesforce_reporting_in_dbt/,1,False,False,False,False
196beob,How to replace AWS NAT Gateway being used by Databricks with VPC endpoints?,"I have upgraded Databricks Community Edition to Databricks' free trial on AWS free tier. Since NAT Gateway was not part of the free tier and I am being charged, I deleted it. This results in me not being able to create clusters.

I've seen suggestions about replacing it with VPC endpoint, but I have zero knowledge about networking so I don't know how to do it myself and I can't make sense of the tutorials I see on the internet how to customise it for Databricks use. Does anyone know any resource that I can follow so I can setup the needed VPC endpoints and for Databricks to work?

Or if there are better options than VPC endpoints which a compete networking noob can easily follow, that would be great too.",3,9,AxenZh,2024-01-14 08:38:57,https://www.reddit.com/r/dataengineering/comments/196beob/how_to_replace_aws_nat_gateway_being_used_by/,1,False,False,False,False
195vooq,Collibra workflow deployment via Jenkins,"Have anyone tried to automate the workflow deployment process using Jenkins. I want to migrate a workflow from QA to production environment, provided I should be able to modify the workflow variables.
Is there a way to do this?",3,0,rags1230,2024-01-13 19:17:39,https://www.reddit.com/r/dataengineering/comments/195vooq/collibra_workflow_deployment_via_jenkins/,1,False,False,False,False
194svgx,Need Serious Help/Suggestions for my Career.,"Hi everyone, thank you for reading this post.

I'm currently working as a Data Engineer, primarily in ETL, with 1.8 years of experience across 3 projects using Azure Data Factory (ADF). I'm proficient in ADF, have intermediate SQL skills, and basic knowledge of Python. I'm contemplating a job switch and facing some uncertainties. Here are my main concerns:

1. Choosing between Snowflake and Databricks, leaning towards Snowflake due to my strong SQL skills.

2. Considering online master courses (e.g., Trendy Tech DE Master Program) for a structured learning approach and to stay motivated.

3. Seeking advice on additional areas to focus on.

4. Unsure if learning Python with DSA is essential and whether online or offline courses are preferable. PS - I don't like to code and also want to be in DE field but just going with the demand.

Your insights would greatly help in easing my confusion and career pressure.

",3,3,Own_Zookeepergame256,2024-01-12 11:27:57,https://www.reddit.com/r/dataengineering/comments/194svgx/need_serious_helpsuggestions_for_my_career/,0,False,False,False,False
194k0s8,Low-Volume ETL,"Please help me solve a fight between the architects on my team....

Here's the situation - we have a set of data that one of our teams currently work with as spreadsheets. There are about 100 different data sources involved. 90% of them come from other databases, the remaining 10% come in as flat files that are transformed and then exported as spreadsheets using some old code that we can't do much with.

The data changes infrequently, anywhere from monthly to yearly. The flat files need some transformation and manipulation, but the others are basically straight copy.

We want to take all of these files and move them into something more sophisticated so the team can use tools like PowerBI etc on them.

One architect wants to go a pretty traditional route and do Python loader scripts into an Azure SQL or Postgres database. One wants to use Azure Data Factory to move the data to a database. One wants to use Azure Synapse/Azure Databricks. 

The Python scripts feel oldschool but solid, and the Synapse solution feels overpowered and overpriced for our needs.

WWYD?",3,9,edugeek,2024-01-12 02:34:36,https://www.reddit.com/r/dataengineering/comments/194k0s8/lowvolume_etl/,1,False,False,False,False
1948ba1,Unsure of my skill level?,"Hi All,

I'm currently a mechanical engineer working in aerospace, looking to transition into data engineering. However, I'm unsure at what level I should be ""selling"" my skills.

I've basically been doing some kind of data analyst/software engineer/data engineer/data scientist job for the past two years, but I don't really have anyone experienced around to compare myself to, so I don't know if I'm competent or totally inept.

I'm happy using Python, pandas, and scikit-learn to do some transforming, cleaning, and machine learning predictions. I also have decent experience with our in-house tool (Palantir Foundry) using PySpark and PostreSQL, and creating some simple dashboards using HTML / CSS / JS.

I've also developed some in-house structural analysis tools using Python, and I'm familiar with object oriented programming, data structures, algorithms etc. but again I don't know if my code is great or total shit as a competent software engineer has never reviewed it.

I'd like to move to a job where I have seniors to learn from, but I'm not sure if I'm ready. I also have no idea how much my walled-garden experience with Palantir Foundry would carry over to things like AWS, Azure etc.

Is there a certain level of competence I should be looking for before trying to move over? I don't want to blag my way into a job and get fired after a couple of months.

Thanks",3,5,anonymous_lurker_01,2024-01-11 18:13:42,https://www.reddit.com/r/dataengineering/comments/1948ba1/unsure_of_my_skill_level/,1,False,False,False,False
1946ghu,Rant! How functional group and bad management affect Data Engineer,"I am working as ETL developer since 4 months(fresher) in a service based company, with tech stak snowflake, IICS. 
First of all IICS is the worst tool, they are very slow and very unpredictable. Idk about powercenter, but informstica cloud is so much slow. We have to face constant issue saving a taskflow. , and updating a taskflow. 

But thank god I got lucky,  i got chance to use snowflake that is great software. one day my project leader was absent, and a BRE template was needed. I took the weekend that was able to process join condition also. 
Then I was asked to created a stored procedure for a part of project that will process around csv data from around 50 vendors having 4 file type from each vendor I.e 200 files. And load it into staging layer in 20 channels. That's also not a straight pass, many to one mapping. I have successfully implemented that procedure. 

Damn my nightmare begins now, first client changed all the  STTM, then our functional group who  made grave mistake they, followed every instructions blindly without analyzing sample data from client. Then data architect changed all the data model. I improvised the code made up the changes. Forcefully developed code to process STTM that will great data model accordingly. My manager was asking to check thoe file mannually 😵‍💫 test those ingestion. Disagreeing with him, I automated testing process. I have to debate over that for 2 hours with testing lead. Why is it necessary. 
After that they are changing vendor to channel map constantly (following sample data). Basically vendor to channel map is being done on basis of a column. And they are undecisive of those map they are changing it every few days. Simple they say you have automated it. Do it! But those automated process is not so formalised. I had developed to easy my work not to automate the process. You should have give more time. . It takes lot of concentration to avoid those unforseen 

I am constantly asking my manager that we are following wrong approach since I have analysed sample data from that on distinct value of that particular column is mapped into each channel. Which is the trigger for that orocedure. When in product environment it will break down graduall. In real data there may be many code. But my manager, god knows why is not forwarding that input. All are acting as per wish. Then they expect refined data. God knows what they do. 🥲🥲🥲",3,4,asud_w_asud,2024-01-11 16:59:47,https://www.reddit.com/r/dataengineering/comments/1946ghu/rant_how_functional_group_and_bad_management/,0,False,False,False,False
19466ch,Considering a Shift from GCP to AWS: Data Engineer Seeking Advice on Transitioning and Market Opportunities,"Hey fellow Redditors,

&#x200B;

I've been working as a data engineer with expertise in Google Cloud Platform (GCP) for a while now. Despite my proficiency in GCP, I've been noticing fewer opportunities in the market and I'm contemplating expanding my skill set to include AWS.

&#x200B;

For those who have made a similar transition or have experience with both platforms, how familiar is AWS for someone who knows GCP well? What challenges did you face, and how did you overcome them? Are there any specific resources or learning paths you recommend for someone in my position?

&#x200B;

I appreciate any insights or advice you can share. Thanks in advance!",3,7,TheOtherNormalOne,2024-01-11 16:48:10,https://www.reddit.com/r/dataengineering/comments/19466ch/considering_a_shift_from_gcp_to_aws_data_engineer/,0,False,False,False,False
1945pt7,How do people manage data security at scale when each vendor adds their security layer on top?,"Each data platform I can think of seems to handle database security similarly: They require privileged access to your database/warehouse through an ""admin"" account and then add their security layer on top. This means no central access governance and no centralized logs, making it hard to put together a picture of who is accessing what. What are people's solutions to this and how do you manage this?",3,0,bk1007,2024-01-11 16:28:54,https://www.reddit.com/r/dataengineering/comments/1945pt7/how_do_people_manage_data_security_at_scale_when/,1,False,False,False,False
1941ulb,Ingesting to Parquet/Iceberg/object storage from Java,"What are folks using to ingest large tabular files from Java into Parquet and friends?

The data is ultimately going to be reprocessed/joined with other tables and queried through an engine like Trino.

Today a dataset will come in as various tabular formats, our users clean and map it using the Java backend, and it's stored in PostgreSQL. I love postgres, but we're bumping up against size and cost limits for some of the larger datasets; each dataset gets its own postgres instance while it's actively being used. I'd like to try moving away from it and toward the Iceberg ecosystem with object storage and a caching layer.

Once the data is already in object storage, or in a relational database, there's lots of ways to convert between formats using Trino or Spark, and tons of benchmarks and guides. But what's the best way to get it in there in the first place? How is big data formed?

I've used ParquetWriter and Avro GenericData.Record a bit in the past. It's quite low-level. I'd like to have a bit of control over how the Parquet files are sorted and partitioned, either during the ingestion process or with some post-processing. I'll bet you can do anything with ParquetWriter and the Iceberg library, but getting it right, so the sorting/partitioning I do in the app, matches what I put in the metadata, would be hard, and the kind of thing I'd normally want a DBMS to handle.

I was wondering about bulk loading into Trino via JDBC. I haven't been able to find performance numbers or guidance on whether this is a sane approach. I'm open to almost anything: using ParquetWriter directly, or DuckDB, or a JDBC connection to Trino or StarRocks, whatever makes sense.",3,4,t0astix,2024-01-11 13:35:07,https://www.reddit.com/r/dataengineering/comments/1941ulb/ingesting_to_parqueticebergobject_storage_from/,1,False,False,False,False
193vaqk,accessory language to python for DE,"what would be the best language to learn as an accessory, the main reason being, I want to learn a low level language to understand Software Engineering better, that would benefit my DE career as well. ",3,2,alphamalet997,2024-01-11 06:42:34,https://www.reddit.com/r/dataengineering/comments/193vaqk/accessory_language_to_python_for_de/,1,False,False,False,False
193uvbq,How do you document systems and pipelines?,"I work on the azure stack (sql, adls, adf, databricks) with a team of about 7 total plus some contractors/consultants at time. 

In the past year I have put a lot of effort into creating documentation not only for our environment but also the systems and pipelines we develop and maintain. At the moment these are just word docs on our SharePoint that take a fair bit of time to manage and maintain especially when everyone does it slightly differently. 

How have others approached this? What has worked/not worked? What key information is essential for a pipeline/system? 

[View Poll](https://www.reddit.com/poll/193uvbq)",3,4,Agitated-Western1788,2024-01-11 06:16:27,https://www.reddit.com/r/dataengineering/comments/193uvbq/how_do_you_document_systems_and_pipelines/,1,False,False,False,False
193dfzd,Advice for Junior Data Engineer job applications,"Hi all,

I have a graduate degree in Data Science and a few months work experience as a Data Engineer in the past, but have not worked in the last 12 months.  
I am currently applying for Junior DE roles and was wondering if anyone had any advice that could help me in my search;

* How necessary is it to have a portfolio of projects included with my applications? and can anyone advise me on what type of project I could do / what to skills to focus on in the project
* I am currently doing some LinkedinLearning DE courses, are these worth doing at all? are there any other courses you would reccomend
* any general advice you could share

Thank you!",3,4,niaall12,2024-01-10 17:16:04,https://www.reddit.com/r/dataengineering/comments/193dfzd/advice_for_junior_data_engineer_job_applications/,1,False,False,False,False
1938bix,How to learn ETL best practices,"I work at a small startup that has implemented some very basic pyspark scripts orchestrated by step functions.
I want to get an understanding of best practices of pyspark processes (how to structure files, optimise pipelines etc) and I'm not sure where to look.",3,0,casematta,2024-01-10 13:30:45,https://www.reddit.com/r/dataengineering/comments/1938bix/how_to_learn_etl_best_practices/,1,False,False,False,False
19384vx,Adding Kafka to my organization. What should be built on top?,"Hey Community,

Adding Kafka to my organization. What would you build on top to make your developers' lives easier? in a sense of self-serve, APIs, boilerplates, out-of-the-box services, etc.

Thank you!

&#x200B;",3,5,yanivbh1,2024-01-10 13:21:12,https://www.reddit.com/r/dataengineering/comments/19384vx/adding_kafka_to_my_organization_what_should_be/,1,False,False,False,False
1935ppc,Just Got Certified as Databricks Data Engineer Pro - Aiming now for the spark badge. Any available voucher codes?.,"Hi guys, 

I recently passed the exam for **Databricks Data Engineer Pro certification** 

[https://www.databricks.com/learn/certification/data-engineer-professional](https://www.databricks.com/learn/certification/data-engineer-professional)

and I would like to expand it with the **Databricks Associate Developer for Apache Spark 3.0.**

[https://www.databricks.com/learn/certification/apache-spark-developer-associate](https://www.databricks.com/learn/certification/apache-spark-developer-associate)

I have reviewed the event list in [https://www.databricks.com/events](https://www.databricks.com/events) but it seems there are no upcoming events where one would get a voucher code for the exam. 

Do you guys know where to get one?. Are the events the only possible way for that?. 

Thanks!",3,5,Due_Percentage447,2024-01-10 10:59:52,https://www.reddit.com/r/dataengineering/comments/1935ppc/just_got_certified_as_databricks_data_engineer/,1,False,False,False,False
192z6bh,Types of tables in dbt,"Tables vs int in dbt

What are the differences between a regular table and intermediate table in dbt? 

I’m learning by doing and while creating my tables I realised that I don’t need to have all tables to be either DIMs or FCTs. 

Then I started searching in the internet and I felt overwhelmed tbh, specially after I came across int page and they said its not meant to be used in dashboards or exposed to end user. 

then, what about the tables that I need them to study my data, build analytics around it and generate ad hoc reports?",3,13,Fuzzy-Example-7326,2024-01-10 04:09:17,https://www.reddit.com/r/dataengineering/comments/192z6bh/types_of_tables_in_dbt/,1,False,False,False,False
192q0bi,Make ssis script variant in snowflake,"Hi there, I am looking for some help or suggestions.

I have ssis script that do some calculations, while and for loops etc for each row of table and need to make the same thing in snowflake.

I managed to create a bunch of procedures and function that together do the thing(as a result it inserts into temp table so i can join to it later) but only for predefined values and I need it to be executed for each row in the table.


I also tried using dbt jinja for this purpose, but because jinja doesn't have while loop(i still have to use procedure for one part) i didn't manage to make script work with columns because it didn't render values, only column names.

Do you have any ideas? I am out of them.
Thanks in advance.",3,3,awkward_period,2024-01-09 21:21:22,https://www.reddit.com/r/dataengineering/comments/192q0bi/make_ssis_script_variant_in_snowflake/,1,False,False,False,False
192l2hm,Iceberg catalog for Trino?,"I've been looking at using Iceberg with Trino, but I see you need a metastore/Iceberg catalog to work with Iceberg tables. I have data in Dell ECS (can't move it to something like Amazon S3).

Can anyone with Iceberg or Trino experience walk me through their thought process on choosing their Iceberg catalog? Why shouldn't I just use Hive Metastore?

Any comparison on experiences with Hive Metastore or another Iceberg catalog like JDBC Catalog would be great. Thank you",3,2,Relative_Unit_7640,2024-01-09 18:04:40,https://www.reddit.com/r/dataengineering/comments/192l2hm/iceberg_catalog_for_trino/,1,False,False,False,False
192gc7y,Batch with Meltano,"Hello,

I have a question wrt Meltano, as I'm considering using it. How well does it work wrt batch jobs? Especially extractions from ftp servers, databases and api's on a daily basis with full overwrites to the target. Based on what I'm reading, it seems to be geared very much towards streaming with the batch related developments being rather new. Is it correct that streaming was always the focus of Meltano?",3,9,limartje,2024-01-09 14:47:03,https://www.reddit.com/r/dataengineering/comments/192gc7y/batch_with_meltano/,0,False,False,False,False
19179j7,First DE Role - How to Prepare,"Hey guys,

Excited to announce that I just landed my first role as a data engineer at a public health insurance agency!  Very excited - I have been wanting to break into data engineering in healthcare for a long time.  

I was a career data analyst before this and have a graduate degree in data science, which I am currently not using.  I was curious on people's recommendations for how to prepare for the role in the 2 weeks leading up to when the job begins.  I have thought about learning data structures from a Coursera course, thinking it could be good to brush up on the fundamentals, but it doesn't seem too immediately relevant.  Wondering if there were any books or things people could recommend.  I was taking a look at Fundamentals of Data Engineering but found it hard to grasp without having concrete grounding being in data engineering in an enterprise in the moment.  I'll look into that book more once I am established in the org.

They seem pretty old school, but want to modernize their tech stack with more APIs and Python which is partially why they wanted to hire me.  So maybe that could give some indications on how to prepare.

Let me know.  12 days until I start and I want to do something useful with my time.  Thanks!",4,3,i_am_baldilocks,2024-01-08 00:19:24,https://www.reddit.com/r/dataengineering/comments/19179j7/first_de_role_how_to_prepare/,0,False,False,False,False
19158af,How would you approach this project? Need some expertise,"Hello everyone,

My boss just quit the company and his project dropped on my lap, and I'm a bit hesitant on how to approach it. Maybe someone can point me in the right direction here..

The premise is this: we have a book of customers who own multiple properties (sometimes in the thousands per customer). Each month, they send us excel/csv spreadsheets with a list of all existing properties,their corresponding addresses and a bunch of other data for us to review. As you can expect, many customers have different formats (how columns are ordered, ""location"" instead of ""address"", ""building #"" vs ""building number"", number of columns etc etc).

What we are trying to do here is create a database with all these properties, which we will be updating monthly as new data come in. The plan is to use Azure SQL database to store this data. 

The question is, what are my options to get all that data normalized and brought to a single format so I can feed it into the database? The way I see it, I would need to create some sort of a profile for each customer in order to parse their spreadsheets correctly and have their data rearranged to fit my database. Not really sure.  Has anyone here worked on something similar?

Really need your expertise here! Thanks a lot in advance",3,12,AerotyneInternationa,2024-01-07 22:53:38,https://www.reddit.com/r/dataengineering/comments/19158af/how_would_you_approach_this_project_need_some/,0,False,False,False,False
190trrs,Dbt developer certification,"I am planning to take dbt certification exam , can you recommend any resources?",5,2,mamuonroll,2024-01-07 14:51:41,https://www.reddit.com/r/dataengineering/comments/190trrs/dbt_developer_certification/,0,False,False,False,False
18zqiz0,Leaving after 3 months,"3 months ago I accepted a position as a junior data engineer at a consultant/ startup without any prior real programming, BI or synapse experience.

My background is around 10 years of service desk experience from apprentice to management positions and I have lots of Microsoft certifications including AZ104 and AZ305, ( I believe they might have been one of the key reasons I got the job even though these are not data engineering related).

Throughout the last 3 months I have been enrolled onto 3 projects; one of them is a cloud infrastructure app project which I’m fine with while the other 2 are purely data engineering projects which at the moment I have basically no real understanding of and I am unable to provide any real input or work independently.

Recently, I have now received another job offer for  a much more related IT position (none programming, a security / IAM role/ ops) which I have decided to take. This was a position I applied for around the same time as my current position.

The pros for the data engineering role were that the pay was unreal for a junior position (i even got counter offered when resigning which was a first for me even though I really feel not warranted in this situation due to my lack of output), and the team atmosphere is by far the best i’ve seen.

Just wanted to share this in case anybody else had tried the shift from IT to CS jobs in the past and it not quite work out?",3,7,jam-iroquai,2024-01-06 03:42:20,https://www.reddit.com/r/dataengineering/comments/18zqiz0/leaving_after_3_months/,0,False,False,False,False
18zn9e7,GCP vs Azure/Databricks career progression,"Im pretty close to reaching my 2 year anniversary at my current role at a company that is all in on GCP. It has been super challenging and rewarding experience and I have a grown a ton as an engineer because I got exposed to a tech stack that gives you the nuts and bolts to build everything from the ground up.

However for reasons that I will not get into, Im looking to make a move to a different role. One of the interviews I got lined is offering a lot more money but they are an Azure + Databricks shop. This makes me a bit concerned because I feel like career progression will not be as “high”. Its worth mentioning that this was the impression I got because before the current role Im in, I used to be in a Databricks/Azure place where I felt everything was way more managed and handled for me.

So I want to know what you guys think? Is this me just being biased or is there some truth to it?

Btw I have total 5 years in all my career.",2,8,fapb88ve,2024-01-06 01:06:06,https://www.reddit.com/r/dataengineering/comments/18zn9e7/gcp_vs_azuredatabricks_career_progression/,0,False,False,False,False
18zfat6,Running selenium on aws lambda,"Hi all, I've got this personal finance webscraper project I'm building and I'd like to take it severless so it just goes without worry. My current issue is trying to get selenium to run on aws lambda and I see a number of approaches online, but not sure what's the best approach to take. Can I just upload the library with the rest of my code? Should I be doing this on Docker?

Thanks!",3,12,peepoo123,2024-01-05 19:31:54,https://www.reddit.com/r/dataengineering/comments/18zfat6/running_selenium_on_aws_lambda/,1,False,False,False,False
18z0vko,[Big Data] How does data parsing happen in your company?,"Hi community,

**Context**: In my current company, we have a data-pipeline (one of the biggest pipelines), which in short works like this: 

* We get raw json events from Kafka dumped in s3. 
* We run a batch job (Airflow) daily, this job picks up the raw jsons in s3, enforces a data parser logic.
* Data parser logic is simply, a service written in python where we explicitly define what attributes we want from raw json, these attributes are accordingly picked up from the json. There could be nested attributes as well which is taken care of.
* Post this parsing, the final filtered json is loaded in a dataframe where later this parsed data is converted to CSV/parquet formats and dumped in s3 in another folder.
* Later this processed CSV is loaded into tables, which is used for analytics, ml models, etc.

**Problem**: 

* Today for every new event we generate and get from Kafka-S3 (raw event), we have to write a parser logic from scratch, if the event structure is different. In case of small changes in event we can update attributes we want to parse in code itself. 
* Post that we have to deploy the service changes (regardless of changes in a json event parser is small or big) which again takes time. 
* If there is someone from a different team publishing the event into Kafka and writes parser logic, he has to learn through our codebase, understand how to write a parser which is can be a time-taking learning curve.  

**Solution** (I can think of):

* Can we have a UI interface, where we abstract all this code (thereby making it language agnostic), and the engineer simply has to select the attributes from json (that could include nested attributes), and post selecting it, the batch job gets that information, applies the parser logic selected from UI and then runs rest of the pipeline as it is. 
* This could help us in avoiding tons of code for parsers, avoid or minimize deployments in for smaller changes, and as our data size grows I think it would be better to abstract things make it language agnostic and streamline stuff. There could be other benefits as well I suppose. 

**Other thoughts**: 

* Is there a smart way of doing it? 
* Do we have any open source alternatives here? 
* Or any good engineering blogs which has covered such/similar scenario?
* How do big companies handle the humongous volume of data and parse relevant stuff into their tables or datalake, etc? 

&#x200B;",3,5,jarusv7,2024-01-05 07:12:22,https://www.reddit.com/r/dataengineering/comments/18z0vko/big_data_how_does_data_parsing_happen_in_your/,1,False,False,False,False
18yvhxx,How to transition to a DE from DBA/DBOps?,"Hi All, I'm working as a Lead DBA/DBOps in my current organisation which is a UK based company and I have around 6+ YOE. I was looking for some guidance to what can be my career path in Data engineering.

I have done alot of POC and implementation on ELT/ETL in the current organisation using Kafka/Airbyte/DBT(around 1.5yrs of hands on experience on these). And I'm starting to really get into ELT/ETL as they are much more engaging then the current DBA/DBOps work I do and my manager has taken that into consideration and have assigned alot of work related to such ELT/ETL. I have worked on Liquibase as well as a source control and automated deployments as well and its the main thing which is being used in our team for all the deployments of SQL scripts. I have experience on SQL development and automations using python, bash and powershell. In a nutshell even though my official designation is of a DBA but in my current organisation we do most of the ETL/ELT, sql development, automation, source/schema control and have all the freedom to spinup all the required ec2 instances for RND proposes. On the Datawarehousing front we also work all the livestreaming implementation to BigQuery. The databases I have worked on are MSSQL and MYSQL. 

So was wondering what more do I need to learn so I can transition to more Data related career as I'm currently bored with the just the DBA work and would like to work on more ETL/ELT and eventually move into some senior position in DE and/or relevant profile. What does a typical senior/lead DE professional have experience in and if someone can guide me in the said learning materials.",2,7,KilluaHatake,2024-01-05 02:27:48,https://www.reddit.com/r/dataengineering/comments/18yvhxx/how_to_transition_to_a_de_from_dbadbops/,0,False,False,False,False
18yaa91,Anybody here have experience with Quix.io for python stream processing?,"I've been researching several tools to build data pipelines, and I'm  wondering if someone has tried this tool or knows of something better?",3,2,Snoo_7731,2024-01-04 10:45:39,https://www.reddit.com/r/dataengineering/comments/18yaa91/anybody_here_have_experience_with_quixio_for/,1,False,False,False,False
18y39dj,Schema evolution using AWS DMS +COPY INTO(Snowflake) + Dbt(snapshot),"Hi folks.

In my projects(mainly datawarehousing, RDBMS) main scenario of schema evolution is when a new column is being added to the source table(RDBMS:MySQL or Postgre) and we need to bring it to datawarehouse. I will not describe the current solution, but it is painful/time-consuming to do it manually.

I was thinking about this : 

1. using AWS DMS to bring source-system data into Raw layer of dwh. DMS supports schema evolution(with some exceptions though)
2. using COPY INTO (Snowflake) to load data from s3 to Snowflake table(raw layer)
3. using dbt (snapshot) to take care of my SCD2 tables

Anyone tried this solution? Can you think of potential downsides? Will that work well for data type changes in source system?",3,2,Valuable-Produce-920,2024-01-04 03:40:21,https://www.reddit.com/r/dataengineering/comments/18y39dj/schema_evolution_using_aws_dms_copy_intosnowflake/,1,False,False,False,False
18xqdvt,Companies at early stage of the data journey: no code?,"Hi! I was reading *""Fundamentals of Data Engineering""* by Joe Reis and Matt Housley and a question came to my mind.

In the book, it says that for a company at the early stage of its data journey (when you are just starting to worry about data) the best approach is to stay away from code. Is that true for real life?

Honestly, I find no trouble in setting up a virtual machine on the cloud, writing some Python scripts and automate them using whichever task scheduler the operating system has. Or even setting up scripts in AWS Lambda or GCP Cloud functions. Is, for example, Apache NiFi better suited for these cases?

I have been working as a data engineer for 2 years now and all I see at companies starting with data is custom Python code. Which I think makes sense.

I would love to read your experiences.

Thank you and happy new year!",3,15,data_macrolide,2024-01-03 18:38:21,https://www.reddit.com/r/dataengineering/comments/18xqdvt/companies_at_early_stage_of_the_data_journey_no/,1,False,False,False,False
18xhl15,Data Bricks SQL warehouse where it is stored .,"I have a basic question , within Data bricks we have  Create SQL . .  
My question considering data bricks being clould portable   
If we are creating a SQL warehouse in databricks . is it fully managed . .if not how it will be portable across. . i am missing something .   
Is it like sql storage is underlying cloud infrastructure. . where exactly sql warehouse is stored and manged is it be databricks or underlying cloud infra ",3,14,Data5kull,2024-01-03 11:48:42,https://www.reddit.com/r/dataengineering/comments/18xhl15/data_bricks_sql_warehouse_where_it_is_stored/,0,False,False,False,False
18wqg4b,Using notebooks to analyze SQL - am I missing something?,"Hello y'all smart people,

I'm a little frustrated by the experience of doing ad-hoc analyses in SQL.

I love how Jupyter notebooks allow you to run your queries, annotate with Markdown, and eventually export your findings in HTML or PDF to share with the team or stakeholders. In addition, most of the time when analyzing a table, the steps are the same (e.g. null values, distribution of data, joins with a dimension or reference table), so it'd be easy to come up with a standardized analysis template that you can copy/paste with some minor adjustments. Now imagine the team saves their analyses in the repo where the SQL models live, then we'd have a nice documentation of the thought process that I can refer to in half a year when I've long forgotten whatever I was doing back then.

However, this doesn't seem to be a thing for SQL. I tried JetBrains DataSpell which looked promising, but the export is just a the actual code garnished with %%sql and %%md, no formatting at all. I tried Azure Data Studio, which feels better, but only seems to work for Microsoft products.

Do I have the wrong expectations here? Is my use case too niche? How do you share your findings within your team/others and make sure it's properly documented? 

I'd be grateful for your input. Thanks!",3,3,UnusualCookieBox,2024-01-02 14:17:46,https://www.reddit.com/r/dataengineering/comments/18wqg4b/using_notebooks_to_analyze_sql_am_i_missing/,1,False,False,False,False
18vw89a,Looking for Data Engineering Icon Packs,"Hi, does anyone have any good resources for icon packs for data engineering including brand logos etc? Looking to build out some architecture packs",3,3,ForMrKite,2024-01-01 12:56:45,https://www.reddit.com/r/dataengineering/comments/18vw89a/looking_for_data_engineering_icon_packs/,1,False,False,False,False
18vuscb,Contribute to DE projects or consultancy,"Any suggestions on where to find side projects or consulting gigs, for people wanting to learn more about Data Engineering from real use cases?",3,2,Azar_e,2024-01-01 11:17:50,https://www.reddit.com/r/dataengineering/comments/18vuscb/contribute_to_de_projects_or_consultancy/,1,False,False,False,False
18vd2es,Snowflake to BQ extract,"I need to copy data from a snowflake warehouse to my organizations bigquery warehouse. There seem to be a dizzying array of authentication, authorization, and extract methods. Just wondering what is the typical way to do this? I see there is a way to copy directly into gcs but not sure who runs or how the COPY INTO command works on the snowflake side.",3,7,arborealguy,2023-12-31 17:52:26,https://www.reddit.com/r/dataengineering/comments/18vd2es/snowflake_to_bq_extract/,1,False,False,False,False
18uyk93,Data engineering incrop / consulting,"
Hi fellow data engineers,

I'm interested in doing some consulting work for small businesses as a side gig, where I can offer 10-15 hours of my expertise per week.

How do you find clients for this kind of work? Do you have any tips or strategies to share? I would appreciate your insights.


Location - Ontario, Canada 

Tech stack - Python, Azure, BI dashboard, ETL

Thanks.",3,1,BirthdayAccording438,2023-12-31 03:49:25,https://www.reddit.com/r/dataengineering/comments/18uyk93/data_engineering_incrop_consulting/,1,False,False,False,False
18ueus8,Cloud tutorial,"Hi everyone,

I plan to start with data engineering this New year.
And I was exploring the various tutorials from major cloud services provider. And I found that the level of organised tutorial has GCP at the top, followed by Azure and then AWS.(personally I didn't like AWS a bit).
Does anyone else have the same opinion or am I going in the wrong direction?",3,8,wandering_soul1103,2023-12-30 12:42:57,https://www.reddit.com/r/dataengineering/comments/18ueus8/cloud_tutorial/,0,False,False,False,False
18u0o43,MES/ERP Database Design Diagram Sample?,"Working in data within a manufacturing role and want a better understanding of how everything is typically linked together. For example shop floor production data, does each record have shift, PO#, WO#, etc. If not, how is that information linked together. How do assets (machines) and resources (cells) link to production goals and routings. 

There are so many moving pieces and I want to better understand it. ",3,9,Reddit_Account_C-137,2023-12-29 23:26:35,https://www.reddit.com/r/dataengineering/comments/18u0o43/meserp_database_design_diagram_sample/,1,False,False,False,False
18srpzd,Productionizing Jupyter Notebooks with Versatile Data Kit (VDK),,3,0,zverulacis,2023-12-28 11:23:44,https://medium.com/versatile-data-kit/productionizing-jupyter-notebooks-with-versatile-data-kit-vdk-ec5824d31b77,0,False,False,False,False
1abteqs,"If your developing an etl library ( for sources that are atypical or not covered by meltano ect) what’s the best way to incorporate gen ai , if at all ?","Outside of like using copilot, I mean in the code base to use ai on support of functions to help abstract a layer ? Almost like maybe idk you pass in just a url or some endpoint and it automation write the correct request or idk even know I guess where you can push the rock so I wanted to open up a convo.


***Maybe Simpler Put***

How can a data engineer in 2024 make the best etl library to digest net new sources & is gen ai able to help that - if so how ? ",2,3,citizenofacceptance2,2024-01-26 21:26:14,https://www.reddit.com/r/dataengineering/comments/1abteqs/if_your_developing_an_etl_library_for_sources/,1,False,False,False,False
1abfnwg,Guidance,"Hello Everyone!
I am starting as an Associate Data Ops Engineer at a Product Based Company.
I have transitioned from being a Mechanical Engineer to the Data Field.
I am very eager to learn and can't seem to get my head around what to focus on and what's the sequence of learning.
Can anyone help me with the right source to follow through?
Also, how important is it to learn DSA?",2,4,ruchirmittal,2024-01-26 10:52:22,https://www.reddit.com/r/dataengineering/comments/1abfnwg/guidance/,0,False,False,False,False
19f6git,3 ways to model product events in the data warehouse.,"Hi, I have worked with multiple B2C and B2B companies of different maturity for several years.   
In this blog post, I summarize and compare what I have learned about the three types of data models most companies use for product events.

If you are planing to build your own product event model in the data warehouse, I recommend to read it and not fall into the same mistakes as some of my previous companies did.

[https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse](https://www.mitzu.io/post/modeling-product-events-in-the-data-warehouse)

&#x200B;",2,0,MitzuIstvan,2024-01-25 10:37:18,https://www.reddit.com/r/dataengineering/comments/19f6git/3_ways_to_model_product_events_in_the_data/,1,False,False,False,False
19ex6ia,Role change,"
Hey guys, I’m a UC Berkeley graduate majoring in operations research with an undergrad in statistics. I graduated last year but due to the bad job market ended up taking up a consultancy stint. I am looking to shift roles into data science or data engineering. Currently my profile includes:

Programming languages: python, c++, R
Database: PostgresQL, MySQL
Bigdata : spark(pyspark and sparksql), databricks
Frameworks: keras, pandas, scikit 

Is there anything else I can add to my profile that would help me secure a position as a data scientist or a data engineer? And can you also suggest how I can go about the application process given the current job market?",2,1,just-another-potatoo,2024-01-25 01:19:28,https://www.reddit.com/r/dataengineering/comments/19ex6ia/role_change/,1,False,False,False,False
19eu9dt,Resources for GCP Professional Data Engineer," Hello everyone, I hope you're doing well! I'm planning to prepare for the GCP Professional Data Engineer exam, and I've heard that Google has changed the exam pattern considerably. I'm a bit confused and don't know which resources would be beneficial for me. Could you suggest some useful courses or books, please? ",2,4,Ordinary_Run_2513,2024-01-24 23:08:23,https://www.reddit.com/r/dataengineering/comments/19eu9dt/resources_for_gcp_professional_data_engineer/,1,False,False,False,False
19epl1b,Large Data Migration to AWS Glacier,"Hi all! 

We have about 1 PB worth of data to move from onPrem storage to Aws. We are considering using Deep Archive as an archive storage solution until we can get more on prem storage. This data is expected to never be used again but cannot be deleted. I have a few questions now

1. Does it make sense to use Deep Archive as an Archive solution if we plan to get on prem capacity in 6-12 months?

2. Are there costs to upload data? Or only to store and download the data? 

3. Is there a method to transfer the data from the regular S3 bucket to Deep archive without setting the 24 hour rule?

Thanks!!",2,2,Helium0205,2024-01-24 19:56:22,https://www.reddit.com/r/dataengineering/comments/19epl1b/large_data_migration_to_aws_glacier/,1,False,False,False,False
19ejjrf,2024 Data Engineering Trends - do you agree with these? What do you consider the most relevant trends in 2024?,,2,2,Round-Following1532,2024-01-24 15:22:53,https://kestra.io/blogs/2024-01-24-2024-data-engineering-trends,0,False,False,False,False
19edw2d,Ingest data from MySql to BigQuery DWH,"is there any way i can replicate/move data from MySql to BigQuery without a third party tool?
 
MySql hosted on Oracle and BigQuery hosted on GCP 

also, my manager insist to do it this way (not using Fivetran) and the other guy said it will be done without any extra charges or fees. 
is it possibly to be done? and for free??",2,9,Fuzzy-Example-7326,2024-01-24 10:16:53,https://www.reddit.com/r/dataengineering/comments/19edw2d/ingest_data_from_mysql_to_bigquery_dwh/,1,False,False,False,False
19ecfst,Spark questions coming from DWH background,"I have a data warehousing background and just started learning Spark. Naturally, I started looking for similarities and analogies between the two systems, and I'm very confused. It seems so very different than typical dwh scenario. Here are my questions:

* Data warehouses typically have a star schema data model. I know spark is just a processing engine, but do spark projects have a ""typical"" data model? Is it also star schema? Or do you have everything-joined one big table for every data load?
* How do you handle daily data insertion? Are techniques like dimension upserts and scds, fact upserts, etc even done in spark?
* Is using data catalog necessary? Or just good to have?
* Are raw parquet files ever used? Or you pretty much must use Iceberg/Delta table?
* How is spark processed data consumed at the end? Do analysts use tools like Presto/Athena? Are BI tools pointed to S3?
* Are these questions even spark questions or data lake questions?
* Finally, are there any articles, videos, or courses about spark design patterns that answer above questions or similar? Am I even asking the right questions?

Thanks!",2,9,PakumCakum,2024-01-24 08:28:56,https://www.reddit.com/r/dataengineering/comments/19ecfst/spark_questions_coming_from_dwh_background/,1,False,False,False,False
19ec88e,Fivetran with DBT,"Hey guys, we are planning on using Fivetran as our data ingestion tool and also leverage its capabilities for hosting dbt projects
I see that it’s on beta currently and so far we haven’t faced any issues on our development warehouse when it comes to using the platform
I would like to know when it’s going to be released as a stable version.",2,0,Professional-Ninja70,2024-01-24 08:13:53,https://www.reddit.com/r/dataengineering/comments/19ec88e/fivetran_with_dbt/,1,False,False,False,False
19ea6tf,How to deserialize json data from an avro message stream in Databricks using DLT?,"I’m reading avro messages from Kafka by connecting to the schema registry through a Delta Live Tables pipeline (DLT). I then use the from_avro method to deserialize this message stream. The records are present as json objects, how do I use from_json to load this data inside delta? It’s asking me for the schema, is there any way I can dynamically infer the schema? 

TL;DR: how to dynamically infer schema from a JSON stream and load inside delta?",2,0,sampasha007,2024-01-24 05:59:47,https://www.reddit.com/r/dataengineering/comments/19ea6tf/how_to_deserialize_json_data_from_an_avro_message/,1,False,False,False,False
19dwfaq,BI-like interface for data modeling in the semantic layer,,2,0,igorlukanin,2024-01-23 19:19:37,https://cube.dev/blog/introducing-playground-2,1,False,False,False,False
19du2hj,Getting a job asap or go full learning?,"Hi guys,

I've graduated my bachelor and decided to put my time in Data Engineering cause it seems more interesting than other fields. I've been looking at what the fundamentals are and if I understand correctly I should focus on getting to an advanced level on Python and SQL first before moving to some suite of tools to build my first solo-project to demonstrate I can actually be of use to a company.

I come to ask you pros/cons on this two approaches for the next months: 

* Get a back-end developer job in my vicinity as quick as possible, even if it is not Data Engineering related, just to get some experience on my resume and not look like a complete beginner to future DE recruiters; i would study on a portion of my free time.
* Not get a job and just spend my days studying DE and making that project to get good quicker than the first option; I don't know how long that would take and if being unemployed for that amount of time would look very bad on my resume. To give more info, I am not in a financial situation where I need a job to keep floating.",2,11,Di4mond4rr3l,2024-01-23 17:44:15,https://www.reddit.com/r/dataengineering/comments/19du2hj/getting_a_job_asap_or_go_full_learning/,1,False,False,False,False
19dtj92,Does AGPLv3 in an upstream system affect licensing of data analytics code?,"  

Hello!

My question is about AGPLv3 open-source licensing as it concerns BI code that works “downstream” of the open source system. I'm hoping that others in this r/ will have had experience with this and be willing to share.

I’m in charge of building a CRM and the corresponding BI platform that reads information that was entered into the CRM. The data platform is envisioned as a simple use of a business intelligence tool, running queries against the relational database that the CRM uses. 

Our goal is to use the CRM/BI combination internally for a few years, then market the whole solution as a product once it’s mature. I think that using an open-source CRM could readily meet the company’s needs while we develop the BI part as proprietary software. But there is the issue of creating a product from it later…

My concern is whether using an AGPLv3 licensed CRM for this interim period will require that I license the BI platform queries and data infrastructure as AGPLv3 as well. I am currently thinking that we’d have to share the BI queries/code because of the CRM’s licensing.

The CRM comes with a core database schema, but it also defines the database schema in response to user inputs (e.g., give an account a FK relationship to some other entity we also defined in the CRM’s GUI, etc.). The fact that the CRM’s database’s core schema represents a creative effort of its own -- and is not merely output from the program like the user-defined schemas are -- makes me think we run afoul of creating a derivative work here. And, with the CRM being AGPLv3, network use is distribution, so we’d have to share everything.

So, is that true?

I understand the wording of the AGPLv3 mentions pipes and other common inter-process communication techniques as not constituting a derivative work, but I do not know where the line is as regards database schema, as described here.

I’m hoping someone will be able to give some clarity. Thanks!",2,0,InternationalTea5284,2024-01-23 17:22:03,https://www.reddit.com/r/dataengineering/comments/19dtj92/does_agplv3_in_an_upstream_system_affect/,1,False,False,False,False
19dpdzg,How to migrate Hive custom functions to BigQuery UDFs,"Excited to share my latest blog post on migrating Hive UDFs to BigQuery SQL UDFs! Whether you're a data engineer or a CTO, this guide is crafted to simplify your migration process. Dive into the step-by-step approach and discover how to leverage BigQuery's SQL for effective data processing. #BigQuery #DataMigration #HiveUDFs  
[https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs](https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs) ",2,0,Constant-Collar9129,2024-01-23 14:16:29,https://www.reddit.com/r/dataengineering/comments/19dpdzg/how_to_migrate_hive_custom_functions_to_bigquery/,0,False,False,False,False
19doe1t,Airflow backfill via UI,"Is it possible to backfill via the Airflow UI? I know that it's possible via CLI, but I have a requirement to do it over the UI, did some searching couldn't find much info, wonder if it's possible to do via UI. ",2,1,cjj1120,2024-01-23 13:27:05,https://www.reddit.com/r/dataengineering/comments/19doe1t/airflow_backfill_via_ui/,1,False,False,False,False
19d1jnj,Data tokenization,"What data tokenization tools have folks found to work really well? (Eg Protegrity? Thales? Acra? Other?) 

Thanks!",2,1,iad05,2024-01-22 17:48:47,https://www.reddit.com/r/dataengineering/comments/19d1jnj/data_tokenization/,1,False,False,False,False
19cz7sp,What is stateful stream processing?,,2,0,mwylde_,2024-01-22 16:13:59,https://www.arroyo.dev/blog/stateful-stream-processing,0,False,False,False,False
19csi4o,cloud-based Hadoop and Spark experimentation platforms,I'm seeking for some cloud platforms to experiment with as I began learning Hadoop and Spark conceptually and my laptop is too sluggish to handle any big data projects on it. Would you kindly recommend some platfoms to me ?,2,3,Mr_bdnt,2024-01-22 10:17:15,https://www.reddit.com/r/dataengineering/comments/19csi4o/cloudbased_hadoop_and_spark_experimentation/,1,False,False,False,False
19cru8f,Learning DataEng Question/Advice,"Hi all,

Apologies if this is not the correct place to ask this question. I have started the dataTalks data eng zoomcamp which I am enjoying. I am a BI/SQL person in my current role (2 years) and I'm keen to transition into data eng so I have been considering portfolio ideas.

What I would like to do is to ingest spotify data into a database and have this take place automatically, perhaps once per day/week. I would then connect to that DB using PowerBI to develop a dashboard. I would like all of this to take place in the cloud so that the ingestion takes place without any reliance on my local machine being turned on or connected to the internet all the time.

Is this a realistic? If so where would you suggest I start?

I don't have much data eng knowledge yet but I have recently learned how to spin up a VM on google cloud, run docker containers/networks with postgres/pgadmin and I have also written a python script to ingest data into the postgres DB by grabbing a CSV from a website. ",2,6,CalligrapherDefiant4,2024-01-22 09:28:49,https://www.reddit.com/r/dataengineering/comments/19cru8f/learning_dataeng_questionadvice/,0,False,False,False,False
19cjl73,What do you guys think about this video?,"Are these mock interviews fr? How can I learn all these or be better than this candidate?

How realistically can someone become this guy with Azure, Python, sql skills ?

[Link to the video](https://youtu.be/JfDGYKvRfWw?si=x3dXEkm2id9AVAPy)",2,2,Jealous-Bat-7812,2024-01-22 01:21:57,https://www.reddit.com/r/dataengineering/comments/19cjl73/what_do_you_guys_think_about_this_video/,1,False,False,False,False
19bz1je,Database Subsetting and Relational Data Browsing Tool.,"[Jailer is a tool for database subsetting and relational data browsing](https://github.com/Wisser/Jailer).

It  creates small slices from your database and lets you navigate through  your database following the relationships.Ideal for creating small  samples of test data or for local problem analysis with relevant  production data.

* The  Subsetter creates small slices from your database (consistent and  referentially intact) as SQL (topologically sorted), DbUnit records or  XML.Ideal for creating small samples of test data or for local problem  analysis with relevant production data.
* The  Data Browser lets you navigate through your database  following the  relationships (foreign key-based or user-defined) between  tables.

Features

* Exports  consistent and referentially intact row-sets from your productive  database and imports the data into your development and test  environment.
* Improves database performance by removing and archiving obsolete data without violating integrity.
* Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.
* Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.
* SQL Console with code completion, syntax highlighting and database metadata visualization.
* A demo database is included with which you can get a first impression without any configuration effort.",2,0,Apprehensive-Fix-996,2024-01-21 08:43:00,https://www.reddit.com/r/dataengineering/comments/19bz1je/database_subsetting_and_relational_data_browsing/,1,False,False,False,False
19bx9w9,"ZeroETL vs. Pipelines, Jobs, Schedules",In the data space there is this new movement of ZeroETL (started by AWS) that advocates the replacement of traditional data movement pipelines (generally implemented using Spark and Airflow) with real-time CDC replication. What is your take on it ? Did anyone fully implemented it ?,2,6,matteopelati76,2024-01-21 06:44:32,https://www.reddit.com/r/dataengineering/comments/19bx9w9/zeroetl_vs_pipelines_jobs_schedules/,0,False,False,False,False
19btc8i,Any real benefit to a queue service over a makeshift queue with Postgres for small scale stuff?,"Basically the title, if it can be done with Postgres… is something like rabbitmq worthwhile?",2,6,DuckDatum,2024-01-21 02:55:19,https://www.reddit.com/r/dataengineering/comments/19btc8i/any_real_benefit_to_a_queue_service_over_a/,1,False,False,False,False
19bm0w0,Common Architectures for Event & Schedule Based Email Notifications System?,"Hello everyone. As is typical (for me at least), I’m looking to garner some insights on approches for a common kind of application before I built one.

I’m going to build an email notification system. Its first set of requirements will be to send out notices a few weeks before some documents expire, unless those documents have been renewed already.

The source data lives in an excel spreadsheet stored in a shared SharePoint drive. It gets updated at regular intervals by some kind of ghost entity that magically knows when/how to update the data.

My first thoughts are that I’m going to need to store the document names and their expiration date, as well as a recipient list somewhere that can be polled daily. If the poll finds any records matching the conditions I’ve described, send an email to the documents recipient list. 

However, I started to wonder if there might be a better approach. For example, maybe the polling service can simply update a `status` column based on how close a document is to expiration. If `status` gets set to `soon to expire`, then a [postgres] `NOTIFY` can get sent to a `LISTEN`ing channel. On the `LISTEN`ing channel, there can be an application that handles these “event based” emails. This decouples the polling service from the email service, and theoretically the email service can get used for any email.

After this, I started to wonder Postgres actually has some built in method to automatically `NOTIFY` if a records `date` column approaches less than X days from current time. I’m doubtful about this, as it just sounds like a built in polling service, or maybe a nasty looking trigger function, but who knows.

For reference I use Postgres for storage, Prefect for orchestration, Python for the nooks and crannies, … I want a solution that’s generally applicable though and can get reused for future projects. 

What’s the typical “generally applicable” approach for email notifications? Thanks!",2,1,DuckDatum,2024-01-20 21:11:13,https://www.reddit.com/r/dataengineering/comments/19bm0w0/common_architectures_for_event_schedule_based/,1,False,False,False,False
19bhkx4,Urgent: Taking 2 extra semesters for a CS Minor as a Stats major?,"(The reason I put urgent is because the deadline for me to choose courses is in 2 days 🥲 i know not the best)

Hello, I’m a fourth year undergrad student majoring in Statistics and minoring in GIS and Human Geography at a university in Canada and I’m looking to become a data engineer after graduating but I’ll likely start as a data analyst first before transitioning into a DE. 

I’m wondering if it would be worth it to take CS courses for an extra 2 semesters to change my Human Geography minor into a CS minor. 

The thing is, CS at my school can be pretty theoretical, and 2 required courses focus on making projects in Java and C which I’m not sure how relevant those languages are for DEs. However, I do have the option of choosing an SQL course and machine learning courses after I complete those required courses so it’s something I’m considering as I assume it is relevant to the data science industry. 

However, if I’m not gonna really use a lot of things I learn such as math proofs, Java and C  during work then I honestly think those courses might be overkill, but if CS will overall teach me skills that are fundamental to the engineering aspect of DE then I’m willing to just push through and finish the CS minor. I know for a fact that I learn better in classroom settings than on my own so that is also something to consider. 

For those who work as a data engineer, what are the most important languages/technical skills to learn in order to start a career as a DE? I know Python is very important and I’m currently using it a lot in one of my upper year stats courses, but how about languages such as Java or C? I’m already familiar with R and will learn SQL on my own. 

If anyone can provide me insight on the current industry and tips on how to get hired in Canada or the US it would be much appreciated!",1,13,smol_llama,2024-01-20 17:59:11,https://www.reddit.com/r/dataengineering/comments/19bhkx4/urgent_taking_2_extra_semesters_for_a_cs_minor_as/,0,False,False,False,False
19b8kma,Marketing agency tech stack,"Hello guys, I've been running my business as a marketing agency owner for a couple of years right now. 

I started developing my martech stack, duplicate for each client, made by a mix of OS and licensed products which are:
1. Hubspot CRM (just for the sales part)
2. Mautic (marketing automation)
3. Sendgrid
4. Mailsync (for feeding Hubspot timeline with SendGrid's events)
5. Make.com (former Integromat) as IPaaS (I have an old subscription with a super cheap deal)
6. Coda.io as a productivity tool shared with clients
7. WordPress as a CMS with Elementor Pro
7. Oviond for analytics dashboards

I've started understanding the competitive advantage of OS (only Mautic and WordPress are on this list) by having them hosted on Google Cloud Platform with virtual images that I duplicate each time and two up-and-running main machines for maintenance. 

I'm looking for any type of suggestions about this because I'm starting to look into RudderStack as a data lake for getting all events but probably it's too big for my clients and we don't have so many events to manage. Still thinking about it and I'm struggling a bit. 

Thanks to anyone who will give me any feedback!
See you",2,5,SpookyLibra45817,2024-01-20 10:05:28,https://www.reddit.com/r/dataengineering/comments/19b8kma/marketing_agency_tech_stack/,1,False,False,False,False
19b1cr8,What is project management like in DE?,More specifically within your de team and collaboration with other non IT/DE folks.,2,2,Traditional_Reason59,2024-01-20 02:44:30,https://www.reddit.com/r/dataengineering/comments/19b1cr8/what_is_project_management_like_in_de/,0,False,False,False,False
19awl9u,What differs an intern from a junior data engineer?,"Hello! I've been interning as a data engineer in a data platform team for the past 7 months and thinking about applying for junior roles. It feels a little hard for me to understand how much both levels differ from each other, since the team I'm currently in has no junior engineers. While I know there are more responsibilities for a jr, I don't know how much more autonomy and technical skills are expected. Anyways, I'd appreciate any help :)",2,7,DerangedMio,2024-01-19 23:05:02,https://www.reddit.com/r/dataengineering/comments/19awl9u/what_differs_an_intern_from_a_junior_data_engineer/,0,False,False,False,False
19atiz3,"Data Lake Ingestion, How do you do it?","I'm curious as how do most of you handle ingestion into your data lake. Do you like the way you are doing it, what would you prefer? What are challenges you have?",2,9,AMDataLake,2024-01-19 20:56:05,https://www.reddit.com/r/dataengineering/comments/19atiz3/data_lake_ingestion_how_do_you_do_it/,1,False,False,False,False
19aroct,Database Dev or SRE as stepping stone to data engineer,"Hi guys,

I've currently been working as a data analyst for 2 years where I've gained experience in Python,R, PowerBI, SQL, and Azure. I've carved out a niche role for myself within my team as essentially a analytics engineer. My main job is to just create, maintain, and monitor ETL pipelines within azure, which has led me onto the path of data engineering.  


Recently however 2 opportunities have presented themselves for lateral moves within my company. One is as a site reliability engineer (SRE) where I will have greater exposure to key data engineering domains such as Infrastructure as code, cloud services and architecture, networking, automation and orchestration, along with exposure to tools such as Kubernetes, Linux, and Golang,  AKS .  


The other option is becoming a database developer. Technically two options since there are the BI dev teams and the database Dev teams. One team works on the OLTP database the other the OLAP database. Naturally there I would get greater exposure to data modeling, data warehousing and storage, ETL, data quality and governance, as well as database performance tuning.

Both would give me exposure to Git, although id imagine the database dev positon would provide greater exposure to software development principles that I never really learned in college ( I graduated as an industrial engineer although about to start my CS masters)

Between those 3 options ( SRE, BI Dev, and Database Dev) which would be the best one to pivot to with the goal in mind of eventually becoming a data engineer. I did want to note it seems that the SRE's making around 110k while the dev's make around 90k other than that in terms of compensation they are essentially the same",2,6,jotama0121,2024-01-19 19:39:01,https://www.reddit.com/r/dataengineering/comments/19aroct/database_dev_or_sre_as_stepping_stone_to_data/,0,False,False,False,False
19a5mog,Any data engineers in healthcare? I'm trying to weigh the pros and cons of different cloud stacks and am looking for guidance,"I've been working on choosing and building a healthcare focused data stack with ETL, data processing, analytics, applications + reporting and so on with a bunch of protected information (PHI). I'm curious if anyone has experience or strong opinions on which cloud stack is the most friendly for healthcare applications?

At this point, I'm primarily considering AWS vs Azure. I like AWS because I have experience with it and also seems more dev friendly. But I've also heard from colleagues about using Azure in healthcare settings. I am not exactly sure if that's mostly a trend or if there are real reasons why one is better than the other. Would love any thoughts or guidance from more experienced folks here!",2,11,maiden_fan,2024-01-19 00:28:01,https://www.reddit.com/r/dataengineering/comments/19a5mog/any_data_engineers_in_healthcare_im_trying_to/,1,False,False,False,False
199w676,Externally Triggering DAGs - Good or Bad?,"I'm (ab)using Airflow to ETL some data, and pretty quickly figured out that DAGs with dynamically generated tasks are a big no-no. The data I'm moving is being generated on the fly, and is spread across several databases with the only constant being a universal run ID. It seems intuitive to me to collect run IDs over a time window, then sequentially aggregate and load those runs. 

The way I've been doing this is to have an ETL DAG and a scheduled DAG. The scheduled DAG grabs the run IDs, then externally triggers the ETL DAG for each run. However, this just feels like a bad work around to dynamically generating tasks. Is this bad practice? It makes my DAG list quite... busy to say the least, and trouble shooting the scheduler DAG is very difficult since it's effectively running through runs in one single task.

Am I thinking about this problem wrong? Is there some standard way of approaching this type of transfer that I'm not aware of?",2,6,Alwaysragestillplay,2024-01-18 17:55:49,https://www.reddit.com/r/dataengineering/comments/199w676/externally_triggering_dags_good_or_bad/,0,False,False,False,False
199u4mt,Letterboard - Generate a dashboard from Letterboxd data (Personal Project),"I completed this project a couple months ago, and trying to get into the habit of sharing my personal projects. Letterboard is a Streamlit dashboard that uses data scraped from your Letterboxd diary. I used BS4 for web scraping and Polars for data frame aggregations.  I graduated last May and have been job searching for Data Analyst/Engineer jobs since, and would like to know what you all think of this project and where I could probably make this project better. Also I plan on doing more projects utilizing Modern Data Stack tools.

Github: [https://github.com/afoshiok/Letterboxd-EDA](https://github.com/afoshiok/Letterboxd-EDA)

Site: [https://letterboard.up.railway.app/](https://letterboard.up.railway.app/) (Recommend using this over Streamlit link)

&#x200B;",2,1,FavourOshio,2024-01-18 16:31:05,https://www.reddit.com/r/dataengineering/comments/199u4mt/letterboard_generate_a_dashboard_from_letterboxd/,1,False,False,False,False
199rrya,Informatica cloud azure storage issue,"Hi! I was working on Informatica Cloud to move some blob storage of Azure on a S3 bucket. The problem is that when a create mapping, I don't know how to parameterized the fact that the blob storage of Azure create each day a new file with a new name. So in my mapping in Informatica the blob to get remain static. Anyone faced this problem and knows how to get the job dinamically? Thanks.",2,0,DNSoundRM,2024-01-18 14:47:05,https://www.reddit.com/r/dataengineering/comments/199rrya/informatica_cloud_azure_storage_issue/,1,False,False,False,False
199n6gu,ODS with dbt,"We are doing another iteration on our dataplatform. This time we will overhaul our ingestion framework from full-loads only to incremental. The deltas are stored as received in a data lake (ADLS Gen2).

We are using dbt as transformation tool and Azure SQL database as compute / storage.

My question is how would you get the incrementals staged ready to be referenced by the dbt models?
How to set the desired column data types?
Where would you perform schema checking?

Any advice is welcome 🤗",2,0,Ashamed_Cantaloupe_9,2024-01-18 10:32:20,https://www.reddit.com/r/dataengineering/comments/199n6gu/ods_with_dbt/,1,False,False,False,False
1993m10,Quick question on the database access management,"Have you tried to programmatically export access details (name, role etc.) from SQL server (cloud or on-Prem) or oracle DBs? 

I am hoping to build a report on access and data rights across various data stores, primarily Azure SQL and others. 

Appreciate your perspective if you have tried something similar.",2,0,abskiing403,2024-01-17 18:22:17,https://www.reddit.com/r/dataengineering/comments/1993m10/quick_question_on_the_database_access_management/,1,False,False,False,False
1991m63,Got some time to mentor?,"Hello experienced devs,
I am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. 

I am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I’m going to ask for. Since I’m a contractor, I cannot connect/ask for guidance in my workplace

Couple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. 

One day I found out that what I was doing was not very different from our in-house data analyst’s role. 

Then, I learned Azure services and got a job as a contractor. Now I’m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.

tl;dr: a noob DE want a mentor to help navigate through the data carrier.",2,3,Jealous-Bat-7812,2024-01-17 17:05:40,https://www.reddit.com/r/dataengineering/comments/1991m63/got_some_time_to_mentor/,0,False,False,False,False
199105x,Redshift sync connector to replace DMS?,"Hello Reddit, i turn to you once again for a DE issue. TIA.

&#x200B;

**Currently:**

""RDS MySQL > DMS > Redshift""

DMS is configured for a \`full load, ongoing replication\` (lags around 6-12 hours everyday ig)

&#x200B;

**Problem:**

Expense (unutilized instance after initial full load) and want to move away CDC to a different tool, as a POC

&#x200B;

**POC** **TODO**: (*no choice here*)

MySQL > Kafka > *Redshift sync connector (necessarily)*

&#x200B;

Assuming the POC *has* to be done..

*Questions:*

1. **Duplicates**: There's mention of ""atleast once delivery"" into sync'd table, and a dummy topic set up on confluent cloud confirmed presence of \~10k dupes per 1mil rows. Support could not answer pattern on when/why it occurs. How should one tackle this? Not sure if ksqdDB/single message transform can help here.
2. **CDC:** There is a latest\_updated\_ts to help determine updates, how can the sync connector handle upserting? [docs](https://docs.confluent.io/kafka-connectors/aws-redshift/current/sink_config_options.html#redshift-sink-config-options) mention insert OR update (""*this will only apply when modifying a record; you can’t use this mode to insert a new record if the record doesn’t already exist*""). Solution to run two parallel ones?
3. What other tests should one do? I've done basic data sanity checks so far, and plan to set up a kafka local to stream to redshift to test a dummy run.

Apologies if there's basic issues in my writings. TIA, once again!",2,2,LocksmithConnect6201,2024-01-17 16:42:19,https://www.reddit.com/r/dataengineering/comments/199105x/redshift_sync_connector_to_replace_dms/,1,False,False,False,False
198upu8,Accelerate Your Data Journey: Practical Guide to S3-to-RDS ETL Using Lambda,,2,0,BigNo3623,2024-01-17 11:46:11,https://medium.com/@naveenkumarmurugan/accelerate-your-data-journey-practical-guide-to-s3-to-rds-etl-using-lambda-daf14010a68f,1,False,False,False,False
198uhpg,When to use Client-side AWS KMS encryption and How?: A step by step Guide,,2,1,BigNo3623,2024-01-17 11:32:14,https://i.redd.it/yzvszoppkzcc1.png,1,False,False,False,False
198pgon,Spark in pair with DWH?,"Hi guys,

How would you approach a project, where a DWH is to be migrated from on-prem, teradata, to a private cloud, spark based solution?

I only know spark as a processing engine, and from what I've been reading, no one actually uses it for it's data storage functionalities ('databases', 'tables', and so on). 

So I don't know too much about data architecture, as I've always been just implementing stuff that someone designed (mostly ETL pipelines with spark/airflow). And spark was used to get data from point A to point B, with transformations in between.

Does any of you work with spark as a processing engine for DWH? How does it work? Can DWH even be in a form of parquet files lying in buckets? How would the data architecture be imposed then? Would dbt help here?

As you can see, I'm new to the architecture side of data engineering, so please forgive me if some of the questions don't make sense. I'd gladly check out any recommended resources. 

Thanks",2,3,Visual-Exercise8031,2024-01-17 05:56:09,https://www.reddit.com/r/dataengineering/comments/198pgon/spark_in_pair_with_dwh/,1,False,False,False,False
198cvhr,Relevant Course,"Hi my brother finished a few years ago his bachlor degree and I want to give him as a gift for his birthday a course that would be relevant for his career and help him in his profession 

Thx for the helpers",2,3,Rude-Issue4573,2024-01-16 20:24:17,https://www.reddit.com/r/dataengineering/comments/198cvhr/relevant_course/,1,False,False,False,False
1987ax9,CRM-ERP and the Data Warehouse,"We are working at integrating our CRM & ERP systems. However, is this necessary if both were connected to the data warehouse? Is this an either/or situation, or is it valuable to have ERP & CRM integrated whilst having both integrated into the data warehouse?",2,1,-CaptainCapuchin-,2024-01-16 16:42:50,https://www.reddit.com/r/dataengineering/comments/1987ax9/crmerp_and_the_data_warehouse/,1,False,False,False,False
1985ama,GitHub - danielbeach/fine-tune-openLLaMA: This repo shows how to fine-tune openLLaMA (7b) model on a GPU. (Made for Data Engineers),,2,0,dataengineeringdude,2024-01-16 15:19:48,https://github.com/danielbeach/fine-tune-openLLaMA,0,False,False,False,False
19842tn,Suggestions for interesting data structuring/ acquisition/ ingestion problems in AI/ML,"I am a CS undergrad with some exp into ML and LLM applications but i am looking to do more research and projects into the DE side, so thats why i am asking if you guys have any open problems in literature or in general (Problems the ML/AI community as a whole are seeing)   with data structuring, organizing and ingestion for training/fine-tuning or measuring ML models.

&#x200B;

I was thinking about a few problems in this area and would like to hear your feedback on these and suggestions of other topics

1. Orchestrating LLM data for model distilation (i.e LLMs creating data for training smaller models to substitute the larger ones)
2. general synthetic data generation and filtering for training ML models

&#x200B;

Any more ideas?",2,0,SnooPineapples7791,2024-01-16 14:26:14,https://www.reddit.com/r/dataengineering/comments/19842tn/suggestions_for_interesting_data_structuring/,1,False,False,False,False
19810ce,Exploring the Transition from Hive to BigQuery: A Detailed Guide,"I've shared a comprehensive blog on migrating Hive’s UDFs, UDTFs, and UDAFs to BigQuery. It’s a deep dive into the practical strategies and best practices for this crucial migration step.

[www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery](http://www.aliz.ai/en/blog/how-to-migrate-hive-udfs-udtfs-and-udafs-to-bigquery)

Let's discuss the challenges and solutions. #DataMigration #BigQuery",2,1,Constant-Collar9129,2024-01-16 11:43:40,https://www.reddit.com/r/dataengineering/comments/19810ce/exploring_the_transition_from_hive_to_bigquery_a/,0,False,False,False,False
197zq93,Newbie question: What solutions should I use to suggest my users posts based on their preferences?,"I hope this question is all right for here. I tried to search the sub, but I'm not really sure what to search for to begin with, so here it goes.

&#x200B;

I'll have a database full of posts that have a description, tags, and the body text. I want to create a suggestion functionality where the user clicks a button, select their preferences and receives a number of suggestions based on what they chose or the other posts they liked, etc. 

I don't know what can be used for that, what solutions etc, that's why I need your help!

I know elasticsearch have this kind of functionality, as I've seen it being used at my work, but it was for a gigantic pool of items, so maybe it's overkill? Or maybe there's a better solution focused only on suggestions?

&#x200B;

Thanks a lot!",2,2,izotAcario,2024-01-16 10:23:35,https://www.reddit.com/r/dataengineering/comments/197zq93/newbie_question_what_solutions_should_i_use_to/,1,False,False,False,False
197rdxe,Next best steps - disconnected datawarehouse,"Hi everyone,

&#x200B;

I had been part of a data project going on for over 2 years now, however support was somewhat lacking, priorities shifted, and ownership was shrouded, all of this created confusion and unclear goals.

Fastforward 2 years we now have

\- data stored in AWS - S3

\- ETL transformation happening in Glue, and pushing these cleaned tables into Postgres

\- Multiple jobs in Postgres, tweaking the tables to make them 'bi ready'

\- Power BI connecting to Postgres via gateways, which sometimes has instability issues.

\- Big desktop model (>250 mb) published to the BI service, consumed by users

&#x200B;

&#x200B;

However the challenges I currently face are:

\- multiple transformations make changing / updating the solution challenging, I would very much like to simplify and take full ownership of the solution, offering stakeholders more agility and accountability on the reports

\- user cannot really query the data outside the reports: due to the sheer size of the model (+400 tables/views) it is unfeasible to have them navigate the tables or build custom views outside BI

\- Furthermore the 'analyse in excel' feature is not usable, as the olap cube in excel times out due to the sheer size (and lack of best practices I believe) of the data

\- Although the data warehouse has id and index columns, these are not connected, there is no real ERD between tables, making querying them even more challenging.

&#x200B;

The intended outcome would be:

&#x200B;

\- provide senior stakeholder a way to query the data (ideally some sort of excel pivot, would that an OLAP cube?)

\- Provide power users the ability to create their own reports in the service

\- Have alignment in definitions, ensuring accurate figures are reported across reports.

\- Simplify the solution a lot, and I mean A LOT, so that I can fully manage and service it.

&#x200B;

I dabbled with power bi fabric, and although I lack programming skills, I think this could be an alternative, pulling the tables from s3 or postgress, simplifying as much as possible in there, and then creating a curated dataset users could leverage.

Am I correct in saying that creating a cube for senior stakeholders and simplifying the solution for a better bi experience (star schema) are two completely different things, and if so, do you recommend one over the other?

&#x200B;

Any guidance would be very much appreciated, my background is of a business/data analyst not an engineer/scientist, therefore the lack of knowledge could prove challenging, however with 10y of exp in the field, I'm keen to broaden my professional horizons,a nd would take this as an opportunity.",2,1,turbo88988,2024-01-16 02:22:17,https://www.reddit.com/r/dataengineering/comments/197rdxe/next_best_steps_disconnected_datawarehouse/,1,False,False,False,False
197ic1z,"Help, How to determine the best option ?","Hello, I'm currently undergoing a BI internship at a company where I have to establish a data warehouse and subsequently create dashboards and reports while also implementing automatic data synchronization.

The company operates within the Microsoft ecosystem. With approximately 400 employees, it falls within the category of a medium-sized enterprise. It uses Microsoft AX ERP database and Excels files as datasources.

I have two questions:

* What are the available tools in microsoft ecosystem for performing ETL processes?
   * Performing ETL within Power BI.
   * Employing dedicated ETL software such as Azure Data Factory or SSIS... (Are there additional tools?)
* How do I determine the most suitable solution among the options mentioned in question 1, what aspects should I analyze and take into consideration?

Thanks in advance.",2,10,CulturalChemical5640,2024-01-15 20:04:01,https://www.reddit.com/r/dataengineering/comments/197ic1z/help_how_to_determine_the_best_option/,0,False,False,False,False
197i22i,Any Hiring Manager here that could shed some light what do you check for culture fit in a candidate?,"Pretty much the title. However, here is some background. 
I am a senior level Data Engineer skilled in pyspark, sql and azure cloud. Know in and out of Databricks too. I can write complex etl code in both functional and OOP style and create end-to-end orchestrated ETL with minimal supervision. I also work well with business teams and strive to create the utility that they are looking for. My past experiences have taught me to he humble no matter what. 
However, I recently i have been failing at behavioral rounds even though I exceed expectations at tech rounds. 
So, for all the Data Engineering Hiring Managers out there, what is it that you look for in a candidate beyond their technical competency. 
Your inputs here would benefit me immensely in improving myself.",1,25,chrgrz,2024-01-15 19:53:22,https://www.reddit.com/r/dataengineering/comments/197i22i/any_hiring_manager_here_that_could_shed_some/,0,False,False,False,False
19743jl,Data Quality & Observability Best Practice,"Hi folks, wanted to ask senior practitioners' POV on how to implement Data Quality and Data Observability using modern data engineering approaches.

**Context**

* I work an ""old-school"" enterprise that uses source systems like SAP ECC, SAP Concur, Anaplan, Yardi
* Our current state approach is to use ELT to land the raw data into ALDS Gen2, then do the subsequent transformations and data quality checks on Databricks and dbt Cloud 
* Our CIO (non-technical person) thinks this is ""inefficient"", believing that we should perform data quality checks on the source system itself and remediate any issues identified from the source 
* The challenge is I do not know of any tool that would allow us to connect to an SAP FICO table for instance, run the DQ rule, then trigger a workflow to write back to SAP to correct a DQ issue 

Would love to get the community's POV on what the best approach here would be ",2,4,Background-Proof5402,2024-01-15 08:41:34,https://www.reddit.com/r/dataengineering/comments/19743jl/data_quality_observability_best_practice/,0,False,False,False,False
19730nu,Freelancing advice,"Hi, I am working as a data engineer since past 4 years in the same company. My expertise is very limited currently; one etl tool and SQL.   
This year I want to leave this job but before that I want to delve into freelancing. Which skills can I learn in the realm of data that are somewhat in high demand? Should I focus on data cleaning or visualization? Or should I learn cloud?

&#x200B;",2,7,Round-Joke5208,2024-01-15 07:29:42,https://www.reddit.com/r/dataengineering/comments/19730nu/freelancing_advice/,0,False,False,False,False
196zhaf,Changing database technology,"Hello,

If some teams are well versed with the Oracle database architecture and its optimizers working and designed application on top of this. Now moving same team to work on aurora postgresql/mysql databases design/development [projects. Is](https://projects.Is) any key design/architectural changes should the app development team or the database design team, should really aware about, so as to take right decision on any new development project in AWS aurora postgresql database?

Or

Is there any list of differences(as compared to Oracle database) in key concepts like for example basic design concepts, Normalization, Partitioning, clustering, backup and recovery, Indexing strategy, isolation level, performance which one should definitely be aware of? ",2,0,Big_Length9755,2024-01-15 04:07:17,https://www.reddit.com/r/dataengineering/comments/196zhaf/changing_database_technology/,1,False,False,False,False
196siba,How to test pytest function containing a multi sum aggregation?,"Hey all,  


We are currently struggling on how to structure our tests for glue pyspark. 

Below is an example of one of our aggregation functions

    def calculate_aggregations(variable):
        aggregations_df = variable.groupBy(""column1"", ""column2"".agg(
        sum(""column3""),
        sum(""column4""),
        sum(""column5""),
        ...
        sum(""column20"")
    
        return aggregations_df 

  
In our current pipeline we have about 5 of these.  


We have multiple options for how we could test these but we cannot figure out which is best in terms of managing the tests and coverage. Im not going to outlay how we currently test them as i would like to get peoples perspective.  


Our options

1. unit test that each column produces the right summed value
2. unit test the function with each column populated sums to each column correctly
3. unit test multiple rows at once and check the outcome is expected
4. integration test that the final end result produces what we would expect (end to end test which would assume these are aggregating correctly if we get the right values at the end  


We have also discussed breaking these bigger aggregations up into smaller ones, but we arent sure how to do that without sacrificing performance. Something like

    def calculate_aggregations(variable):
        aggregations_df = variable.groupBy(""column1"", ""column2"".agg(
        sum(""column3"")
    
        return aggregations_df 
    
    def calculate_aggregations(variable):
        aggregations_df = variable.groupBy(""column1"", ""column2"".agg(
        sum(""column4"")
    
        return aggregations_df 

Love any perspective on this, and any resources that you may have in terms of doing this properly

  
Thank you!",2,5,Manyreason,2024-01-14 22:39:33,https://www.reddit.com/r/dataengineering/comments/196siba/how_to_test_pytest_function_containing_a_multi/,1,False,False,False,False
196lma9,Looking for some guidance around online data platforms,"By way of introduction - I’m primarily an econometrician by trade and focus on building smaller scale econometric/ML models for clients. I typically work with a range of datasets, but importantly it’s collectively small enough to fit in to local memory and work with using R or Python.

As the lines between data science and econometrics continue to blur, I find myself working more and more with much larger datasets. Too large to fit in to local memory.

As an organisation we use Snowflake to store data that we don’t want on our network drives (100gb+). As of now, I use Python's Snowpark library to aggregate large datasets on snowflake in to a level where I can transfer locally and then deploy an ML model locally. I'm also comfortable writing the SQL directly. 

However using cloud or online data platforms hasn't really been my bread and butter and wondered if anybody could offer any advice around the following:

1. I'm unsure if its best practice to aggregate on the cloud and then analyse locally. Supposing its not - is it possible to deploy a ML model within a snowflake environment instead? In other words – all data wrangling and model execution is done on the snowflake engine.
2. I hear a lot about databricks as an alternative to snowflake, but struggling to understand what it offers, and what it can do that’s different? Could anybody help to explain this to me like you would to a small dog?

Thanks a lot! ",2,3,LDM-88,2024-01-14 17:51:03,https://www.reddit.com/r/dataengineering/comments/196lma9/looking_for_some_guidance_around_online_data/,1,False,False,False,False
196kexn,Data sets for Open Source package downloads (Maven / NPM / etc),"Posting here as I'm not big into the DE world, but figure yall probably know where noobs like me could find data sets to brush up with.  

Does anyone know if there are any downloadable data sets for downloads of open source packages? NPM / Maven / Composer / etc – any statistics like that would be helpful!

&#x200B;

P.s. I know I could try harvesting from APIs, but there's a LOT of stats to download, and I can't help but imagine someone's done this.",2,0,dwelch2344,2024-01-14 16:59:49,https://www.reddit.com/r/dataengineering/comments/196kexn/data_sets_for_open_source_package_downloads_maven/,1,False,False,False,False
196gjd0,Kedro Intro and Hello World example,"Kedro is often overlooked in Data Science projects despite offering structure, caching and tracking datasets, MLOps features as well as powerfull intergrations with other Data tools",2,1,dnulcon,2024-01-14 14:01:32,https://youtu.be/r-oQ701wgDQ?si=roh4zqGCb7pithUY,1,False,False,False,False
196eln8,Understanding the Rationale Behind Writing Subset of Data Back to Azure Blob in ETL Process,"Hi All,

I'm following a Microsoft Tutorial on MS Fabric. I'm working on an ETL process using Azure Blob Storage and Apache Spark, and I've come across a step in the process that I'm trying to understand better. The workflow involves reading a large dataset from Azure Blob Storage, processing it, and then writing the first 1000 rows of this dataset back to Blob Storage. After this, the script reads this subset again, performs further transformations, and finally loads it into a Delta table.

My question is: What could be the rationale for writing a subset of the data (1000 rows) back to Azure Blob Storage, only to read it again for further processing and loading into a Delta table? Are there specific benefits or use cases for this approach, such as performance optimization, data segmentation, or testing purposes that I might be overlooking?. Here is the link to the tutorial:  


[https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/10-ingest-notebooks.html)

Any insights or experiences you can share regarding this method would be greatly appreciated, especially if there are more direct or efficient ways to handle such a workflow.

Thanks in advance.",2,1,Ok-Necessary-6455,2024-01-14 12:15:09,https://www.reddit.com/r/dataengineering/comments/196eln8/understanding_the_rationale_behind_writing_subset/,1,False,False,False,False
195z6b2,Streaming Pipelines Architecture with Kafka and Spark,"Hello fellow redditors!

So I’m beginning to get interested in building streaming pipelines. Things like a near real-time data ingestion (a bronze table, for instance), silver tables or even building near real-time specialized datasets (gold datasets).

I know from some researches that in service oriented architectures companies use apache kafka as a integration layer/hub for consumers and producers.

As far as I’m aware a producer generates some event and sends it for the kafka broker and the consumer can process it right after, and depending on the architecture the consumer even deletes (?) the event that already has been processed to avoid duplication.

If I’m a data engineer interested in the data that this producer pushed to kafka, how would I reliably get access to this data, without intruding into the integration between the microservices? 

I’m looking into kafka connect as something that tries to solve this issue, but I’m quite unsure.

Can someone with more experience in these streaming context bring some light to the subject?",2,2,gabbom_XCII,2024-01-13 21:50:06,https://www.reddit.com/r/dataengineering/comments/195z6b2/streaming_pipelines_architecture_with_kafka_and/,1,False,False,False,False
195l6dn,How hard is it for a Data Engineer to move into ML/Data Science?,"I have completed a few projects on data engineering (real world projects and not demo projects). I know how and where to use the common tools. I have mostly been doing this on Azure. So I am familiar with services like synapse analytics, data factory, databricks (pyspark) , SQL databases. And PowerBI for visualization purposes. I had done the basics of ML(classification, clustering, some simple Deep learning, etc...) a few years ago. But that's not enough. How much time would it take to get decent at ML and and probably be amongst the top 15% in this industry globally?",2,12,_areebpasha,2024-01-13 10:31:52,https://www.reddit.com/r/dataengineering/comments/195l6dn/how_hard_is_it_for_a_data_engineer_to_move_into/,0,False,False,False,False
19572tl,"""Entry Level"" Salary","Hello,

Apologies if I shouldn't post this here. Long story short, I have been interning at a tiny startup (10 employees total) as the ""data guy"". My responsibilities have so far been a sort of data full stack: tons of web scraping, data cleaning, applying off the shelf algorithms, dashboarding, etc. They want to hire me on fulltime. Most of my time will be spent building data pipelines, building out a data infrastructure, building multiple front end for internal tools, etc. What's a reasonable salary expectation?

\- This is my first role in a data position/tech

\- Currently getting my masters in Data Science

\- Self-taught for 2 and a half years

\- Location is in a USA tech hub (Not CA WA or NY)

Thank you!

\*edited for location",3,19,crispybacon233,2024-01-12 22:00:10,https://www.reddit.com/r/dataengineering/comments/19572tl/entry_level_salary/,0,False,False,False,False
19572kd,"🚀 Introducing UCX v0.9.0: Enhanced Assessment, Migration, and Error Handling",,2,0,serge_databricks,2024-01-12 21:59:52,https://medium.com/databricks-labs/introducing-ucx-v0-9-0-enhanced-assessment-migration-and-error-handling-3ccc006e0e26,1,False,False,False,False
1950045,Replication options from Oracle,"Hi, looking for some suggestions on replicating an on-prem oracle DB to either Azure or a SQL Server db. ~10 million rows, so not huge by the standards of this sub.

We run dagster as our orchestrator right now (self hosted) so looking for something that integrates with it. No K8 either or CDC for the oracle source...

Lots of chatter around airbyte being an option, but not a huge fan of the startup-cost of it (# of containers for example) and the lack of K8. Does feel like the most straightforward option; certainly divided opinions on this sub about it.

Generally we have steered clear of data factory, but still on the table. I don't think it would play nice with dagster though.

My current best feeling is to just work through the data in chunks and replicate by date time. Combining that with partitions in dagster to make it manageable. Does feel a bit over-worked to have it essentially doing what AB or Meltano is doing (meltano has terrible oracle taps from my experience). plus i'd be dealing with the pyodbc overhead (no spark currently, but also on the table!)

Initial sync would be slow, but the incremental chunks after would be ok. Downside is i need to essentially store the whole block in memory or disk while replicating.

Suggestions/thoughts? Appreciate the help dealing with these ""legacy"" systems... cry for me too...",2,7,Namur007,2024-01-12 17:04:48,https://www.reddit.com/r/dataengineering/comments/1950045/replication_options_from_oracle/,0,False,False,False,False
194bqr6,Question: What have been the most useful techniques in tagging your dbt models?,"I'm curious how people have used the tagging features, and if they follow any particular principles when tagging their models?",2,1,AMDataLake,2024-01-11 20:35:22,https://www.reddit.com/r/dataengineering/comments/194bqr6/question_what_have_been_the_most_useful/,1,False,False,False,False
1949o8w,Tools/processes for running data change scripts in a release?,"I am looking for guidance on how people currently manage the process for promoting a releasing data change scripts into their production environments.


Requirement:

I need a method for making data changes to tables in SQL server as part of a DevOps release process into our production database.


Current state of play:

I have a set of metadata tables which drive Azure Data Factory pipelines that move data between 3rd party systems and our on-premise (SQL server) DB.

These metadata tables are deployed using a single, huge script which drops, recreates and populates the tables every time we perform a release into Production. 

This causes us a lot of problems with complications such as difficulty finding the right lines of code to amend, commit conflicts and people forgetting to update the script after making emergency ad-hoc changes which then get overwritten on the next deployment.


Proposal:

We want to move to a process where we can commit small data change scripts (Inserts, Updates to a single row etc.) alongside any other changes into our code repository. These data change scripts would sit in a 'PreDeployment' folder.

We then, as part of the production release process need a way to pick up the scripts in this folder and run only those that have not previously been run, in the correct order (scripts will be numbered). 

Tools such as Liquibase and Roundhouse appear as if they will do this job nicely, but in order to test these I will have to write proposals, go through a procurement process etc. so it would be great to know I'm doing the right thing before going through this.



My questions to you:

1. Is this the correct process for changes such as these? Is there a better alternative out there?


2. If this process is correct then has anyone used any of these change management tools and do you recommend them? 
Bonus points if you know a good tool that is owned or backed by Microsoft as this will make the process for me to gain access to the tool substantially easier.


3. Has anyone written a process for running scripts such as these themself instead of using the tools? 


Thank you in advance for any help and advice!",2,2,p1stash,2024-01-11 19:09:48,https://www.reddit.com/r/dataengineering/comments/1949o8w/toolsprocesses_for_running_data_change_scripts_in/,1,False,False,False,False
19412ic,Negotiating job title - Staff Data Engineer or Expert Level Support,"I got an offer from a consulting company as Expert Level Support - Level 3 (There's no Level 4). I originally applied for Big Data Engineer, which they changed later during my interview process. My team manager uses DevOps Manager on his LinkedIn.

I read that some people have tried to negotiate their titles during the job offer stage. Is it a good idea to negotiate it to Staff Data Engineer / Principal Data Engineer or even DevOps Engineer? Which title aligns better with expectations in the industry?

The thing is, I'm not sure if I would apply to another DE role in the future or try DevOps. Perhaps the vagueness of the current title could work in my favor if I ever want to explore other opportunities, perhaps not. Are there significantly more remote opportunities for DE over DevOps? That could be a factor.",2,4,aguhon,2024-01-11 12:55:24,https://www.reddit.com/r/dataengineering/comments/19412ic/negotiating_job_title_staff_data_engineer_or/,1,False,False,False,False
193vqtm,Web scraping via executing custom javascript.,"Hi everyone, 

I’ve come across a particular use case where we have to scrap some website data and the website here in question is Indeed.com

Now the idea is to scrap job details from a particular post but on the website the job details are displayed in tile format and we cannot scrap more details from that respective tile. Assuming major part of our requirements are part of the hidden html tags which when expanded gives us more information of that particular job post.

We’re using the following snippet in order to do so. However we need some context or approach or guidance where we’re going wrong. Since the data extracted is in html and in a non readable format.

Sharing the code snippet below for your reference. 

Requesting your help and guidance on the same

Code snippet:

// Select all elements with the class name ""jcs-JobTitle css-jspxzf eu4oa1w0""
var jobTitleElements = document.getElementsByClassName('jcs-JobTitle css-jspxzf eu4oa1w0');
 
// Iterate through the elements and click on each one
for (var i = 0; i < jobTitleElements.length; i++) {
  jobTitleElements[i].click();
}",2,2,apache444,2024-01-11 07:11:11,https://www.reddit.com/r/dataengineering/comments/193vqtm/web_scraping_via_executing_custom_javascript/,1,False,False,False,False
193sfou,Impact to Key Strategies When Serving a ML Use Case,"I was recently hired as a Lead Data Engineer of a pipeline that will serve ML (Machine Learning) models on a Google Cloud & Snowflake stack. I always learned that  is best to understand the end goal of the data pipeline and if it will serve reporting, analytics, or machine learning. I come from a background of pipelines for mostly reporting and analytics use cases. I will be responsible for architecting, modeling, enhancing, and managing the pipelines.

I am curious as to how a ML use case will fundamentally impact my decisions/approach when it comes to some of following key considerations:

* Data Sources
* Determining Ingestion Strategy
* Processing Plan
* Storage Setup
* Monitoring and Governance
* Data Quality and Validation
* Data Drift Monitoring
* Consumption Layer
* Currently ETL (Should I push for the more moden ELT approach)?

Thank you in advance for all guidance and insights!",2,4,JoseyWales10,2024-01-11 04:01:30,https://www.reddit.com/r/dataengineering/comments/193sfou/impact_to_key_strategies_when_serving_a_ml_use/,1,False,False,False,False
193qo6p,Model Deployment,"So I’ve been working on this machine learning model to detect improper payments. 

The performance isn’t exceptional but it’s acceptable given the data. 

The model is saved and works well. The issue I’m having now is deploying and automating the model to run everyday. 

The process currently is I have a sql automation to export the features needed to input into the model into an excel file. 

I use python to open the excel and input the the features into the model and return the probability for each one. 

Im thinking the excel file is probably a bit excessive as I could use python to connect to database and gather the features from there then input in the model. Without opening an excel file directly. 


However, after the features are exported. Everything is done manually. What are some ways I could automate running the model daily. 


I’m thinking of just using a batch file to run the python script. However, I’m not sure if there’s a more efficient way to do this. 

After that I wanna put this all on a tableau dashboard.

Any suggested work around for this, trainings or whatever?",2,7,CaptainVJ,2024-01-11 02:33:43,https://www.reddit.com/r/dataengineering/comments/193qo6p/model_deployment/,1,False,False,False,False
193e279,Production Fuzzy Entity Linking / Disambiguation,"I'm researching fuzzy entity linking and disambiguation in production environments and am interested in learning from your experiences. Specifically, I'd like to understand:

* Sources and Schemas: What types of sources are you linking (details as permissible), and how many? How similar are their schemas? Can you provide examples of fuzzy or ambiguous cases?
* Ground Truth: Do you use a standard knowledge base like Wikipedia as ground truth, or is your ground truth more dynamic, emerging from the disambiguation process itself?
* Approach and Challenges: What's the general strategy of your approach? What aspects are particularly time-consuming, costly, or complex? Any notable edge cases?
* Deterministic Methods: If using a deterministic approach, how do you balance false positives and negatives? How crucial is atomicity?
* Tooling: Are there specific tools or strategies that you found effective?  Specifically interested in non-Wikipedia linking here, so excluding Azure AI Language.

I appreciate any insights or strategies you can share, especially those backed by real-world examples. Thank you.",2,2,emu-la-tor,2024-01-10 17:41:05,https://www.reddit.com/r/dataengineering/comments/193e279/production_fuzzy_entity_linking_disambiguation/,1,False,False,False,False
193d288,What's the best code based ETL tool for working with Salesforce and AWS.,"It should also be able to take care of functionalities like scheduling, error handling, auditing past runs , alerting for errors etc. 

The main use case of this is exporting and importing data to and from Salesforce and AWS.

I am very new to this, Sorry, if this is a ill informed question.

Edit:

Where?

I need to take the data usually from aws s3 and upload it Salesforce. Its usually in csv format but could be in any format. Also I am interested if it could use other aws services like secrets manger etc.

Why?

The reason for doing this is data integrations and migrations from 3rd party systems.

Any transformations?

Yes, since there are all from 3rd party system. I need to do a lot of transformations for it to fit the data structure of salesforce.Changing the date format,Mapping the values to similar but different values present in salesforce,Integer to BigDecimal,Concatenating data,Splitting data,Being able to do queries to find lookup values that are already uploaded in the system(Salesforce) etc.

How much data?

The data is usually not very much around 50,000 rows of data on some of the biggest object exports. Other objects are usually much lower than that average would be 20k rows of data records.

How often?

Integrations needs to be done more often daily, weekly, monthly. And they take less time to build as well. Compared to full data migrations they are one time thing but take a lot of time to built. Since the data is coming from a 3rd party.",2,8,Brilliant-Republic75,2024-01-10 17:00:41,https://www.reddit.com/r/dataengineering/comments/193d288/whats_the_best_code_based_etl_tool_for_working/,1,False,False,False,False
193chiq,Data governance: who is allowed to create new views in your data warehouse?,"Genuinely curious what the standard is for this, or if there is a best practice? Who is allowed to create new views? Only the data engineers?",2,4,icysandstone,2024-01-10 16:37:05,https://www.reddit.com/r/dataengineering/comments/193chiq/data_governance_who_is_allowed_to_create_new/,1,False,False,False,False
193bc8w,Data Warehouse Analytics - The Latency Problem,,2,0,dataengineeringdude,2024-01-10 15:48:44,https://dataengineeringcentral.substack.com/p/data-warehouse-analytics-latency,1,False,False,False,False
1939sm3,Specialisation vs End to end analytics?,"What is better for a career, specializing or being able to work on a data analytics project end to end(DE, DBA, BI)?

Generally, I tend to lean to its better to be great at one thing than good at many as long as you can work 8h or more. 

But, on the other hand it is good to be able to see the whole picture and it might help moving into an architectural/managment role.",2,1,Interesting-Monk9712,2024-01-10 14:41:18,https://www.reddit.com/r/dataengineering/comments/1939sm3/specialisation_vs_end_to_end_analytics/,1,False,False,False,False
1938z3i,Setting a stable environment for data pipeline,"I'm a Jr Data engineer, and  I always thought the most frustrating part in the DE world is setting up a local environment for your side projects, especially all tutorial starts with the coding with little or no information on that first initial step which is setting the environment what are you thoughts on this ? And do you have any tips on how to build a stable local environment  ( feel free to propose the stack you're using and OS  )",2,2,mechniwegeff,2024-01-10 14:03:17,https://www.reddit.com/r/dataengineering/comments/1938z3i/setting_a_stable_environment_for_data_pipeline/,1,False,False,False,False
19345cj,Dataform & CI/CD,"Hey pairs,  


hope you're doing well. I'm integrating dataform in a CI/CD with terraform handling provisioning.  
I have some questions tho if you're using it in your data stack:  
\- How do you handle CI/CD?   
\- Is there a way to now have to go in the UI and apply the commits from the third party repo to the dataform GCP repo? Automatizing it?  


Thank you lots",2,2,anfawave,2024-01-10 09:10:47,https://www.reddit.com/r/dataengineering/comments/19345cj/dataform_cicd/,1,False,False,False,False
19317lt,Need some help with dbt,"Need some help with dbt

Hi everyone I have started my learning journey in dbt. I have understood the basic concepts plus how to work with a standard dataset. But I m really confused when it comes to higher level when I m dealing with lots of table and huge data. 

How can I make my models generic? 
Do I need to purely use macros for this? 
How does a dbt project structure look like? 

Would be grateful if someone could guide me through this.",2,4,misaaaa18,2024-01-10 05:57:55,https://www.reddit.com/r/dataengineering/comments/19317lt/need_some_help_with_dbt/,1,False,False,False,False
192yp9u,How does your company handle ETL/ELT processes?,"I originally posted this in /r/dataanalysis, but in hindsight it likely makes more sense for this audience, so asking it here as well.

I work in higher education as a senior data analyst. As we have been adopting more and more external data sources (APIs, cloud-based databases, SFTP dumps), it has become clear that we need a formal ETL solution. We already have an on-premise data warehouse and staff to support it. As we start to look into whether we should buy a tool or train staff on writing custom python scripts for everything, I was hoping others at organizations might share what they do.",2,4,farm3rb0b,2024-01-10 03:44:00,https://www.reddit.com/r/dataengineering/comments/192yp9u/how_does_your_company_handle_etlelt_processes/,1,False,False,False,False
192wo0c,Exploring the Scope of Data Engineering in India as a Fresher: Seeking Advice and Insights," 

I am currently a 3rd year College Student with a keen interest in pursuing a career in data engineering. I've been researching about the field and its prospects in India, especially for someone entering as a fresher.

I would greatly appreciate insights and advice from the **experienced members** of this community. a few questions:

1. **Current Scenario in India:** What is the current state of the data engineering job market in India? Are there ample opportunities for entry-level positions?
2. **In-Demand Skills:** As a fresher, what specific skills or technologies should I focus on to make myself more marketable in the field?
3. **Industry Preferences:** Are there specific industries or sectors in India that have a higher demand for data engineers? Any emerging trends worth noting?
4. **Certifications:** Are there any certifications that are highly regarded in the Indian job market for data engineering roles?
5. **Advice for Freshers:** What advice would you give to someone like me who is just starting out in their data engineering journey in India?
6. **Networking Opportunities:** Are there any local or online communities, meetups, or events in India that you recommend for networking and staying updated on industry trends?",2,2,Aj412803,2024-01-10 02:03:56,https://www.reddit.com/r/dataengineering/comments/192wo0c/exploring_the_scope_of_data_engineering_in_india/,1,False,False,False,False
192v7q3,AWS or GCP certification - NA job market value?,"Hello, I'm torn between acquiring an AWS or GCP certification. I've looked through past posts of similar discussion topics and I understand that cloud skills are transferrable, but in this challenging job market I'd like to make myself stand out to recruiters while trying to break into a DE role.

**GCP**: I have the most experience working with it at work and through personal projects. Any experience I gain can potentially be transferable to my current job (hybrid on-prem & cloud). Seems to have the smallest market share in North America, and thus less job opportunities.

**AWS**: has the largest market share in NA and more job opportunities. I have no hands on experience working with it.

&#x200B;

Some background about me: I have an MEng with a focus on Data Science/Analytics, with almost 3 years of DA experience working as the only data specialist at a small org based in Canada. I've been reading/studying DE and worked on DE pet projects outside of work for the past 1.5 years. Due to limited growth opportunities in my current role, I've been applying for DE positions the past 6 months with limited callback success for interviews, but I did make it to the final round for one company. Currently, looking to grab some DE certifications to boost my credentials.  


I'd appreciate any advice related to certifications or career expectations!",2,0,SpacExclamationmark,2024-01-10 00:56:39,https://www.reddit.com/r/dataengineering/comments/192v7q3/aws_or_gcp_certification_na_job_market_value/,1,False,False,False,False
192c93r,How do you guys approach High Availability and DR?,"Considering data storage, processing or visualization. Do you guys put much effort in duplicating these capabilities or implementing observability/automations to reduce downtime?

Also, do you guys prefer multicloud solution or does your service provide a good enough SLA?

My case: my stack basically consist of BigQuery/Airflow/PowerBI.
For instance, BigQuery has a SLA of 99,9%. But Im pondering about duplicating storage and processing on Redshift/Synapse to increase SLA. But I dont know if this is common practice.",2,9,RobvicRJ,2024-01-09 11:05:02,https://www.reddit.com/r/dataengineering/comments/192c93r/how_do_you_guys_approach_high_availability_and_dr/,1,False,False,False,False
1922oee,Migrate legacy database ETL to newer DAG-based,"Probably only one thing that is more painful than working with legacy ETL stored procedures/DMLs is migrating them.

New data teams normally start with a data analyst or data scientist come up with a number of SQLs, then setting up cron jobs materializing views into tables. Over time, it becomes a pile of mess. Data leanage exists nowhere but in their minds. Tables being updated by god-knows DMLs.

I have seen a lot of efforts in migrating existing SQL codes and DML to DAG-based solution like DBT. But doing so manually is extremely tedious. I wonder there must be automation service for this already.

Has anyone come across this problem?",2,0,JayDoDr,2024-01-09 01:48:10,https://www.reddit.com/r/dataengineering/comments/1922oee/migrate_legacy_database_etl_to_newer_dagbased/,1,False,False,False,False
191x6w3,Get multi-table join results without running the query,"Hi all,

I am trying to think of ways to approximate the count of records returned on a series of queries without processing the query. 

I am using snowflake, and we create a number of temporary tables that are either inner or outer joined together to support AND / OR logic. 

For example, say we have Temp Table A, Temp Table B, Temp Table C and we want a count of records for ids that appear in ((A AND B) OR C). Is it remotely possible to approximate a count without running a query? My first thought is to use **HyperLogLog** algorithm ([https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:\~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization](https://redis.io/docs/data-types/probabilistic/hyperloglogs/#:~:text=HyperLogLog%20is%20a%20probabilistic%20data,accuracy%20for%20efficient%20space%20utilization)), however this seems to be more applicable for interacting with 2 tables and finding the approximate intersection. 

We don't need to be very precise, just a ballpark count. Any ideas out there to estimate the count of multiple tables being joined together?

&#x200B;",2,3,lt-96,2024-01-08 21:52:04,https://www.reddit.com/r/dataengineering/comments/191x6w3/get_multitable_join_results_without_running_the/,1,False,False,False,False
191k13k,Seeking Advice: Beginner-Friendly Projects to Dive into Apache Airflow Learning,"Hello, community! 🚀 I'm eager to kickstart my journey into Apache Airflow and looking for suggestions on beginner-friendly projects. Any insights, recommendations, or hands-on learning ideas that can help me grasp the ropes of Airflow effectively? Thanks in advance for your valuable input!",2,3,ramesh4f,2024-01-08 12:22:26,https://www.reddit.com/r/dataengineering/comments/191k13k/seeking_advice_beginnerfriendly_projects_to_dive/,1,False,False,False,False
191jbc9,Premier League football data pipeline,"The [project](https://github.com/JawaharRamis/PL-Football-ETL-with-Airflow-AWS) orchestrated using Airflow retrieve raw data from an api saving into an S3 bucket which is then transformed using a glue job and then stored onto another S3 bucket. The processed data is fed into Quicksight for visualizations. 

I have been trying to build up my DE portfolio hoping to land a job in this field. You can find more of my projects in my [Github](https://github.com/JawaharRamis). Most of these projects helped familiarize myself with the tools. My goal is to develop a really good DE project within the next few months which could help me really stand out. 

Share me your advices/suggestions on my project and also how I could build up my portfolio further.",2,1,jawz96,2024-01-08 11:38:43,https://www.reddit.com/r/dataengineering/comments/191jbc9/premier_league_football_data_pipeline/,1,False,False,False,False
191j9iy,Airbyte Cloud or self hosted on EC2,Do you use Airbyte cloud or self hosted on EC2 for production data integration pipelines? Mainly looking for only data ingestion tool.,2,3,Liily_07,2024-01-08 11:35:27,https://www.reddit.com/r/dataengineering/comments/191j9iy/airbyte_cloud_or_self_hosted_on_ec2/,0,False,False,False,False
191a1k3,GCP Data Engineering Jobs,I’m a Data Engineer having most of the experience in GCP cloud. I see more Data Engineer job postings on AWS cloud comparatively than GCP. How do companies look at a candidate resume when their tech stack doesn’t match but has the relevant experience.,2,3,PurpleCurrent3576,2024-01-08 02:25:49,https://www.reddit.com/r/dataengineering/comments/191a1k3/gcp_data_engineering_jobs/,0,False,False,False,False
1907hhl,Terraform Google Cloud,"Just leaving the link to my Terraform repo here. It's set up for various GCP services that I'm using in a project. Might be useful for some of you.

* **VPC Configuration**: A new VPC is configured with the intent to utilize all services within a single VPC.
* **Compute Engine Setup**: A compute engine is set up to read the **Cloud SQL** via a private IP, configured in another module. A startup-script.sh is included, which has code to establish SSH connection directly from the local machine.
* **Cloud Run Creation:** A Cloud Run is created in version V2 with Direct VPC Connection to Cloud SQL.
* **Automated Cloud Build Mechanism**: A mechanism is set where the cloud build is triggered automatically following the code push to the repository that builds the image.

[https://github.com/mazzasaverio/terraform-gcp](https://github.com/mazzasaverio/terraform-gcp)",2,0,Xavio_M,2024-01-06 19:11:49,https://www.reddit.com/r/dataengineering/comments/1907hhl/terraform_google_cloud/,1,False,False,False,False
19019li,Thoughs on date formats,"A random though crossed my mind while working on a new datasource.

Usually we accommodate the date format for each data source, some are dd/mm/yyyy others are mm-dd-yy and so on.

How about converting everything to Unix epoch and leave the format for anyone consuming the data from the warehouse.",2,15,Human-Failure-99,2024-01-06 14:34:42,https://www.reddit.com/r/dataengineering/comments/19019li/thoughs_on_date_formats/,0,False,False,False,False
190115j,MiniFirehose - A lightweight data buffering and delivery system," 

Hi everyone!

I wanted to share a project I made in just a week. It's called MiniFirehose (somewhat similar to AWS Firehose). It's a really simple tool for managing data, kind of like Kafka or MQTT but much easier to use and lighter on resources.

With MiniFirehose, you can collect messages and send them to places like your local filesystem or Amazon S3. It's not complicated to set up, and it's great if you're working on something small and don't need a big system.

Since I made it pretty quickly, there might be some small bugs. If you try it and have any ideas on how to make it better, I'd love to hear from you. Or, if you know other tools like this, let me know! so I don't spend much time on it.

You can check it out here: \[[mini-firehose](https://github.com/waqar-ahmed/mini-firehose)\]. I'm looking forward to hearing what you think!

Thanks!",2,2,wqrahd,2024-01-06 14:23:02,https://www.reddit.com/r/dataengineering/comments/190115j/minifirehose_a_lightweight_data_buffering_and/,1,False,False,False,False
18yhfnc,Help a 3rd world country colleague about hist future,"Hello my fellow table guys.

Can you help me about my future?

I'm from South America, working as DE for the biggest bank in Brazil. With my fiance, we're planning to immigrate to Canada, for multiple reasons, by the end of the year or the begining of the next. Can you guys help me with some stuff?

1 - What are the most in demand skills to data engineering in Canada?

2 - What do you think is the best city for me to keep my career in data engineering?

3 - How do you think is the most effective way to create network and apply to jobs?

4 - What to avoid?

5 - How about remote jobs? Does it still exists?

6 - Any other suggestions?",3,2,SantoPVL,2024-01-04 16:39:51,https://www.reddit.com/r/dataengineering/comments/18yhfnc/help_a_3rd_world_country_colleague_about_hist/,0,False,False,False,False
18yavrc,One-off operations against data warehouse,"What do you use to run and track one-off operations against the warehouse? 

In our team we have some custom tooling that effectively allows us to:

* Write arbitrary SQL script files to git repo
* Script will be executed on merge to main
   * If it fails, you can re-submit the same file, and it will re-run
* Scripts will be executed in order

We use this for 1-off DDL (eg. create schemas), some access controls definitions, 1-off data migrations (copying data around), and probably some other operations.

The annoying part is it relies on manually defining a sequential ID for each file, used so the tooling can track what has been run to prevent re-execution (or force re-execution on failure), and this invariably causes clashes and forces manual resolution of file IDs. It shouldn't be too much hassle to change it so that it works of file md5 hashes, but we'd need some extra work to allow sequential execution and I wonder if there's any off the shelf tooling people use to achieve the same.

Some of what we use it for is arguably better managed another way (eg. [brainly tf provider for user and access controls](https://registry.terraform.io/providers/brainly/redshift/latest/docs)), but what do you use for operations you do want to:

* Execute once on success
* Re-execute on failure after modification / resubmission
* Track what was defined / executed
* Integrate with monitoring / logging to alert and surface failures
* Allow sequential execution (ideally, not 100% sure required)
* Ideally also provides additional capabilities for development workflows such as validating execution before deploying (verify sql validity, commit & rollback?)",2,0,Turbulent-Profile-27,2024-01-04 11:23:13,https://www.reddit.com/r/dataengineering/comments/18yavrc/oneoff_operations_against_data_warehouse/,1,False,False,False,False
18y7jzw,Need Advice: Data Analyst to Data Engineer,"Hey there, I'm currently working in a B2B SaaS startup as a data analyst with a role that's a mix of data analysis and data engineering. I've been involved in various data engineering tasks, such as creating pipelines from ELB to BQ using Python, leading a project on Product Metrics involving ETL (shell scripting), dashboarding, and handling pipeline failures. My team is considering exploring Airflow as well.

I'm contemplating whether to switch jobs because my current title doesn't fully reflect the work I'm doing, or should I stay and continue learning in my current organization. I know it might seem like I'm fixated on the title, but I'd appreciate any advice or insights. Thanks!",3,5,IntrovertCheesecake,2024-01-04 07:42:20,https://www.reddit.com/r/dataengineering/comments/18y7jzw/need_advice_data_analyst_to_data_engineer/,0,False,False,False,False
18y21pg,Title Question - Applications,I internally transferred from a data analyst to data engineer. A year in we did a reorg and data engineering moved from R&D to Data Ops. My title changed to “Data XYZ Engineer”. When I go through external DE and SWE interviews should I list the title I was hired for or the non standard one I was moved to? It’s not an industry term and is obviously management trying to lower starting wages for new hires that can’t get a job. I build ETLs/data pipelines and just finished transitioning us to the cloud.,2,5,OkMacaron493,2024-01-04 02:41:42,https://www.reddit.com/r/dataengineering/comments/18y21pg/title_question_applications/,1,False,False,False,False
18xsni9,What salary % increase would it take for you to leave your current role?,"Genuinely curious. Assuming you were interviewing for a role you thought would be a good fit, what % salary increase would you want/need to make the jump?   
A few days back I posted about taking a new role and a handful of comments were made around salary increase (what % is worth it or not).   
In your current role what % increase do you feel is ""worth it"" to you when entertaining taking a new role?

[View Poll](https://www.reddit.com/poll/18xsni9)",2,18,MasterKluch,2024-01-03 20:07:32,https://www.reddit.com/r/dataengineering/comments/18xsni9/what_salary_increase_would_it_take_for_you_to/,0,False,False,False,False
18xoeqv,It is better to host Airbyte in EC2 or AWS lightSail?,"Hi everyone, I'm build a test modern Data Stack and intent to use Airbyte Self hosted version. I was going to host into a t2.medium EC2 but saw some people hosting a lot of docker containers inside AWS LightSail. 

Does LightSail is cheaper than a EC2 in that situation? 

Also, I want to use Prefect, Postgres and DBT. A friend told me that I could to all of this in one LightSail instance. What you guys think? ",2,6,EconomySuch7621,2024-01-03 17:19:33,https://www.reddit.com/r/dataengineering/comments/18xoeqv/it_is_better_to_host_airbyte_in_ec2_or_aws/,0,False,False,False,False
18xnwp2,Container management for EDA workflow on kubernetes.,Recently came across a design decision while implementing EDA using celery canvas workflows to distribute celery tasks and workers across multiple containers or keep them at one place. Need y'all to share an opinion on my piece and appreciate any tips or help on this.,2,0,Drphysics5,2024-01-03 16:53:55,https://medium.com/@rajani.param1/ce-238793394b0d,1,False,False,False,False
18xmqus,What are some good UDEMY courses for data engineering skills ?,"What Udemy courses would you recommend   
Possible topics:   
\- ETL/ELT

\- Python for data engineering

\- Designing data warehouses and datamodeling

\- Use of cloud tech (AWS, Azure)

\- Optimize data models and structures for maximum - performance and scalability.

\- integrating new database technologies.

\- Optimizing query performance and data access for users.  


I am currently pursuing a degree in Big Data, but it lacks hands-on projects in data engineering technologies. ",3,3,SteffooM,2024-01-03 16:01:08,https://www.reddit.com/r/dataengineering/comments/18xmqus/what_are_some_good_udemy_courses_for_data/,0,False,False,False,False
18x0tav,"Distributed Training and Experiment Tracking with Ray Train, MLflow, and MinIO","Over the past few months, I have written about a number of different technologies ([Ray Data](https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/), [Ray Train](https://blog.min.io/distributed-training-with-ray-train-and-minio/), and [MLflow](https://blog.min.io/mlflow-tracking-and-minio/)). I thought it would make sense to pull them all together and deliver an easy-to-understand recipe for distributed data preprocessing and distributed training using a production-ready MLOPs tool for tracking and model serving. This post integrates the code I presented in my [Ray Train post](https://blog.min.io/distributed-training-with-ray-train-and-minio/) that distributes training across a cluster of workers with a deployment of MLFlow that uses MinIO under the hood for artifact storage and model checkpoints. While my code trains a model on the MNIST dataset, the code is mostly boilerplate - replace the MNIST model with your model and replace the MNIST data access and preprocessing with your data access and preprocessing, and you are ready to start training your model. A fully functioning sample containing all the code presented in this post can be found [here](https://github.com/minio/blog-assets/tree/main/ray_mlflow?ref=blog.min.io).

[https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm\_source=reddit&utm\_medium=organic-social+&utm\_campaign=distributed\_training\_experiment\_tracking\_ray\_train\_mlflow+](https://blog.min.io/distributed-training-and-experiment-tracking-with-ray-train-mlflow-and-minio/?utm_source=reddit&utm_medium=organic-social+&utm_campaign=distributed_training_experiment_tracking_ray_train_mlflow+)",2,0,swodtke,2024-01-02 21:25:04,https://www.reddit.com/r/dataengineering/comments/18x0tav/distributed_training_and_experiment_tracking_with/,1,False,False,False,False
18wjwgf,Testing in a migration project,"Hello fellow data people,
I am currently working on a migration project where we are moving from traditional RDBMS to on Prem pyspark cluster.
Just wanted to have some insight on how testing is managed in your projects, working on a similar usecase. Thanks",2,0,johndough990,2024-01-02 07:42:42,https://www.reddit.com/r/dataengineering/comments/18wjwgf/testing_in_a_migration_project/,1,False,False,False,False
18wi7fq,"Auto-generated visualization of JSON,Prisma, Graphql, Protobuf and XML files","We created a collaboration platform for data models and API schemas and we would love to hear your feedback!

Our goal is to give teams more focus time and create a more collaborative software development environment.

[JSON file ](https://preview.redd.it/80mh40c9vy9c1.png?width=2550&format=png&auto=webp&s=9d69312a71e8f685ae5c1f98bbcec1219ce6eaa9)

Our key features are:

* Auto-generated visualization of JSON,Prisma, Graphql, Protobuf and XML files
* Editting with two-way sync (code <> visualization)
* Synchronous and asynchronous collaboration
* Sharing schemas in secured hubs
* Creation and improving schemas with the power of AI
* Full Git workflow integration (GitOps)
* Using comments, notes, suggestions, labels, & highlighting

Could you imagine using hubql? What would be the usecase? We would love to hear your feeback!   
Go check our page on [https://www.hubql.com/](https://www.hubql.com/) 

Thank you very much for your support 🙏",2,0,furkanhere,2024-01-02 05:59:43,https://www.reddit.com/r/dataengineering/comments/18wi7fq/autogenerated_visualization_of_jsonprisma_graphql/,1,False,False,False,False
18wggcl,Best way to design a webscraping pipeline,"I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? 


My initial idea is to have a python script that scrape the website. It's scheduled by airflow. 
I was considering integrating the scraping functions in api calls, but I don't really know how that would look like. 

Can you please give me tips and hints on how to approach this problem with the best practices.

Do I need to use docker for example?

Thanks a lot",2,9,dimem16,2024-01-02 04:25:06,https://www.reddit.com/r/dataengineering/comments/18wggcl/best_way_to_design_a_webscraping_pipeline/,0,False,False,False,False
18vkg7o,Cleaning data with sed and tr,"Cleaning up data with sed

So at work. I have this giant pipe delimited file from a dos server and ofncourse I'm dealing with carriage returns line feed he'll. Without giving away too much info, a few fields have essay sized note columns and I need to basically erase those. But there were multiple carriage return line feeds that make it hard to parse. 

Enter Sed.

Sed combined with tr have been amazing but still having issues.ive used from Unix and unixtodos to get rid of these line breaks but then I'm stuck with the same issue, just parsing \n instead of \r\n.

My colleagues are lacking in advice bc they say shkt like ""oh just load it in pandas "" ☠️ like you can see my issue here. Also the spreadsheers a few gigabytes so vash scrupting is much more efficient for parsing than an interpreter language.

It's proprietary data so I can't share examples other than vague recreations but it looks like this rn:

Col1|col2|...|bad_col|bad_col2|...|end_col\n

Aaaa|bbb|...|\n

Bla bla blabla\n

\n

More bad data..\n

|Another essay data...\n

\n

more data...\n

|more bad data...\n

\n

More worthless text data\n

|good|from|...|here|row should end.\n

New|row|starts\n

And now it beings...\n

|all over again.\n

...\n

|and so on...



All pipe delimiters are at the front of each line that would be it's own field i git that far. I need.to find a way to erase all.the junk between a line that starts with a pipe until the next line that starts with a pipe
I've been hitting the sed faq and Messing with conditionals but in driving myself nuts. Any advice or wisdom on using sed or tr or any other packages to parse spreadsheets delimited ny pipes, with no quotations, where some columns have multiple paragraphs? 

I'll upload my sed script I'm working with but I don't wanna sign into my reddit on my company owned lapt9p lol",2,7,JucheCouture69420,2023-12-31 23:59:12,https://www.reddit.com/r/dataengineering/comments/18vkg7o/cleaning_data_with_sed_and_tr/,1,False,False,False,False
18v64o8,dagster project design help,"I have started trying to use dagster for a personal project and am trying to migrate some legacy windows scheduled jobs  (I'm just using subprocess with a meaningless return value to execute these).  It's generally working well, but I have one particular issue I am struggling with.  If any ther dagster users can give me any pointers I would be very grateful.

Upstream asset is a web scraper:

* The target website has one page per 'url-date', and the content of the page changes.
* Using a scheduled job, on each day, (t) the scraper should scrape data for pages dated t-1, t-2 and t-7.
* I want to partition the asset.
* 'url-date' is a natural partition dimension
* age *might* be a good partition dimension too (to allow simple recording of age)
* the scraper should only be run for one partition concurrently (due to throttling)

I have no experience on dagster (other than manual, videos and trial and error I have been doing) or any similar system so could really just do with **someone telling me if I am going about things the wrong way or on the right track, please**?

Here is what I have tried:

Setup the scraper as an asset with multi-partition of ('url-date', 'ageInDays-at-materialization'=\[1,2,7\])

* Attempt 1 - set up an scheduled op-job which accepts a run-date and calls materialize() 3 times on the asset for partitions:(run-date-1,2,7)\*(t-1,2,7).  The issue here is that in the GUI the asset-partition never shows as materialized (possibly due to the job being an 'ephemeral\_asset\_job' and running in a temp folder / with a different default io-manager).  I suspect his is incorrect/unintended usage of materialize()
* Attempt 2 - set up a scheduler which returns a 3 runRequests one for each (url-date=\[run-date-1,2,7\])\*(age=\[1,2,7\]).  The issue here is that the runRequests run concurrently. To solve this I will try using 'Limiting specific runs using tags' [https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines](https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines).  Perhaps this will work...  I did originally expect the whole scheduled task to run in one run, but that might be unrealistic.
* Attempt 3 - set up a scheduler to run a single runRequest for a range of partitions. As well as scraping the incorrect partitions this returns ''dagster.\_core.storage.fs\_io\_manager.PickledObjectFilesystemIOManager'> does not support persisting an output associated with multiple partitions. '

Other thoughts:

* Partitioning by url-date\*age means I will have many existing and empty partitions (which we are not able to materialize yet) and so will have to handle/ignore these in downstream dependencies.  Generally seems less than ideal.
* Partitioning by sample-date\*age seems like it would be so much more straightforward to implement, but provide a worse user experience (since sample-date is nowhere near as important to the user as url-date)",2,2,BeigePerson,2023-12-31 11:43:16,https://www.reddit.com/r/dataengineering/comments/18v64o8/dagster_project_design_help/,1,False,False,False,False
18usz11,In preparation for potential technical interviews,"Hello, all. 

I'm currently preparing for potential technical interviews I may encounter when I start applying for DE roles in early spring. The problem is, I don't really use SQL (anymore) in my day to day activities for work, so my skills have atrophied a bit, but I'm very familiar with the syntax and it's the coding language I'm most comfortable using. I'm trying not to burn myself out on strata scratch questions, but I'm attempting to complete 150-200 questions before late february. The problem is, some of the concepts I'm learning during this process aren't sticking, and that may be due to the sense of urgency I have. To be brief: should I just focus on mastering the 80-85 questions I've answered so far (and their concepts), and if I'm applying for D/E roles, what level should I aim it (entry or mid)  

Some background on me: 

* 2-3 years of experience 
* A masters degree in analytics 
* Don't want to reveal any more personal information, but I've essentially worked as a data analyst (with some D/E) work for the past two years

Any tips or advice would be greatly appreciated ",2,1,MiserableCharity7222,2023-12-30 23:24:56,https://www.reddit.com/r/dataengineering/comments/18usz11/in_preparation_for_potential_technical_interviews/,1,False,False,False,False
18ulpgs,DBT (SQL ) for querying Non relational data & use case in Data warehouse,"Sorry newbie here with modern data stack , Say I am doing ELT , now lets say data is there in the warehouse with relational data and also lot of non relational data eg. xml ,json , mainly json , logs   


Now I want to do transformation via DBT   
1) is it targetted towards BI use case . . ie DBT is for helping making the star schema from the raw data ?    


2) Can DBT (yes via sql ) be used to query and transform both relational and non relational data from the data ware house .  
If yes then . . can u pls give one e.g. how will sql alone do join or query with non relational data . .  


2a)

If no then   
Or we are saying that club PySpark +DBT to query relational +non relational together for transformation   
3)Can DBT (Batch )+ PySpark(streaming) -effectively replace any major traditional or upgraded ETL tool like informatica ?   


I googled and read through previous thread on same but i am not able to infer a confident answer ,Any senior can guide would be grateful .",2,2,Data5kull,2023-12-30 18:11:12,https://www.reddit.com/r/dataengineering/comments/18ulpgs/dbt_sql_for_querying_non_relational_data_use_case/,0,False,False,False,False
18ukb0f,"For those who have done a Snowflake certification (Snowpro), do you get results immediately?","Title is pretty self-explanatory. I'm curious as to how quickly you get results after doing a Snowpro certification. I'm planning to do the Snowpro Core cert soon, and might schedule it sooner so that I can slap it on my resume/apps.",0,3,jbnpoc,2023-12-30 17:09:35,https://www.reddit.com/r/dataengineering/comments/18ukb0f/for_those_who_have_done_a_snowflake_certification/,0,False,False,False,False
18ufcne,Are you looking to migrate data from Google Sheets to a PostgreSQL?,"

You might find this helpful.",2,2,logicdata,2023-12-30 13:11:36,https://medium.com/data-epic/data-migration-how-to-migrate-data-from-google-sheets-to-a-postgresql-using-python-561ada38efb4,1,False,False,False,False
18u5aix,No code transformation for customer data onboarding,"What tools have people encountered to enable non engineering and analytics folks to construct their own transformations to get data in the shape they want?
We onboard a bunch of customer data for each new customer, and data engineering needing to build bespoke transformation code is becoming a bottleneck for customer go live. I’ve looked at Flatfile and the like, and they would be way too expensive (like 500k a month too much).
We could build out a solution in house, but wanted to call upon the good people of Reddit before I go build something that’s already been built.",2,6,Psychling1,2023-12-30 03:05:36,https://www.reddit.com/r/dataengineering/comments/18u5aix/no_code_transformation_for_customer_data/,0,False,False,False,False
18tbxq3,How do I exclude files in ADF?,"Need to SFTP files generated every 15 minutes up to an external SAAS product.

Problem is the app generating these files has a hard lock on them.  It monitors realtime and keeps appending.  The target system doesn't actually need those files.  But it does need all the other files in the folder.

The files don't have an extension so I can't pattern match the ones to copy up.

Is there anyway to do this in ADF natively?

Or if it can't do that can I run a python script or powershell and pass the file list I want it to send back to ADF?

The files have RTA in the name so I'm sure I can regex that string in python.  Its the passing that list back to ADF for the pipeline job is the bit I have no clue.",2,6,ComfortAndSpeed,2023-12-29 02:34:07,https://www.reddit.com/r/dataengineering/comments/18tbxq3/how_do_i_exclude_files_in_adf/,0,False,False,False,False
18t3fpz,🚀 Introducing UCX v0.8.0: Powerful New Features to Streamline Your Unity Catalog Migration 💼,,2,0,serge_databricks,2023-12-28 20:19:14,https://medium.com/databricks-labs/introducing-ucx-v0-8-0-powerful-new-features-to-streamline-your-unity-catalog-migration-828d8c9b7aef,1,False,False,False,False
18t0d7o,Distributed Training with Ray Train and MinIO,,2,0,swodtke,2023-12-28 18:09:08,https://blog.min.io/distributed-training-with-ray-train-and-minio/?utm_source=reddit&utm_medium=organic-social+&utm_campaign=distributed_training_ray_train,1,False,False,False,False
18ssp71,Can u get databricks lakehouse fundamentals if you are not working for a company?,"New to this, I was checking the certification and it is asking for company email?",2,5,hayleybts,2023-12-28 12:22:38,https://www.reddit.com/r/dataengineering/comments/18ssp71/can_u_get_databricks_lakehouse_fundamentals_if/,0,False,False,False,False
1abu6ra,"Agile: With no planning poker, siloed non-cross-trained members, and unpredictable support work","We've been told our team is going agile. 

The main issues in my mind are:

  * We do unpredictable support work. Like > 1/2 of the week being as-it-came-in support isn't unheard of. 
  * We (3 folks) are 90% siloed, so effectively we're 3 teams of 1

Due to the 3 folks all being 'teams of 1', I'm told: 

  * No planning poker, we'll just each make our own estimates
 
For support work aspect...no-one seems quite sure yet. Either:

  * We have a user story that represents support
  * We just keep support out of Devops, and guesstimate our support time per week, and reduce planning points accordingly. 

Anyone tried agile in the above situation?  Cross training wasn't well received (Lack of time - and we're *very* siloed in all the different things we do: Not just data engineering - I'm just throwing this in dataengineering for wont of a better subreddit).",1,3,cdigioia,2024-01-26 21:59:21,https://www.reddit.com/r/dataengineering/comments/1abu6ra/agile_with_no_planning_poker_siloed/,1,False,False,False,False
1absst1,Optimizing memory-intensive pandas,"I'm a developer, not a DE. I have the most experience in Python, and some in Pandas (more each day). 

I inherited a large repo of pandas scripts that run ~9 hours each day, every day on a 750 GB EC2 instance. They take a handful of large partitioned parquet datasets, a few other daily CSV files, calculate that day's data, and rewrite those datasets as the output. Each day, each dataset will have 1 additional parquet file. The repo was written by an individual with very very bad programming experience and skills, but good salesmanship. The code sucks. It's very hard to follow, multiple thousand line functions, I could go on.

The large parquet datasets are partitioned by date going back to 2007, so there's ~6000 individual parquet files in each dataset called something like 'action_date=2022-01-05.parquet'. This data has to be Point-In-Time, so previous day's data are read but not modified. The outputs are the up-to-date versions of each dataset, with a new parquet file for the current date. 


For now this system can't change. Clients expect these parquet files structured as they are. 


The memory requirement is huge, it's currently running on a 750 GB instance, and uses most of it. 
A single dataset (there's 5-10) is ~30 GB as a pq dataset, but once loaded into Pandas, 170GB is used.


 I'm doing everything I can to minimize the memory consumption. I want to at least make it run in a 500 GB instance and save a lot of money. I'm using more optimal Pandas dtypes, I'm verifying that all columns are actually needed when reading in a parquet dataset, and I'm extracting a lot of intermediate steps to separate functions, so dataframes not needed anymore are garbage-collected, instead of lingering in memory until the end. I also updated Pandas from 1.2 to 2.0, which helped lower the runtime a bit but memory consumption remained constant.

I'd be happy to have a longer runtime for lower memory requirement. 

I can't change out of Pandas for the whole project in the short term. I'm trying to find a way to load these into Pandas more efficiently. Right now, data is loaded:

    dataset = pq.ParquetDataset(filepath, filters=filters)
    dataset = dataset.read()
    dataset = dataset.to_pandas()

I've been recommended other tools, like Dask. I tried it, but when you run .compute() on the lazy-loaded df, it uses the same amount of memory.

I'm open to any suggestions or advice here. I have to maintain the date-partitioned parquet dataset, but any way I can load these into dataframes more efficiently would be welcomed.",11,26,foyslakesheriff,2024-01-26 21:00:25,https://www.reddit.com/r/dataengineering/comments/1absst1/optimizing_memoryintensive_pandas/,0,False,False,False,False
1abrv0q,Looking for product feedback on newly developed tool for data teams,"Hello everyone, I am the Head of Growth at a Silicon Valley startup and we've pivoted to build a new product and I would love to demo it to as many data engineers or consultants as I can. The tool we are building is an AI chat interface powered by our customers event data. The goal is to goal is to reduce ad-hoc data requests by 80% while also efficiently managing our customers data. We are in the phases of product development so it is not live just yet.

Please let me know your thoughts and let me know if I can demo it for you.",0,7,Icy_Rooster_2217,2024-01-26 20:20:51,https://www.reddit.com/r/dataengineering/comments/1abrv0q/looking_for_product_feedback_on_newly_developed/,0,False,False,False,False
1abjdyg,The Difficulties of Senior Engineer …. are not Engineering,,0,0,dataengineeringdude,2024-01-26 14:18:59,https://www.confessionsofadataguy.com/the-difficulties-of-senior-engineer-are-not-engineering/,0,False,False,False,False
1abj66s,Creating new primary keys from multiple non-unique fields,"Hi everyone, supply chain data analyst here. I have a series of poorly ingested fields without primary keys. And for my own sanity I want to create them. And not just slap an auto incremented field on it with a unique constraint.

I want to find the fields that would make a primary key unique (and then slap an auto increment on those).  For example, maybe load_num, shipment_status, and arrival_time make a unique key.

But im finding my phrasing for this process to be incorrect, bc when i google it all I get are links to creating constraints.

Am I even thinking about this correctly?",1,5,tjfrawl,2024-01-26 14:08:30,https://www.reddit.com/r/dataengineering/comments/1abj66s/creating_new_primary_keys_from_multiple_nonunique/,1,False,False,False,False
1abit3d,Best Dashboard/Visualization options in 2024,"Hope this is the right place to ask. I'm a Director at a startup and we're in the process of building our first analytics MVP for a customer. We've got a basic pipeline built and some raw data sitting in Postgres (RDS), and now we'd like to visualize. One of our engineers is in the process of building queries for specific metrics.

There are a ton of dashboard options out there. My first thought was Grafana since there's an AWS service for it and security would be integrated with everything else we're doing.

What are some other good options? Prefer something I can at least evaluate fully for free, and doesn't cost a fortune after that. Prefer open source but we're flexible.

Requirements:

* Can produce dashboards from REST API and/or directly from Postgres
* Reasonably flexible to embed in web applications (React)
* Secure: Would like to be able to assign/manage users to specific dashboards or cards/metrics",1,4,zambizzi,2024-01-26 13:51:22,https://www.reddit.com/r/dataengineering/comments/1abit3d/best_dashboardvisualization_options_in_2024/,1,False,False,False,False
1abeeqn,Arch – the Multi-Tenant Data Platform as a Service for Consultants,,1,0,sbalnojan,2024-01-26 09:23:04,https://arch.dev/blog/arch-the-multi-tenant-data-platform-as-a-service-for-consultants/,1,False,False,False,False
1abbn48,Learning resources?,"I have 1 yoe as an data analyst with mostly working on SQL. I want to switch jobs but most of the companies also require data visualisation tools knowledge as well. I know SQL, PySpark and Python and little bit of ADF and Databricks but have only worked with SQL.
I observed companies want following tools knowledge- SQL, python, MS Excel, Tableau/Power BI mostly.
My question is from where can I learn these tools to be prepared for interview and where can I get sample projects regarding the same to showcase on my resume?",1,1,k_amit,2024-01-26 06:11:03,https://www.reddit.com/r/dataengineering/comments/1abbn48/learning_resources/,0,False,False,False,False
19fncyj,"ECS and Databricks to design, develop and maintain pipelines?","
Just got an interview invite to help out a team that uses Amazon ECS for container orchestration and Databricks. 

My guess is the ECS is used to help distinguish various dev environments but doesn’t Databricks do that already? 

Where does Amazon ECS come into play here? Anyone know?",1,1,Background_Debate_94,2024-01-25 23:41:51,https://i.redd.it/dk1wkrh8aoec1.jpeg,0,False,False,False,False
19fjowq,What makes a good ML & Data Engineering Manager?,"I just got a second interview for an ML & Data Engineering Manager at a huge pharma company. I’ve been a Product Manager, Analytics Engineer, data scientist, but not a data engineer. 

What makes a good data engineering and ML manager from your experience? 

I know it’s a broad question and can vary, but any general advice to know would be great. I am not sure if my skills in managing teams well would directly apply to this role.",1,5,imjusthereforPMstuff,2024-01-25 20:58:50,https://www.reddit.com/r/dataengineering/comments/19fjowq/what_makes_a_good_ml_data_engineering_manager/,0,False,False,False,False
19ffjqd,Need some advice.,"For those of you that are happy with your current job, please can you give me 5 of your main reasons why? 


I’m in my first DE job, and it has progressively worsened since I’ve joined. I just want to see what it’s like for others in their roles. 

Thanks in advance",1,5,silentwardrbe,2024-01-25 18:03:28,https://www.reddit.com/r/dataengineering/comments/19ffjqd/need_some_advice/,1,False,False,False,False
19fenqt,apiRAG: Retrieval Augmented Generation for LLMs from APIs with Function Calling,"I'd love your feedback on a different approach to Retrieval Augmented Generation (RAG) for LLMs that uses function calling to retrieve the most relevant data from APIs. It aims to solve the problem of connecting LLMs with your data so that the LLM can pull in the context it needs to provide high quality answers to user questions. 

The [Github repository](https://github.com/DataSQRL/apiRAG) contains the code and some examples. Here is a video that shows how it works:

[https://www.youtube.com/watch?v=Mx-slh6h42c](https://www.youtube.com/watch?v=Mx-slh6h42c)

We developed apiRAG because we found that existing RAG approaches (text search, vector based, FLARE, etc) don't work well with structured and semi-structured data.

apiRAG can efficiently augment from structured and semi-structured data by translating user questions to relevant API requests and then presenting the result data to the user. It supports textual presentation (with Markdown for tables and such) and visual presentation by charting data when appropriate. Check out the IoT chatbot that can answer questions about collected sensor data or the credit card example that gives users a customized spending analysis (shown in the video).

Unlike RAG approaches that use text or vector search, apiRAG uses the LLM to determine what information should be retrieved and augmented into the context via function calling. 

That means, it often does a better job at identifying relevant information by pushing down filters, like when you ask ""Who appears in the third episode of season 2"" in our ""Rick and Morty"" example.",1,1,matthiasBcom,2024-01-25 17:27:33,https://www.reddit.com/r/dataengineering/comments/19fenqt/apirag_retrieval_augmented_generation_for_llms/,0,False,False,False,False
19fdkvi,DBT final table aliases,"I've a question for those who use DBT on a daily basis. So i was working on defining some models in DBT and we ran into this problem. The naming convention that we are currently using to name our table is useful and intuitive for us (ML engineering team) but not for the DS teams that use final tables (e.g. table name **sales\_customer\_info\_customer\_country\_daily** should be something like **customer\_distribution**).  
Therefore, my question is, how can i set up the YAML file for that specific table so that it uses an alias when building the final table keeping the current name unchanged for our dbt project?   
I already tried browsing the DBT documentation but haven't found anything useful ",1,2,stefano250396,2024-01-25 16:42:41,https://www.reddit.com/r/dataengineering/comments/19fdkvi/dbt_final_table_aliases/,1,False,False,False,False
19fbtgh,Proposed short courses for starting on the data engineering for product people,"Hi All, I lead a product management team. One of our products is around specific features oriented at data engineers.  


I'd like to get up to speed hands-on with the core workflows and problems that data engineers face. Particularly around data transformations.  


Do you have any good hands-on training suggestions that could give a good flavor - working with Databricks, Snowflake, Azure Synapse / Fabric, dbt to provide an immersive experience for a determined PM with some coding and SQL skills? perhaps some course that has an overview of multiple platforms?",1,1,ivenger,2024-01-25 15:24:43,https://www.reddit.com/r/dataengineering/comments/19fbtgh/proposed_short_courses_for_starting_on_the_data/,1,False,False,False,False
19fauzo,delta table querying question,"We have some delta tables set up in Databricks and the question came up about whether we could do some querying against them without spinning up a Databricks compute cluster.

I was trying to use some python libraries to get this done, namely:  
\- delta-rs: used this to open the delta table and convert to a pyarrow table  
\- duckdb: used this to perform queries against this

The problem I ran into is that I couldn't get it into a pyarrow table without loading the entire thing into memory, which is a problem for very large tables.  Is there a way around this?  My attempts have been unsuccessful to this point.

Note:  I'm not a data engineer, so I'm kind of winging it here.  We're also trying to stick to Python for this, as it aligns with the technology stack we are using.",1,3,jaltiere,2024-01-25 14:41:19,https://www.reddit.com/r/dataengineering/comments/19fauzo/delta_table_querying_question/,1,False,False,False,False
19fad2j,Preferred Approach to Warehouse Modeling?,What is your preferred way in modeling your warehouse and why? Any tips to better execute common modeling situations?,1,1,AMDataLake,2024-01-25 14:18:27,https://www.reddit.com/r/dataengineering/comments/19fad2j/preferred_approach_to_warehouse_modeling/,0,False,False,False,False
19f3vul,Need Help Understanding Cloud Dataflow Retry Mechanism and Sequential Execution,"I'm working on a project where i'm using apache beam's  `ReadFromJdbc` for reading a set of data in a certain time range from a database in one sub-network and then processing the data and finally writing the processed dataset to a table in another database in a separate sub-network using `WriteToJdbc`.  I'm using Apache Beam's Python SDK to develop the scripts. These pipelines i build will be hosted in Google's cloud data flow. 

&#x200B;

I need some help from you guys/gals clarifying few questions i have regarding this project which i wasn't able to find answers to by my own. My questions are,

1. If the cloud data-flow  pipeline fails during a writing phase is there a method to implement a retry mechanism? I ask this cause if the data is being written parallel to the target tables i can't use the  maximum id written to the database to start from there. 
2. When reading from the database how do i make sure the reading process doesn't cause any issues to the other operations that takes place in the database. (I'm reading data from a database that is being used by another application, how to make sure read, write operations to this database isn't effected by my pipeline.)
3. I need to read data from several tables and then write them to several tables in the target database, can i do this in the same pipeline like the below code snippets given at the end of the post? If not and i have to build separate pipelines for each read and write operations is there a way i can ensure a cloud data flow pipeline runs its job right after the completion of the first  pipeline's job is completed. (The reason i want the pipeline to run one after the another is to avoid the high resource burden it might cause on the source database when the data is being read at the same time).

&#x200B;

My code snippets for possible solutions to read from  and write to multiple tables are given below. Let me know if this is possible. 

    def run(source_table, source_connection_url, source_username, source_password, source_query, target_table, target_connection_url, target_username, target_password):
        options = PipelineOptions(
            runner='DataflowRunner',
            project='',
            staging_location='',
            temp_location='',
            region='',
            template_location='',
        )
        with beam.Pipeline(options=options) as pipeline:
            pcollection_tablex = (
                pipeline
                | f'Read from source database tablex' >> ReadFromJdbc(
                    table_name=source_table,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=source_connection_url,
                    username=source_username,
                    password=source_password,
                    query=source_query,
                    fetch_size=5000
                )
            )
            (
                pcollection
                | 'Write to target database tablex' >> WriteToJdbc(
                    table_name=target_table,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=target_connection_url,
                    username=target_username,
                    password=target_password
                )
            )
            pcollection_tabley = (
                pipeline
                | f'Read from source database tabley' >> ReadFromJdbc(
                    table_name=source_table_y,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=source_connection_url_y,
                    username=source_username_y,
                    password=source_password_y,
                    query=source_query_y,
                    fetch_size=5000
                )
            )
            (
                pcollection_tabley
                | 'Write to target database tabley' >> WriteToJdbc(
                    table_name=target_table_y,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=target_connection_url_y,
                    username=target_username_y,
                    password=target_password_y
                )
            )
    
    
    
    
    
    if __name__ == '__main__':
        run()

&#x200B;

    def run(source_table_x, source_connection_url_x, source_username_x, source_password_x, source_query_x, target_table_x, target_connection_url_x, target_username_x, target_password_x,
            source_table_y, source_connection_url_y, source_username_y, source_password_y, source_query_y, target_table_y, target_connection_url_y, target_username_y, target_password_y):
        options = PipelineOptions(
            runner='DataflowRunner',
            project='',
            staging_location='',
            temp_location='',
            region='',
            template_location='',
        )
    
        with beam.Pipeline(options=options) as pipeline:
            pcollection_tablex = (
                pipeline
                | f'Read from source database tablex' >> ReadFromJdbc(
                    table_name=source_table_x,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=source_connection_url_x,
                    username=source_username_x,
                    password=source_password_x,
                    query=source_query_x,
                    fetch_size=5000
                )
            )
            (
                pcollection_tablex
                | 'Write to target database tablex' >> WriteToJdbc(
                    table_name=target_table_x,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=target_connection_url_x,
                    username=target_username_x,
                    password=target_password_x
                )
            )
    
        with beam.Pipeline(options=options) as pipeline:
            pcollection_tabley = (
                pipeline
                | f'Read from source database tabley' >> ReadFromJdbc(
                    table_name=source_table_y,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=source_connection_url_y,
                    username=source_username_y,
                    password=source_password_y,
                    query=source_query_y,
                    fetch_size=5000
                )
            )
            (
                pcollection_tabley
                | 'Write to target database tabley' >> WriteToJdbc(
                    table_name=target_table_y,
                    driver_class_name='com.mysql.cj.jdbc.Driver',
                    jdbc_url=target_connection_url_y,
                    username=target_username_y,
                    password=target_password_y
                )
            )

&#x200B;",1,1,Plus_Flight8909,2024-01-25 07:29:07,https://www.reddit.com/r/dataengineering/comments/19f3vul/need_help_understanding_cloud_dataflow_retry/,1,False,False,False,False
19ex79w,Free Postgres hosting for personal project,"Hey everyone, I have been using postgres for one of my personal projects, and I have been using Neon as the host for my postgres server, I have about 1.5 GB of data on it my Neon server. Neon has updated its free tier plan and I can not use them anymore as it is not going to be free from February onwards, so i was wondering do you have any suggestion for a free postgres server for about 2 GB of data?",1,0,AH1376,2024-01-25 01:20:22,https://www.reddit.com/r/dataengineering/comments/19ex79w/free_postgres_hosting_for_personal_project/,0,False,False,False,False
19ewexu,How to implement kappa architecture for marketing data lakehouse,"Hello, I work for a small analytics agency and I’m learning to build a data lake house specifically for marketing data (Google analytics, social media, and customer data). My coworkers tell me I need to build it in Kappa architecture to meet the clients requirements. I don’t know what that is, and I only have sql skills so not sure where to start. 

Can anyone point me to some resources and tutorials on how to do this from scratch?",1,1,BlueskyPrime,2024-01-25 00:43:39,https://www.reddit.com/r/dataengineering/comments/19ewexu/how_to_implement_kappa_architecture_for_marketing/,1,False,False,False,False
19evpb6,data extraction?,"Hello everyone, I've a project that i work on it's a data extraction of different financial statements i want to extract the data that is an image form (paper scanned) i want to use generative AI or LLM or any tool that gives me good result any advice ?",1,1,BilelKort,2024-01-25 00:10:38,https://www.reddit.com/r/dataengineering/comments/19evpb6/data_extraction/,0,False,False,False,False
19eqx1m,Serverless Storage with Knative and Ceph,,1,0,Dave-at-Koor,2024-01-24 20:50:51,https://koor.tech/blog/2024/serverless-storage-knative-ceph/,1,False,False,False,False
19el0fd,Data Governance Analyst Vs. Data Engineer,"I am currently in a Data governance role in a bank which involves a lot of root causes analysis, gap analysis use cases and reports development as well (All using SQL and Excel). 
Meanwhile I got a job offer as a Data engineer and I can’t make my mind on which career to pursue. 
Which career do you think has a better future, and will be most beneficial for my career development ?",1,1,reddituserml1,2024-01-24 16:25:55,https://www.reddit.com/r/dataengineering/comments/19el0fd/data_governance_analyst_vs_data_engineer/,1,False,False,False,False
19ek46z,Scheduled Python Jobs with Prefect and Coiled,"I was playing around with running Prefect workflows on cloud-hosted data with a new integration from some colleagues at Coiled. Turns out it was pretty straightforward to deploy a daily data processing job on a NASA dataset I’ve been working with lately. Thought I’d share a write up of what this looks like. 

    # python workflow.py              # Runs locally
    coiled prefect serve workflow.py  # Runs on the cloud

blog post: [https://medium.com/coiled-hq/schedule-python-jobs-with-prefect-and-coiled-b22180a25f1f](https://medium.com/coiled-hq/schedule-python-jobs-with-prefect-and-coiled-b22180a25f1f)",1,0,dask-jeeves,2024-01-24 15:47:35,https://www.reddit.com/r/dataengineering/comments/19ek46z/scheduled_python_jobs_with_prefect_and_coiled/,1,False,False,False,False
19ejzm0,How does your organisation/client utilise Data Engineering concepts/tools ?,"Hello, I am thinking to switch into data engineering role from system engineer role, I have learned some basic concepts and tools related to DE. 

I want to know how Data Engineering is being used by your organisation in real life scenario. This might help me to understand real use cases and work on similar projects.",1,0,Accomplished_Rip3587,2024-01-24 15:41:58,https://www.reddit.com/r/dataengineering/comments/19ejzm0/how_does_your_organisationclient_utilise_data/,1,False,False,False,False
19edlow,Future scope for GCP data engineer,Need help from senior folks in data. Also how difficult it will be for me to switch into other cloud service,1,0,dummymum,2024-01-24 09:55:37,https://www.reddit.com/r/dataengineering/comments/19edlow/future_scope_for_gcp_data_engineer/,0,False,False,False,False
19e6ofb,SW engineer learning about ETL pipelines,"Software engineer here just recently joined a team heavily focused on data science and data engineering. The expectation is that i take their scripts and refine and improve their automation. While still ramping up on the data science and data engineering problem space, coming from a software engineer perspective would it make sense to use a tool that offer ci/cd pipeline capabilities to create an ETL pipeline? Ive just normally been exposed to ci/cd pipelines with tests, static code analysis tools, etc that promote/deploy an application to an environment. Whereas, from my understanding, the new team im on wants to gather data, normalize said data, and upload it to a data warehouse. So from a tech stack perspective would it make sense to also use tools that offer ci/cd pipelines to achieve that? Like basically break up their script into stages so i can also create stages to test/ensure quality of the data prior to moving on to the next step, as opposed to traditionally having devop-specific stages .Or are there some obvious reasons that im just not aware of as to why people use tools like domino, aws glue, snowflake or airflow instead? Like maybe its an issue of scalability when dealing with large volumes of data? Any feedback would be appreciated.",1,7,monkeypaw64,2024-01-24 02:49:39,https://www.reddit.com/r/dataengineering/comments/19e6ofb/sw_engineer_learning_about_etl_pipelines/,0,False,False,False,False
19e6bpi,ADF Linked Service to REST API with only token - how?,"I'm attempting to connect to a vendor API (Invoca). They provide only an OAuth token, which I'm supposed to pass as a header/parameter. My options for a REST linked service in ADF either require a username and password (when using Basic method) or need a token generating endpoint, client ID, and secret (when using OAuth2). Any idea how I can make this connection?",1,1,Sub1ime14,2024-01-24 02:32:36,https://www.reddit.com/r/dataengineering/comments/19e6bpi/adf_linked_service_to_rest_api_with_only_token_how/,1,False,False,False,False
19e1pjk,I built Panora: an Open source alternative to Merge.dev,"Hey, I felt like paying a shit ton of money to unified APIs was unfair, so I decided to build an open-source alternative to Merge.dev.  
It makes connecting your product to third-party SaaS such as CRMs, ticketing sofware easy. Using one API only, you get access to multiple SaaS.   
This way you no longer need to dive into API documentation and data transformations.  


Here's the Github : [https://github.com/panoratech/Panora](https://github.com/panoratech/Panora)   


Have you ever used unified APIs? Any thoughts on them? ",1,0,Rich-Dentist-4255,2024-01-23 22:58:23,https://www.reddit.com/r/dataengineering/comments/19e1pjk/i_built_panora_an_open_source_alternative_to/,1,False,False,False,False
19e0duk,Delta Live Tables - Handling Duplicates in Source Data,"Hey all -- 

Terribly sorry if this is a stupid question, but having trouble figuring out the best way to handle duplicates in source data. Source data is coming into my bronze layer from parquet files in google cloud storage using autoloader. some records, but not all are duplicates.

I'm very new to DLT (thus learning all aspects of it right now, contributing to my analysis paralysis) really having trouble finding some documentation on some best practices/patterns on how to handle this...

what's especially difficult to wrap my mind around is that the duplicates will continue to live in the google cloud bucket... so if I use dropDuplicates() on an interim dataframe, I essentially have to perform the operation for the whole dataset each time?

Links to any resources on where to start with this would be great.

Thanks in advance.",1,3,osmosisjonesin,2024-01-23 22:03:03,https://www.reddit.com/r/dataengineering/comments/19e0duk/delta_live_tables_handling_duplicates_in_source/,1,False,False,False,False
19dz9k8,Storage Location being referenced to ADLS Container which is not mounted to Databricks cluster,"Hi Everyone,

We are facing issue while reading file to process it.

All the storage locations and parameters were set correctly. Still while file is being read into a data frame the storage is referring to ADLS Container which we didn't mention anywhere in the notebook nor it is mounted to Databricks Cluster. Any help is appreciated in resolving the issue.

This is first time posting here. Kindly bear my mistakes if any.",1,9,Its__Ram,2024-01-23 21:17:08,https://www.reddit.com/r/dataengineering/comments/19dz9k8/storage_location_being_referenced_to_adls/,1,False,False,False,False
19dz9i8,First Pipeline Help!,"I'm building a pipeline for a client. I usually focus on the modeling portion and recommend a tool like Fivetran or Stitch, but that wasn't an option. 

So I've built a pipeline in Python that is moving data from their on-prem sources (oracle and SQL server) and loading it into Snowflake.

What is the best/proven order of operations for loading data? My current process: 1) Pull data from the source system to a flat file, 2) load flat file to a stage on snowflake, 3) query meta data from source table to generate logic and create snowflake table, 4) load the stage to a table, and then 5) clean out the stage and the flat file.

What am I missing? What are som ""best practices"" that I could apply to this process?",1,1,tpedar50,2024-01-23 21:17:05,https://www.reddit.com/r/dataengineering/comments/19dz9i8/first_pipeline_help/,1,False,False,False,False
19dsj3n,Pytest on Databricks,"I'm pulling my hear out with this one: 

We've upgraded our cluster from 10.3 LTS, to 13.3 LTS and all the unit tests are not running . For 11.3 and everything in between it's the same.

We even have a simple one that just checks if a given table has data. 

As in: 
assert df.count() > 0 

And for each and every one of them we get: 

OSError: [Errno 95] Operation not supported: 'Workspace/Repos/..../tests/__pycache__'

Test called using
If __name__ == '__main__':
    pytest.main([test_file.py])

Made sure these files are 'files' not 'notebooks', tried to delete them, put them back. Nothing seem to work. 

Any advice?",1,2,albowiem,2024-01-23 16:36:37,https://www.reddit.com/r/dataengineering/comments/19dsj3n/pytest_on_databricks/,0,False,False,False,False
19dsg0t,Navigating the Path to Data Engineering,"Hey guys I've recently graduated from college  and have completed chemical engineering and I want to transition to  data domain . I  need your help guys to get the answer to few of my questions 

1) What are some specialized areas within data engineering, and how can one decide on a niche to focus on.

2)Are there specific skills or certifications that stand out in the job market?

3)What key elements should be highlighted on a resume to catch the attention of recruiters?",1,3,No-Salt5525,2024-01-23 16:33:05,https://www.reddit.com/r/dataengineering/comments/19dsg0t/navigating_the_path_to_data_engineering/,1,False,False,False,False
19dq2rf,Cronitor & Airflow,Is anyone using Cronitor to monitor their Airflow instance? Would be curious to hear what the value add is to have another tool in the loop.,1,0,Express-Comb8675,2024-01-23 14:49:15,https://www.reddit.com/r/dataengineering/comments/19dq2rf/cronitor_airflow/,1,False,False,False,False
19dojoh,How to manage drift between dbt and Redshift,"Hello !

I am managing a team of 5 data engineers for the analytics of a retail compagnie and we use dbt and Redshift. The team is 5 years old and we have an accumulation of old databases that were not removed + manual changed that were made through the console and are not under control.  
I have been searching but didn't find good tools to monitor it (would love to get something that points the differences between dbt and the real state of snowflake).  
Any recommendations?",1,1,New_Detective_1363,2024-01-23 13:35:08,https://www.reddit.com/r/dataengineering/comments/19dojoh/how_to_manage_drift_between_dbt_and_redshift/,0,False,False,False,False
19dniju,The Approach vs Technology Confusion: Where do Data Products Fit In?,,1,0,growth_man,2024-01-23 12:39:40,https://moderndata101.substack.com/p/the-approach-vs-technology-confusion,0,False,False,False,False
19dega8,Using Airflow to execute preprocessing data for machine learning,"Hi guys, I began using Airflow recently. I am using it for my process server which user can import their file to data lake. Then I will pass these file location, ways to process and processed file destination to **Airflow** webserver throught API. **CeleryExecutor** then will add job to **rabbitMQ** and **workers** will execute that **DAG**. **DAG** contain 3 task: **Extract** (download data from data lake and read it as dataframe), **Transform** (perform some transformation to that df) and **Load** (load processed df back to data lake). My question is Is it common to reload data from data lake every time a dag is executed. I mean if user want to perform many process separately, then workers will have to reload data to RAM over and over again. Is there any way to optimize this or is there any better way to do this? Thanks for your reading ",1,7,resrrdttrt,2024-01-23 03:06:29,https://www.reddit.com/r/dataengineering/comments/19dega8/using_airflow_to_execute_preprocessing_data_for/,1,False,False,False,False
19daecg,Need career advice,"Im currently doing my final year in btech Artificial Intelligence and Data Science and I got an internship through placements at a startup . The thing is , this company made me to handle a client with no prior experience (literally 0) and makes me do everything under a module . As an intern I want to learn more and help but Im stressing  myself working on pipelines, modifying transformation logic , fixing pipelines, documentation, cloud , writing apis and more . Im extremely exhausted and feel Ive wasted my time here .And at time I feel if i stayed back and just learnt stuff I would have got a better job.  Is this a common thing out there ?? (Ps : my stipend is only 155 dollars / month and i still handle the client )",1,1,pradishhh,2024-01-22 23:53:09,https://www.reddit.com/r/dataengineering/comments/19daecg/need_career_advice/,0,False,False,False,False
19d54ho,Switching to data analyst position from engineering?,"Hi all, I’ve been a junior DE for around 8 months now. I lack a CS degree and don’t currently plan to get a masters degree, though I’m pretty committed to self learning. At the moment however, I’m not 100% set on being a DE. I mostly ended up here due to placements after a rotational software engineering program. 

If I get a different position within my org as an analyst, would it be bad for my career? I feel that I may be a good fit for analytics as I used to work in an R&D science & engineering department for a manufacturer and did a ton of analytics on lab data. I think I’d enjoy the work. 

Would this be a step backwards, as analysts are typically considered to be less technical than engineers? 
I see a lot of people trying to switch from analytics into data engineering and am wondering if I’m crazy for trying the reverse haha.",1,3,aaloo_chaat,2024-01-22 20:15:52,https://www.reddit.com/r/dataengineering/comments/19d54ho/switching_to_data_analyst_position_from/,1,False,False,False,False
19d2pj8,How to become a committer,"Background: Work in strategy and ops | Non-technical

Are there open source projects that accept volunteers from non-technical backgrounds?

How do I get started?

What would be good open source projects to target?",1,0,ProfessorFinanceBro,2024-01-22 18:35:31,https://www.reddit.com/r/dataengineering/comments/19d2pj8/how_to_become_a_committer/,1,False,False,False,False
19d2m9p,Need Help with Interview Practice,"I took a job as a data and analytics engineer two years ago. The job is very limited in its growth and skill ability, and the majority of the harder data engineering work is done through an out-of-the-country contracting firm. My position is mainly translating requirements for them to be able to build and maintain. I am looking to leave this firm to continue growing my skill set, but I am out of practice interviewing, especially in the current market. I am specifically targeting Sr. Data Engineer positions with growth potential as either a Staff Engineer or a Data Architect. Does anyone have any groups for mock interviews and/or study curriculum in order to review for interviews? I specifically need assistance in Python algorithms and system design.",1,1,Jonesy-2010,2024-01-22 18:31:56,https://www.reddit.com/r/dataengineering/comments/19d2m9p/need_help_with_interview_practice/,1,False,False,False,False
19d1i7l,How would you get targets/budgets from Gsheets into BigQuery in my situation?,"First off, I rolled into some DE work from data analysis, which means I've not that much technical knowledge. I got all the core data sorted out, but this little silly thing is costing me too much time.

I'm trying to figure out a good way to get targets/budgets that are entered in a Google Sheet into BigQuery. All transformations in BQ managed through Dataform. The input would be a wide table from Google Sheets with a column for each week (or month), while the rows have the target/budget type and who's target/budget (usually a column or 2 or 3 with identifying data) it is. I want to use a wide table because that makes it easier for management to adjust the numbers if need it. After that I of course need to unpivot it in a long table.

My initial setup was the following:

1. Wide table
2. Use a script inside the sheet to unpivot it in long form
3. Create Gsheets based table in BigQuery
4. Go from there in Dataform

I ran into a problem that sometimes I don't get any data. I believe this is because the script kicks off but BigQuery doesn't wait for it to finish (its only like 1 second, not a heavy script). The second time I run the query it will get data. However, Dataform naturally only runs the query once in a workflow. Very frustrating, because I used the same scripts a couple of years back when I had a sheet as a data source in Tableau (that I joined to a BQ table inside Tableau) and there it would just work.

Getting the wide table in BQ (to unpivot there) is also a pain, because all the column names would be dates. But the worst part is that I would like to keep adding columns in the source sheet and that breaks everything.

How would you guys approach this?  Do I do something with Python in Cloud Composer (haven't had the need to use that yet)? Is there something else I should try? I prefer to keep it as simple as possible, I have a little experience with Python but wouldn't call myself a great programmer. My last resort would be to just manually update the table in BQ when someone from the business changes something (and lets me know). ",1,15,KoeitjeNL,2024-01-22 17:47:14,https://www.reddit.com/r/dataengineering/comments/19d1i7l/how_would_you_get_targetsbudgets_from_gsheets/,1,False,False,False,False
19d0339,Apache Beam source code certainly feels like walking into a Spaghetti factory,"I have been using Beam for like 2 years at work (Java SDK, but developing in Scala) and every now and then I have some issue that requires me looking deeper into the definitions of the various Beam methods.

Man, is that something hellish. I understand the SDK has to be super generic because it's meant to run on a plethora of platforms, but still...something just feels very off about it.

For example: Recently had an issue where a pipeline using a PeriodicImpulse did not drain, so I go ahead and look...turns out it was a bug with an underlying transform in PI.

So I go and look into PeriodicImpulse, and see that it's actually just a wrapper around PeriodicSequence, which in turn is just a wrapper around a DoFn, and so on for like 3 steps, and I find the piece of code that MIGHT solve my issue, so I try to make my own PeriodicImpulse, but I can't because it has no public constructors, so I try with the PeriodicSequence, and it's the same issue, so I try with DoFn because that's all PS is, right? 

Well apparently not, once again some access issue lol, and then you try to read into the intricacies of each class, and each one is a huge rabbit hole that just goes around and around.

So just a bit of a small rant, but yeah, I feel their code is very difficult to read.",1,1,yourAvgSE,2024-01-22 16:50:07,https://www.reddit.com/r/dataengineering/comments/19d0339/apache_beam_source_code_certainly_feels_like/,0,False,False,False,False
19cwnqx,Passing from Junior analyst in Big 4 toward an internship in a bank,"I don't know if it is worth the sho. osition in a major investment bank but it's a 9-month internship where I don't know if there could be a possibility to have a full-time position. Some pros is that is abroad, far from my country and this is something I like to do while cons, the major one is to leave a position where I'll become senior next year.  


also, the position will be slightly oriented on risk with quantitative applications while atm I'm a data engineer. The good news is the possibility to move to other teams once inside the company. 

I don't know if it is worth the shot. ",1,3,Dry_Frame_7025,2024-01-22 14:19:04,https://www.reddit.com/r/dataengineering/comments/19cwnqx/passing_from_junior_analyst_in_big_4_toward_an/,1,False,False,False,False
19cpbvj,Fork in the road - masters degree,"I searched, didn't see anything relevant.

I have an industry background (10 years actuarial) and have done ""data work"" for 5 years.  I won't quite call it data science (my title) or engineering.  SQL, Pandas, Dplyr, Teradata.  Most of the modeling is ""gather all the things and perform feature selection""

The biggest gap in my education coming from finance is computer science fundamentals.  When I lurk on this sub, I realize that I don't understand most of the jargon being used. 

What masters degree would give me the best education?  I am 36.",1,7,Kegheimer,2024-01-22 06:30:36,https://www.reddit.com/r/dataengineering/comments/19cpbvj/fork_in_the_road_masters_degree/,1,False,False,False,False
19ciwki,DE with basic programming?,"
While I am working towards improving my python and SQL.  Is it still possible to get into DE with minimal programming skills. 

Have you seen any examples or is it you? Or should I not even try without improving my skills first?

Are there roles in DE that don't require as much programming but can help me get into the field? ",1,9,Wise_Shop6419,2024-01-22 00:49:35,https://www.reddit.com/r/dataengineering/comments/19ciwki/de_with_basic_programming/,0,False,False,False,False
19cimgf,Schema structure in data warehouse,"How do you design your database schemas in your data warehouse to accommodate the varied applications of your data? I know this is nuanced and can widely vary depending on the business you're in. 

I recently moved to a 1)RAW (lake layer) 2) INTEGRATION (semantic layer) 3) ANALYTICS (BI/ reporting layer) structure, but buy-in with other DE's has been challenging and I don't find it a one size fits all structure accross all use cases. We throw in the odd Data Mart when required.

Thanks!",1,1,warrenbuddgett,2024-01-22 00:35:48,https://www.reddit.com/r/dataengineering/comments/19cimgf/schema_structure_in_data_warehouse/,1,False,False,False,False
19cdg40,"Is anybody using Azure Synapse, and if so, are you having any issues with private endpoints?","I've been using Synapse for a couple of years, and as of 1/13, our private endpoints in Azure stopped working. I'm working on setting it up with a new client, and we can't get private endpoints deployed there, either.  I've heard from some Microsoft engineers that Synapse can no longer run on a private endpoint.  This is against the security policy where I am.  Has anyone else experienced this? ",1,0,Swimming_Cry_6841,2024-01-21 20:52:18,https://www.reddit.com/r/dataengineering/comments/19cdg40/is_anybody_using_azure_synapse_and_if_so_are_you/,1,False,False,False,False
19cda7q,How to optimise my learning journey,"Hello everyone! I’m currently enrolled in an intensive 12-month data engineering program in South Africa, and I’m seeking advice on how to strategically position myself for success. The program includes eight months of learning followed by a four-month internship with sponsoring companies. My goal is not only to secure a job locally but also to be attractive to international employers for better earning potential, considering the current exchange rates. What skills and strategies do you recommend to enhance my marketability? Your insights will be invaluable as I navigate this exciting journey. Thanks in advance!",1,1,Fearless_Jicama2909,2024-01-21 20:45:22,https://www.reddit.com/r/dataengineering/comments/19cda7q/how_to_optimise_my_learning_journey/,1,False,False,False,False
19c99y5,"Created a pipeline ingesting data via kafka, processing via akka streams in Scala and moving it to Snowflake","This is one of the projects I have created to learn how to work with real time data and understand how to connect to cloud storage and use snowflake features.

**About the project**:

1. Yelp dataset containing business data across is produced to kafka.
2. Real time data then is consumed from kafka via alpakka connector and transformed using akka streams with Scala.
3. Data is moved to mongo DB and also to azure data lake storage gen2 gin multiple files.
4. Once the data is there in ADLS, snowpipe is configured to moved that data to Snowflake.
5. Snowflake script is present in the /conf folder of the repo.

Github URL :  [https://github.com/sarthak2897/business-insights](https://github.com/sarthak2897/business-insights)

Technologies used : Kafka,Scala, Akka streams, Mongo DB,Azure Data Lake Storage Gen2, Snowflake

Please provide feedback on how I can improve and modify the pipeline. Thanks!

&#x200B;",1,1,sarthak2897,2024-01-21 17:58:46,https://www.reddit.com/r/dataengineering/comments/19c99y5/created_a_pipeline_ingesting_data_via_kafka/,0,False,False,False,False
19c5i59,Best method to run a dbt project in an Airflow DAG (Cloud Composer) with GCP/BigQuery?,"Hey! I'm currently working on integrating my dbt (data build tool) project, which is stored locally on my PC, with Airflow in a Cloud Composer environment on GCP using BigQuery. I'm a bit torn between using BashOperator, DockerOperator, or GKEPodOperator to execute the dbt project in my Airflow DAG.

Any advice or insights on which operator is the best fit for running dbt in this setup? And if you've got any tips to share, that would be a huge help!

1. BashOperator
```
run_dbt_task = BashOperator(
    task_id='run_dbt',
    bash_command='dbt run',
    dag=dag,
)
```
2. DockerOperator:
```
run_dbt_task = DockerOperator(
    task_id='run_dbt',
    image='your_dbt_docker_image',
    api_version='auto',
    command='dbt run',
    dag=dag,
)
```
3. GKEPodOperator:
```
run_dbt_task = GKEPodOperator(
    task_id='run_dbt',
    project_id='your_project_id',
    location='your_gke_cluster_location',
    cluster_name='your_gke_cluster_name',
    pod_name='run-dbt-pod',
    image='gcr.io/your_project/your_dbt_image',
    cmds=['dbt', 'run'],
    dag=dag,
)
```
Out of these three operators - which one would be the best one to use to run my dbt project in my Airflow DAG?

Any help/advice would be good, thanks!",1,1,saucyhambon,2024-01-21 15:15:05,https://www.reddit.com/r/dataengineering/comments/19c5i59/best_method_to_run_a_dbt_project_in_an_airflow/,1,False,False,False,False
19bvcjk,Need help with companies shortlisting,"Hi All, 
Long story short-  I am trying to switch companies as i think i am going to be put into PIP at a FAANG. I have 2 years of experience as a data engineer. What other good companies that i can apply to? I know the market is pretty bad now a days but i wanna target and try applying to some good companies as well. I work in India. 
1. I always wanted to work in European companies. Is there a chance now given current market? If yes how do i do it. Just go to company’s career site and apply or is there someother way?
2. I also know that trying to work elsewhere is not very optimal rn . Even in India i would like to get some good data engineer companies.
Please let me know if anyone has any ideas or suggestions.",1,1,mad_peace,2024-01-21 04:48:52,https://www.reddit.com/r/dataengineering/comments/19bvcjk/need_help_with_companies_shortlisting/,1,False,False,False,False
19bghjl,Suggestions for a Scalable Search Engine for Aggregating Results from Multiple Queries,"Hello everyone,

I'm currently working on a project that involves creating a searchable database. As the database is expected to grow into millions of records with titles, description and others, I'm looking for an efficient and scalable search engine solution.

Here's a brief overview of my requirements:

* **Data Size**: The database will contain millions of records.
* **Functionality**: I need to aggregate and deduplicate search results from multiple queries. Each user can have multiple favorite search queries, and I want to provide a personalized feed based on these queries.
* **Challenges**:
   * Some queries might return thousands of results.
   * The aggregated results need to be sorted globally (e.g., by date or relevance).
   * The aggregated results should possibly be filtered by columns like filestypes\[\] and is\_free
   * The solution needs to be efficient and scalable to handle the increasing data size and query load.
   * The solution needs to be selfhosted.

**Current Approach**: I'm currently considering using Typesense for its simplicity and speed, but I'm open to other solutions that might better suit my needs, especially considering the need for efficient multi-query aggregation and global sorting of results.

I would appreciate any suggestions or insights on:

* Which search engines you would recommend for this use case.
* Strategies for efficiently aggregating and deduplicating results from multiple queries.
* Any other considerations or tips for managing large-scale search operations.

Thank you in advance for your help!

Btw. if there are subreddits that are more suitable for my question please let me know.",1,5,LarsSorensen,2024-01-20 17:10:38,https://www.reddit.com/r/dataengineering/comments/19bghjl/suggestions_for_a_scalable_search_engine_for/,1,False,False,False,False
19atllz,Kubrick Group - Is it a good company? What are they like?,I'm going through the interview process as a data consultant at Kubrick Group. There isn't much online about them so I was wondering if anyone had experience working for/with them and can tell me if the company is a good opportunity?,1,1,Impossible-Kick-8290,2024-01-19 20:59:13,https://www.reddit.com/r/dataengineering/comments/19atllz/kubrick_group_is_it_a_good_company_what_are_they/,1,False,False,False,False
19allhd,IBM datastage to Spark,"My client has 100s of IBM datastage jobs and now wants to move into open source solution like spark.

It's a hard job to go through each datastage job and understand it's functionality then start converting it.

My suggestion was to use the requirement or mapping documentation if available and start the conversion. 

Do we have any other solutions or ready tools for conversion?",1,2,soujoshi,2024-01-19 15:26:46,https://www.reddit.com/r/dataengineering/comments/19allhd/ibm_datastage_to_spark/,0,False,False,False,False
19akfno,Help Dbt and loading encryption key pgcrypto,"Hi,
I work in a database that uses postgresql with column level encryption.

I plan to decrypt all the data while Dbt orchestrates the transformations in ephemeral tables, so they just get dropped once everything is done running.

Once all the transformations are done I plan to encrypt the data again in their final data sources.

Is there a way to pull my encryption key from the vault, or store it somewhere, without putting it as a var directly in the Dbt project file or hard coding it in each model?

Using Dbt cli (not cloud) 

I will need to pass the key to each of the models that encrypts/decrtpts sensitive data in respective columns. 

Just trying to figure out how to do this without hard coding the key in the models themselves. 

Is it possible to make a macro that grabs it from a file? Or something else? 

Thanks!",1,2,Substantial_Ranger_5,2024-01-19 14:33:21,https://www.reddit.com/r/dataengineering/comments/19akfno/help_dbt_and_loading_encryption_key_pgcrypto/,1,False,False,False,False
19ai52b,Snowflake beginner query,"I'm a beginner in Snowflake(& to cloud too) and recently got access to snowflake in our company. 

I have been using snowflake only to query and pull data as and when and required. Also taking courses on their official website to improve. 

So my question is - what tasks I could do in Snowflake which would help me showcase Snowflake as my primary skills, apart from normal data querying?",1,11,GuardObjective9018,2024-01-19 12:37:04,https://www.reddit.com/r/dataengineering/comments/19ai52b/snowflake_beginner_query/,0,False,False,False,False
19aff6p,Super-fast deduplication of large datasets using Splink and DuckDB,,1,0,RobinL,2024-01-19 09:40:33,https://www.robinlinacre.com/fast_deduplication/,0,False,False,False,False
199qn92,Prefect Users: Is it possible to have Flow dependent dynamic runtime environments?,"I’m trying to run the new Prefect setup with a worker, a work pool, a flow, and prefect server. The flow has dependancies on specific versions of requests, pandas, and selenium. I backed up the flow in a remote gut repository and set it up so that workers shall pull it from this repository before every flow run. So, this way, flows stay up to date with the main branch of remote storage.

Right now, it seems like the only way to get a worker to run this flow is to build the worker in an environment that contains all the flows library dependencies. Such as, creating a `venv`, `pip install -r requirements.txt`, `source ./venv/bin/activate`, then `prefect worker start` with environment variables set for the PREFECT_API_URL and PREFECT_API_KEY. Then the worker will connect to the server, see it’s assigned work pool has work, pull the updated flow code from its repo, and execute the flow.

Am I misunderstanding something though? This suggests that every worker must be built with foreknowledge of the flows it will run and the dependancies of that flow. Let’s say you want to add a new flow with new dependencies, seems like you’d need to rebuild the worker to contain those dependencies. If the two flows have conflicting dependencies, you’d need an entire new worker and work pool. This means workers and work pools are essentially organized by clustering compatible tasks based on their dependancies. That specific architecture also seems to limit the usefulness of workers and work pools, as they now must follow this specific organization pattern.

Is there any way at all to associate requirements with a flow, or a python `venv` with a flow, so that the worker just [builds and] activates the necessary `venv` prior to each flow run?

If not, what am I missing? How are we supposed to set up flows that have package dependancies without imposing additional complexity on how the platform much be used and planned?",1,0,Duck-Delta,2024-01-18 13:53:53,https://www.reddit.com/r/dataengineering/comments/199qn92/prefect_users_is_it_possible_to_have_flow/,1,False,False,False,False
199py4s,Ingesting Webhooks From Stripe – The Better Way,,1,0,Glittering_Bug105,2024-01-18 13:18:07,https://memphis.dev/blog/ingesting-webhooks-from-stripe-the-better-way/,1,False,False,False,False
199j43i,MPP Database Consultant GPT,"I just created a custom GPT to explore various MMP DBs. 

If you have access to the OpenAI GPT Marketplace, please give it a try at https://chat.openai.com/g/g-yrhRXkSOH-mpp-database-consultant. 

I would appreciate any feedback on what is working and what is missing.

Thank you!",1,0,drighten,2024-01-18 05:57:13,https://www.reddit.com/r/dataengineering/comments/199j43i/mpp_database_consultant_gpt/,1,False,False,False,False
199gmbd,question for azure DEs,"Coming from GCP, etls, pipelines, maintenance routines, data manipulation, everything is -or can be- code. (beam, airflow, spark, functions, bash scripts, terraform). Code that is usually version controlled on a git repo, and cicd pipelined to deploy. I am aware that there are some no-code tools, such as data fusion, but hadn't really seen those used for production.

Now, I am facing an azure project for the first time, so having my first contact with this cloud. I've seen that in the team, everyone uses the web interface, so for example if required to check the contents of some parquet files in a storage container, they would download them from the storage, by using the web portal, to the local machine and run there a python script, while in gcp you tipically do that using a cloud shell or even a vm, by using shell commands (sdk or client libraries). Is this the way to go in azure? am I missing something?

In the same sense, when they work with data factory, they just click and configure. is this really a no-code tool?  If it is, is this at least creating a pipeline file in the backstage that you at least back up? can the data factory pipelines be version-controlled in a repo? 

by the way, this idea  of backing it up is coming because in the same way, they keep several versions of the power bi pbix files on their local machines. 

&#x200B;",1,10,untalmau,2024-01-18 03:41:50,https://www.reddit.com/r/dataengineering/comments/199gmbd/question_for_azure_des/,1,False,False,False,False
199ghsm,How do you think the job market will be this year (2024)?,Title,2,10,marcelorojas56,2024-01-18 03:35:07,https://www.reddit.com/r/dataengineering/comments/199ghsm/how_do_you_think_the_job_market_will_be_this_year/,0,False,False,False,False
199fdad,"Interview prep - DS&A, System Design, Probability","I’m interviewing for a new data engineering position. I had the recruiter screening and they explained the rest of the interviews will be technical / about my experience, and then technical with data structures, algorithms, system design, and Bayesian probability problems. I’m a data engineer currently and haven’t touched any of that in years since my schooling. I’m actually a little surprised any of this (other than data structures) would appear in a data engineering interview. 

Any good resources out there aside from just doing Leet Code?",1,7,justanator101,2024-01-18 02:39:57,https://www.reddit.com/r/dataengineering/comments/199fdad/interview_prep_dsa_system_design_probability/,0,False,False,False,False
199drv2,Any sql editor plugin for eclipse ?,"DBeaver is a great tool but the sql text editor is awful.   
Writing PLV8 functions in this editor is very time consuming.

Is there any eclipse plugin that could save the day ?",1,0,dsl400,2024-01-18 01:23:34,https://www.reddit.com/r/dataengineering/comments/199drv2/any_sql_editor_plugin_for_eclipse/,1,False,False,False,False
199bcw0,What’s a good approach to evaluating an existing data engineering code base / library and determining how to improvement it.,"For detail , let’s say it’s written in python and it has a series of methods that help you extract data from various apis, websites and other databases , land it in cloud storage , transform and load it into a database. 

Improved would be easier to develop net new pipelines with the database, improved ability for data quality checks at a system level , improved monitoring and logging. 

Any good things like these are the basics / foundations need to exist and elevated improvements. As well any AI that we feel is good for code review ?",1,3,citizenofacceptance2,2024-01-17 23:35:28,https://www.reddit.com/r/dataengineering/comments/199bcw0/whats_a_good_approach_to_evaluating_an_existing/,0,False,False,False,False
199779a,Demo Dataset,"Hi,  


Help! i'm searching for the most awesome dataset(s) you have ever seen💪🚀

I'm trying to make a demo with DBT & Snowflake, I want to use the demo prove that two tools can solve ""real life"" problems 🛠️⚙️❄️

Therefore I have these requirements:

* Minimum one table with minimum **1tb** data
* Minimum **20** tables
* High variety in d**ata types**
* As many **file types** as possible; CSV, JSON, Parquet etc

&#x200B;

Do any of you have experience with public data sets that meet these requirements?

&#x200B;",1,9,formaldehyden,2024-01-17 20:44:31,https://www.reddit.com/r/dataengineering/comments/199779a/demo_dataset/,0,False,False,False,False
1993z9w,Seeking Advice: Egress Concerns,"What are the technical requirements for an organization to justify using a SaaS offering? 

Context: We are evaluating a fully managed SaaS platform where data is egressed from user data sources (any cloud DB/DW/Object storage i.e. RDS, Redshift, S3). They are SOC2 certified with ISO27k in-process. Everything is encrypted end-to-end/in-transit/at-rest. They offer a Bring-Your-Own-Cloud approach but do not have things like VPC-peering or PrivateLink equivalents.",1,0,Quirky-Repair6791,2024-01-17 18:36:54,https://www.reddit.com/r/dataengineering/comments/1993z9w/seeking_advice_egress_concerns/,1,False,False,False,False
1991so2,Champions Program for Apache Airflow- Invite to Apply,"Hey All!

Today, I'm launching [a project that I have been working on](https://www.linkedin.com/posts/briana-okyere_introducing-the-astronomer-champions-program-activity-7153422011896074240-qimp?utm_source=share&utm_medium=member_desktop) for the last 6 months, and I want to share it with all of you.

The Astronomer Champions Program for Apache Airflow aims to **recognize outstanding data practitioners worldwide** who have **demonstrated excellence in leveraging the full capabilities of Apache Airflow in diverse capacities**. Today, I'm celebrating our Inaugural Cohort, and if you are passionate about Airflow, please [apply](https://docs.google.com/forms/d/e/1FAIpQLScSKVzfRf3wppjUbzx0dUzFLwoP66ZfQ6rYLjk9ZASzpKA2Dw/viewform) to our next cohort.

[**Learn more about the program here**](https://www.astronomer.io/blog/introducing-the-astronomer-champions-program-for-apache-airflow/)**, and feel free to respond with any questions!**",1,0,BrianaGraceOkyere,2024-01-17 17:12:00,https://www.reddit.com/r/dataengineering/comments/1991so2/champions_program_for_apache_airflow_invite_to/,0,False,False,False,False
198zyr4,API Orchestrator Solutions,"API Orchestration Solutions

Hi,

I am looking for an API Orchestrator solution.

Requirements:

1. Given a list of API endpoints represented in a configuration of sequence and parallel execution, I want the orchestrator to call the APIs in the serial/parallel order as described in the configuration. The first API in the list will accept the input for the sequence, and the last API will produce the output.
2. I am looking for an OpenSource library-based solution. I am not interested in a fully hosted solution. Happy to consider Azure solutions since I use Azure. 
3. I want to provide my customers with a domain-specific language (DSL) that they can use to define their orchestration configuration. The system will accept the configuration, create the Orchestration, and expose the API. 
4. I want to provide a way in the DSL for Customers to specify the mapping between the input/output data types to chain the APIs in the configuration.
5. I want the call to the API Orchestration to be synchronous (not an asynchronous / polling model). Given a request, I want the API Orchestrator to execute the APIs as specified in the configuration and return the response synchronously in a few milliseconds to less than a couple of seconds. The APIs being orchestrated will ensure they return responses in the order of milliseconds.",1,5,RedHawk004,2024-01-17 16:01:44,https://www.reddit.com/r/dataengineering/comments/198zyr4/api_orchestrator_solutions/,1,False,False,False,False
198zbgn,VSCode vs. IntelliJ community edition,"Had an interesting debate at work (RTO brings those stuff) regarding IntelliJ community edition and VSCode. I am looking to install plugins for working with Kafka.

While IntelliJ has Kafka plugin available as part of their Ultimate edition, there is no way to install one with the community edition. VSCode is more flexible on that front yet lacks some capabilities on the actual programming side. 

&#x200B;

I wonder if you had similar experiences and which IDE are you currently using? ",1,1,joinjoin_oom,2024-01-17 15:35:12,https://www.reddit.com/r/dataengineering/comments/198zbgn/vscode_vs_intellij_community_edition/,1,False,False,False,False
198w3ao,Comparing graph databases?,"Have a project where I need to compare two different knowledge graph in SQL. 
Is there a paper or a book that I can refer to that can point me in the right direction?

I know there is schema comparison for SQL but not sure if that is even applicable for graph database types. 

Any help is much appreciated.",1,0,Whole-Yogurtcloset16,2024-01-17 13:04:41,https://www.reddit.com/r/dataengineering/comments/198w3ao/comparing_graph_databases/,1,False,False,False,False
198ur91,Question About SQL Practices in Microsoft Data Warehousing Tutorial,"HI All, 

I'm currently going through a Microsoft tutorial on data warehousing and came across a T-SQL script that has raised some questions about best practices. Here's the full code from the tutorial:

     CREATE SCHEMA [Sales]
     GO
            
     IF NOT EXISTS (SELECT * FROM sys.tables WHERE name='Fact_Sales' AND SCHEMA_NAME(schema_id)='Sales')
     	CREATE TABLE Sales.Fact_Sales (
     		CustomerID VARCHAR(255) NOT NULL,
     		ItemID VARCHAR(255) NOT NULL,
     		SalesOrderNumber VARCHAR(30),
     		SalesOrderLineNumber INT,
     		OrderDate DATE,
     		Quantity INT,
     		TaxAmount FLOAT,
     		UnitPrice FLOAT
     	);
        
     IF NOT EXISTS (SELECT * FROM sys.tables WHERE name='Dim_Customer' AND SCHEMA_NAME(schema_id)='Sales')
         CREATE TABLE Sales.Dim_Customer (
             CustomerID VARCHAR(255) NOT NULL,
             CustomerName VARCHAR(255) NOT NULL,
             EmailAddress VARCHAR(255) NOT NULL
         );
            
     ALTER TABLE Sales.Dim_Customer add CONSTRAINT PK_Dim_Customer PRIMARY KEY NONCLUSTERED (CustomerID) NOT ENFORCED
     GO
        
     IF NOT EXISTS (SELECT * FROM sys.tables WHERE name='Dim_Item' AND SCHEMA_NAME(schema_id)='Sales')
         CREATE TABLE Sales.Dim_Item (
             ItemID VARCHAR(255) NOT NULL,
             ItemName VARCHAR(255) NOT NULL
         );
            
     ALTER TABLE Sales.Dim_Item add CONSTRAINT PK_Dim_Item PRIMARY KEY NONCLUSTERED (ItemID) NOT ENFORCED
     GO
        
     CREATE VIEW [Sales].[Staging_Sales]
     AS
         SELECT * FROM [ExternalData].[dbo].[staging_sales];
     GO

 My question revolves around the use of the following line:

     IF NOT EXISTS (SELECT * FROM sys.tables WHERE name='Fact_Sales' AND SCHEMA_NAME(schema_id)='Sales')
    

In this script, a schema is created, and then there's a check if a table exists within that schema. I find this approach unusual. Why would a table exist in a schema if the schema itself was just created? I've read several other Microsoft materials, but this is the first time I've seen this implementation. Also, concerning the line:

    ALTER TABLE Sales.Dim_Customer add CONSTRAINT PK_Dim_Customer PRIMARY KEY NONCLUSTERED (CustomerID) NOT ENFORCED 

I recall other Microsoft tutorials where tables and constraints were created simultaneously, without using ALTER TABLE.

Is this a common or recommended practice in SQL, or is there a specific reason for this approach in the context of data warehousing?

Any insights or clarifications would be greatly appreciated. Thank you!",1,0,Ok-Necessary-6455,2024-01-17 11:48:36,https://www.reddit.com/r/dataengineering/comments/198ur91/question_about_sql_practices_in_microsoft_data/,1,False,False,False,False
198tb7f,DuckDB vs ClickHouse performance comparison for structured data serialization and in-memory TPC-DS queries execution,,1,0,dingopole,2024-01-17 10:14:07,http://bicortex.com/duckdb-vs-clickhouse-performance-comparison-for-structured-data-serialization-and-in-memory-tpc-ds-queries-execution/,1,False,False,False,False
198sxiv,Data quality library for spark?,Are there any data quality libraries for Spark that you recommend based on your experience? We plan to gather metrics in Spark after processing the data and send them to Prometheus for visualization in Grafana. Any suggestions?,1,3,VegetableRecord2633,2024-01-17 09:47:25,https://www.reddit.com/r/dataengineering/comments/198sxiv/data_quality_library_for_spark/,1,False,False,False,False
198oz5d,How do I know replication is up to date?,"Customers run MinIO wherever they need fast, resilient, scalable object storage. MinIO includes several types of replication to make sure that every application is working with the most recent data regardless of where it runs. We’ve gone into great detail about the various replication options available and their best practices in previous posts about [Batch Replication](https://blog.min.io/announcing-minio-batch-framework-batch-replication/), [Site Replication](https://blog.min.io/minio-replication-best-practices/) and [Bucket Replication](https://blog.min.io/active-active-replication/). 

[https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm\_source=reddit&utm\_medium=organic-social+&utm\_campaign=replication\_up\_to\_date](https://blog.min.io/how-do-i-know-replication-is-up-to-date/?utm_source=reddit&utm_medium=organic-social+&utm_campaign=replication_up_to_date)",1,0,swodtke,2024-01-17 05:28:28,https://www.reddit.com/r/dataengineering/comments/198oz5d/how_do_i_know_replication_is_up_to_date/,1,False,False,False,False
198idro,Tips for summer,"I am currently in my freshman year of college and am thinking of going into data engineering, and had a few questions before I start my sophmore year (hopefully in data engineering).

What are the best majors or minors to pair with data engineering degree. Is it a good idea to to a business major alongside it?

Also if you have any tips whatsoever on what I should be working on over the summer, or if there's anything I can learn to hopefully get me an internship in my sophmore year or to just get ahead of the competition.",1,8,Karma-4U,2024-01-17 00:09:09,https://www.reddit.com/r/dataengineering/comments/198idro/tips_for_summer/,1,False,False,False,False
198gxzg,Which GCP database to use?,"Never used NoSQL before and been a long time BigQuery user. Can you help me think through if BQ or BigTable is the solution I need?

I have to batch ETL a star schema database into our GCP environment every day. Records will populate a database for analysts to query (exact use cases unclear) and populate a web hosted map (the data contain lat/long). The data are case files of user problems that will have at least 3 entries (create complaint -inspect problem-resolution) and an unbounded upper limit of inspections (impossible to know how many are needed (10 is routine)). Let’s call this progressing workflow the “status”. Each case file can have one or many problems (maybe a user has 5 issues they need addressed). The problems may be addressed asynchronously. I need to load in the dataset with each entry as case files are added or progress toward resolution. There are about 250K case files and that number could grow a good bit. Depending on table design, the multiple status and problem per case file could lead to a huge number of rows. One bit of good news is that there are a small number of columns to deal with. Thanks for reading this far :)

I can see a few options for where to load data (suggest others if you like!). Writes happen only once per day and this should be much more of a read heavy design. 
1) have a fully denormalized BQ table, deal with redundant data in all columns except the one or two that contain the progressing updates to the case file. 
2) Least favorite just because it sounds non-performant(?): append new info to the status field each day (DML the existing field to have all previous data and then the new data as a record or structure). 
3) Most intriguing for me: use BigTable and have all the static info as a column family. Have the status field update as a time stamped k/v pair in a second column family. 

Would populating a map daily (so a read of every case file’s lat/long) be performant from BigTable? Single case file queries (from another component of the web app) would be easy in BigTable I’d think. Is it cheaper to just have the fully denormalized BQ table for the queries? Any thoughts? I can provide more info as questions arise if needed. Thanks!!!",1,7,Ornery_Vanilla1902,2024-01-16 23:08:03,https://www.reddit.com/r/dataengineering/comments/198gxzg/which_gcp_database_to_use/,0,False,False,False,False
198diz2,Opinion on a project,"I did a project with Python, Airflow, Docker, and Microsoft Azure last month, and I wanted to get some suggestions for it. I created a dataset for video games released between 2000 -2022 using RAWG API for, filtered by PlayStation, Xbox, and PC games. This is my first project with Airflow / Docker and wanted to know if its considered a professional data engineering project to showcase in my portfolio.   Any suggestions on how to improve the GitHub repo to better display what I did would be much appreciated!

[https://github.com/asadgun006/Video-Game-Warehouse](https://github.com/asadgun006/Video-Game-Warehouse)

Ps. I did not know that there was already a dataset available on Kaggle before I made this. However, the code for that project seems relatively complex for using the RAWG API for extracting the game details. I was able to do this with the free number of API calls RAWG gives you.

&#x200B;

&#x200B;",1,3,AlwaysAsad,2024-01-16 20:50:24,https://www.reddit.com/r/dataengineering/comments/198diz2/opinion_on_a_project/,1,False,False,False,False
1987pog,[Opinion] The Data Market is not consolidating. It is growing in Complexity like never before.,,1,11,Pleasant-Guidance599,2024-01-16 16:59:24,https://www.y42.com/blog/data-market-is-not-consolidating,0,False,False,False,False
1987mrf,Engineering and Development with Non-Productive Data???,"So coming out of a conversation including our Senior Platform Architect (Software Background), he says that IT developers/Engineers who are responsible for developing data integration pipelines and data models in a data warehouse are not allowed to access live/production data and says everything can be built from sample/testing data. Basically build what you need in a development environment with only sample data and deploy to prod, but you have no access to the data in prod. I get that you want to limit access to end users on productive data, but limiting data on the IT data engineering/BI development team??? I don't know...

How can you monitor data quality or have any type of data observability on top of data you aren't allowed to access? We need to ensure data quality on productive data one way or another. How do you figure out how to conform different customer tables from a CRM and ERP into a single table without having access to the actual data from those systems?

This idea of only needing test data makes since from a software development point of view where the data scope is relatively small and finite especially when compared to an enterprise data platform. But an enterprise data platform is not some piece of software, data is largely dependent and not finite. 

So, are you guys building things with actual data from your productive systems, or are you using test/sample data for development? Is this at all common practice?

&#x200B;",1,3,EarthEmbarrassed4301,2024-01-16 16:56:09,https://www.reddit.com/r/dataengineering/comments/1987mrf/engineering_and_development_with_nonproductive/,1,False,False,False,False
1980zvj,Incremental load between private SQL DB and AZURE MI,"Hello,

I have the following use case:

There are dozen of SQL tables on private server. I would like to pull the data into Azure MI. Sync should be performed daily. 

I came up with one idea:

1. Activation of CDC feature for SQL source tables.
2. Use Azure Data Factory to extract data from cdc tables and load it to Azure MI tables. 
3. At the beggining whole state of tables should be migrated.

&#x200B;

Questions:

1. How to handle deletions?
2. Should I first extract data to some Blob Storage and then load it to destination tables?
3. Maybe you have better proposals?",1,4,BigDataMax,2024-01-16 11:42:50,https://www.reddit.com/r/dataengineering/comments/1980zvj/incremental_load_between_private_sql_db_and_azure/,1,False,False,False,False
197ydsd,Career advice on salary,"Hello all I'm currently a Junior Data Engineer at a company based in California. I work remotely from Oregon. I'm currently on track for a promotion to simply Data Engineer. When I first joined the team most of my work consisted of simple database migrations, running/monitioring jobs in Kubernetes and simple Pandas transformations.

&#x200B;

Now I've developed whole repo's worth of ETL for reporting and analytics to our data warehouse. Ensuring almost all our namespaces are running at high performance. I've deployed multiple machine learning models developed from the data science team. Assisted in our transition to utilizing DBT across our entire codebase. My boss' long term goal for me is to implement AirFlow in the future to make our downstream jobs more fault tolerant.

&#x200B;

I currently make $95,000, what is a reasonable salary I can negotiate for?",1,8,thetruthhurts351,2024-01-16 08:51:27,https://www.reddit.com/r/dataengineering/comments/197ydsd/career_advice_on_salary/,0,False,False,False,False
195ketb,oracle equivalent for mac?,"can i work with it through the vscode extension tho? i'd like to know cause thats what we are studying in uni for now , but if thats not the case then i would be okay with a close equivalent  ",1,4,Getsuga_H,2024-01-13 09:38:11,https://www.reddit.com/r/dataengineering/comments/195ketb/oracle_equivalent_for_mac/,0,False,False,False,False
1954xw0,How to Migrate from Athena/JSON on S3 to Presto on Unix.,"We are currently using Athena with JSON files in S3. We use all Presto SQL features - our JSON files and corresponding Athena tables have arrays, etc. What would you recommend for a local stack on Unix? Install Presto + JSON as local files or Presto + Hadoop/Hive? Are there any other options? Thank you, -- Alex",1,0,flareplf,2024-01-12 20:29:40,https://www.reddit.com/r/dataengineering/comments/1954xw0/how_to_migrate_from_athenajson_on_s3_to_presto_on/,0,False,False,False,False
1952e3i,Joining a New Team,"I'm moving to a new team DataOps at 50% capacity. This was announced to be effective December 2023 no significant movement on the same. I tried to reach the team lead in December she mentioned she is busy and will reach back. Nothing happened. She gave me a link for video dated seven months its already a legacy as the meeting she mentioned there is going to be lot of changes. Again in January 2024 I reached out again asking for more information, spoke over zoom for the first time. Gave me few run books to look at. I asked provide me an architecture which I can understand what job I'm going into. Its been a week now no other progress don't know I will be blamed and it seems it might be a way to lay me off. Any advise on this. I cant blame the the tem lead to the director cos I have been already raising my voice to the director for the past couple of years about stagnation",1,1,BeginningAd4923,2024-01-12 18:43:38,https://www.reddit.com/r/dataengineering/comments/1952e3i/joining_a_new_team/,0,False,False,False,False
1950lxe,Great video on Spark internal workings,"Hi, I'm preparing myself for a interview for a data egeneer role next week, and I'm asking you for a good video material on Spark internal workings.
It should cover some of the following topics:
1. Partitioning
2. Shuffling
3. Persistence and Caching
4. Broadcasting
5. Catalist optimiser
6. Sort merge join

Reading materials would also be fine but I prefer video materials with good explanation of those topics. 

Thanks in advance.",1,1,dark_knight_bg,2024-01-12 17:29:42,https://www.reddit.com/r/dataengineering/comments/1950lxe/great_video_on_spark_internal_workings/,0,False,False,False,False
194cjs0,What I would do differently getting into Data Engineering in 2024,"After nearly 10 years in the industry I collected some of my thoughts about what I would do differently if I started out today.

Would love to hear similar retrospective insights from ya’ll!",1,4,dan_the_lion,2024-01-11 21:08:41,https://www.arecadata.com/what-would-i-do-differently-about-getting-into-data-engineering-2024/,0,False,False,False,False
1945vbc,Struggling to find ideal tech stack solution within the constraints of my giant corp,"I've been fighting a back-and-forth battle with one project in particular for awhile now at my job and would love some outside perspective on this.

THE APP:

Without going too in the weeds, I'm using Python's concurrent futures library to, in parallel, generate a few hundred thousand pandas dataframes. Each dataframe is time-series data with 360 rows. These dataframes currently get inserted to a MS SQL Server table with no indexes or pkeys (these are built once all inserts are done). The connection driver is mssql+pyodbc.

This data is heavily aggregated in various ways by the end users. 

THE PROBLEM:

Our SQL Server is SO moody and finnicky when dealing with the parallel inserts. The most common error is a connection timeout, but it could be one of a handful of different errors. I work at a huge corp, and any SQL Server-side changes have to go through a lot of red tape and a DBA who works on a different floor. I've managed to allocate more memory and storage to our db to help the process, and while performance has improved, it's still far from production-ready.

THE CONSTRAINTS:

Our corp's tech stack is a petty gnarly Frankenstein's monster of on-prem stuff. Our two primary database options are MS SQL Server and Dremio (if you even want to call that a db). Compute is handled by an on-prem Kubernetes cluster. We have S3 storage as well. I don't have the ability to ""just throw it on Cloud BigTable"" or anything like that. 

&#x200B;

GOAL:

Improve reliability of this solution without sacrificing the end user's ability to run their aggregations and analytics quickly.

&#x200B;

As far as I can tell, my only other option within these constraints is to write each dataframe as a parquet to S3 and use Dremio to read it like a DB. I'm just worried that analytics will be way slower under these circumstances.

&#x200B;

Would greatly appreciate any insight here!",1,4,ttothesecond,2024-01-11 16:35:17,https://www.reddit.com/r/dataengineering/comments/1945vbc/struggling_to_find_ideal_tech_stack_solution/,1,False,False,False,False
19450qg,YAML errors auto indent using python,I am trying to auto indent any errors in the yaml file with respect to indentation. did anyone of you implement it or any suggestions would be of great help. I am trying using ruaeml.yaml and regex..but with not much progress yet,1,0,MediumZealousideal29,2024-01-11 15:59:35,https://www.reddit.com/r/dataengineering/comments/19450qg/yaml_errors_auto_indent_using_python/,1,False,False,False,False
194304v,Reviews on Data Engineer Academy?,"Hi all, I was looking for a useful and interactive program that could help equip me with the appropriate technical skills I need for my analyst role- I’m moving more towards the data engineering side of my project but there are skill gaps I have that I need to fill. I’m just looking for structure without bootcamp commitment. 

I came across Data Engineer Academy and looked through some of their content. Does anyone have any experience with DE Academy? How does it compare with other alternatives you can vouch for?",1,4,rae190,2024-01-11 14:29:44,https://www.dataengineeracademy.com,0,False,False,False,False
193yc3e,Integrate Azure Data Factory with Sage Business Account Software(Cloud),"Hello Everyone,

I have been tasked to pull data from Sage Business Account Software for the finance team and create a data warehouse.  


I don't have an issue with the other process just I haven't connected Azure Data Factory or Synapse Pipeline with Sage Business Account Software before and there is little to no material online on how to go about it.  


I would appreciate it if I could get any support, guidance or link on how to configure ADF/Synapes Pipeline to connect with Sage.  


Thanks house!  
",1,0,kiddojazz,2024-01-11 10:10:41,https://www.reddit.com/r/dataengineering/comments/193yc3e/integrate_azure_data_factory_with_sage_business/,0,False,False,False,False
193wa6n,Impact on data engineering projects,"Hi all

I’m working as a data engineer from 3 to 4 years in service based startup company. 

I am the only one from my company working with a client team which consists of 3 to 4 data engineers. we are helping data scientists to create a data pipeline for their ML model. It is kind of MLOps ,where we industrialize the models and deploy into production. my main concern is I have worked on 3 to 4 industrialization of machine learning models, but I am not sure the impact of that. 

The POC is done by data scientist and it and the model is given to our team, we create a pipeline with data quality, data ingestion, transformations, monitoring, etc. that would be deployed to the production. Application maintenance team take it further.

 I am trying to find out what Impact am I making to the team and the company but I am not able to understand from where I’ll get to know what the Impact is. There are few models which has been, which has never been used by the business, but we still do some of the enhancements.
What is the best way to create an impact in our projects, how did you do?",1,1,Delicious_Attempt_99,2024-01-11 07:48:08,https://www.reddit.com/r/dataengineering/comments/193wa6n/impact_on_data_engineering_projects/,0,False,False,False,False
193tc80,Enable Rowdependencies on Table,"We want to enable rowdependencies for all the tables running in PROD.  The catch here is can't just alter table to add it but should have to drop and recreate table in Oracle to enable it. 

Is there any ways using latest DE tools to accomplish it",1,0,rajekum512,2024-01-11 04:49:43,https://www.reddit.com/r/dataengineering/comments/193tc80/enable_rowdependencies_on_table/,0,False,False,False,False
193tbef,Balance between speed and quality/maintainability in an analyst-heavy team?,"Hi, I recently had my performance review with leadership in my company and we talked about what could be improved in the existing environment, for context:

\- I am the only data engineer in the team, there are other data engineering teams in the company which focus mostly in maintaining our existing company-wide data pipelines (**Snowflake, Databricks, SQL Server**) that the analytics teams get data from, my team is one of those teams.

\- My day is to day is mostly **Python + SQL**, a lot of our existing ETLs are **Stored Procedures** in SQL Server, the team also owns a **Windows VM** where we have several scripts running daily/hourly. There is currently no version control or a proper CICD process for any of this code, if someone wants to make a change they just go ahead and do it. Most of these scripts are owned by analysts and changes are made by them.

\- Analysts also have access to and frequently modify the Stored Procedures to modify logic or add new columns to tables.

\- I don't really get to participate in the development of the data pipelines that feed data into Snowflake, or the SSIS that moves data in SQL Server, our department has its own schema in the data warehouse that we maintain, mostly through these stored procedures and scripts.

\- There are some Databricks notebooks scheduled on jobs, with also no version control, which move some data from Snowflake into SQL Server.

&#x200B;

Wondering if you can provide any suggestions on how this environment could be improved, I started by adding version control to the scripts that I own, but not sure about how people who have never used Git before would feel about having me push them to start using it.

I'm also working on understanding how existing teams use Azure to build and deploy their code. To be honest, I'm not even aware, is 'deploying' data pipelines a standard in the industry?Leadership suggested that we need to find some balance and would be open to adding this 'complexity' and maintainability when related to more business-critical portions of our data.

&#x200B;

Any advice would be greatly appreciated, thank you!",1,3,Fuzwipper,2024-01-11 04:48:28,https://www.reddit.com/r/dataengineering/comments/193tbef/balance_between_speed_and_qualitymaintainability/,1,False,False,False,False
193d01s,Freelancing/Contract Structure,"I am looking to start doing some pipelining and analytics/dashboarding on the side for some local small businesses but have a few questions relating to how I should structure our contracts and bidding jobs for them.

Do you guys charge by the hour or by the job/project? When it comes ongoing analysis and dashboarding, what/how do you charge for maintenance and upkeep costs of the dash? Does it make sense to sign a one year contract for continued support and then reevaluate next year or start with outlined jobs?

Would really appreciate any resources y’all have around this!",1,2,iHusk,2024-01-10 16:58:17,https://www.reddit.com/r/dataengineering/comments/193d01s/freelancingcontract_structure/,1,False,False,False,False
193ar4k,Any other internal tools that you make aside from dashboards/pipelines?,I am curious if the people here who make dashboards or pipelines (or both) develop any other kind of internal tooling for their company?  I've recently had an interest in the idea of developing tools for people to use but I am unsure what it looks like in practice.,1,2,codeejen,2024-01-10 15:23:34,https://www.reddit.com/r/dataengineering/comments/193ar4k/any_other_internal_tools_that_you_make_aside_from/,1,False,False,False,False
193acll,"Ticketing/Change Tracking for reports, modeling, and etl?","So.... I don't really know what we want/need here yet so I think I am hoping to get clarity on how others are handling this but let me explain what we currently do and you can get an idea of what you do differently/better.  


Right now we're using sharepoint for users to submit report requests for Cognos. If new data is needed in the warehouse or a data change is needed, our BI team creates and ETL request ticket in a new list/form on sharepoint. The ETL guys then work the ticket making notes and documenting the Datastage jobs affected and what package was pushed to complete the ETL request. There is no versioning system implemented other than package level backups from the test and production environments (only applicable for edits to existing jobs of course). Once the ETL ticket is done, there is a workflow to send the request to the FM modeler if needed (manually flagged in the ETL ticket). The main concern right now is that its very difficult to review changes made to any given report, tie those changes to ETL changes, and review iterative change reporting for ETL requests. Its a disjoint 'solution' and we feel there has to be a better way, but we are midly stuck within the confines of Sharepoint currently.

I would love to hear about other tools and solutions used, or even just a summary of best practices for this?  


I apologize this is all kind of vague, let me know how I can provide additional clarity.",1,2,I_Am_Jacks_Voice,2024-01-10 15:05:59,https://www.reddit.com/r/dataengineering/comments/193acll/ticketingchange_tracking_for_reports_modeling_and/,1,False,False,False,False
1939wsh,Anybody Have Experience with Ballerina.IO?,"I was off exploring service bus options and stumbled across [https://wso2.com/](https://wso2.com/) which led me to [https://ballerina.io/](https://ballerina.io/).  

I'm just starting up the learning cycle with this and am curious what other people think about it.  ",1,2,Captain_Coffee_III,2024-01-10 14:46:23,https://www.reddit.com/r/dataengineering/comments/1939wsh/anybody_have_experience_with_ballerinaio/,1,False,False,False,False
1934svt,Looking for data engineering final thesis project with IOT application,"I am a junior and will be senior undergrad , my major is computer engineering, so my final thesis should be hardware related. However, I aim to be a data engineer, so I want to build a IoT streaming project. I will receive the data from iot devices via MQTT.   
I am familiar with spark and kafka, any advice about tools and ideas that i can use as a beginner in this project and may it could benefit me in my future job as well.  
This [medium](https://sanchezsanchezsergio418.medium.com/iot-event-streaming-architecture-fb790c634c2f) inspired me to come up with the idea.",1,2,FriendshipEastern291,2024-01-10 09:57:35,https://www.reddit.com/r/dataengineering/comments/1934svt/looking_for_data_engineering_final_thesis_project/,0,False,False,False,False
1930kmd,Pivot Back to Industry,"Hi everyone, 

DE with 6 YOE. My first 4 years were at a small and mid size SaaS company, and my last two have been consulting at one of the big firms. I made the jump to consulting to develop project management,  leadership skills, and gain experience in different industries. I want to head back into tech in an IC role. It feels like my consulting experience is coming off as a net negative during the screening process.  Does anyone have experience pivoting back to industry after consulting? Any advice would be greatly appreciated

Thanks",1,3,boboshoes,2024-01-10 05:22:22,https://www.reddit.com/r/dataengineering/comments/1930kmd/pivot_back_to_industry/,1,False,False,False,False
192yain,Should I study Information Retrieval ?,"Hi   
In my spring semester of master degree in computer Science I took the course called **Information Retrieval**. Actually I don't know whether should I take this course or not. Because I have no knowledge of its application through industry point of view. In this subject we are learning core algorithms and techniques used behind the search engine which includes the topics like Indexes, scoring, Probability models,  Link Analysis - PageRank  etc..  


Can anyone help me whether these topics and knowledge of this subject can help me in future or not if I want to be Data Engineer or Data Scientist in Future ?  
Thank You  
",1,1,cycoder7,2024-01-10 03:22:55,https://www.reddit.com/r/dataengineering/comments/192yain/should_i_study_information_retrieval/,1,False,False,False,False
192w7gf,One Billion Row Challenge with Snowflake and Databend,,1,3,PsiACE,2024-01-10 01:42:24,https://www.databend.com/blog/2024-01-05-1brows/,0,False,False,False,False
192ui49,Does data warehouse serve my system end-users?,"I'm a software engineer working in a company that is a online store with more than 100k users. The company wants to implement a page of all the payments for the sellers, so they can view in a paginated table format and they can export everything to CSV or excel. They can also filter payments in the table.

The infrastructure of the company is based on Mongodb and Nodejs. This page would make a lot of lookups in Mongodb to gather collections of payments, products, users, and so on. In the total it would be 4 lookups on collections with millions of documents. I think that should cause problems in production database because of the heavy queries and we will have a lot of sellers trying to export everything or to consult payments in the page.

Are data warehouses or data lakes suitable for this specific use case? What would you recommend? My understanding, from studying data warehousing, is that they are primarily used by a company's internal employees for business intelligence or data science purposes. However, I haven't come across information about whether system users, as opposed to internal employees, also utilize these resources.",1,1,gsharpchord,2024-01-10 00:25:08,https://www.reddit.com/r/dataengineering/comments/192ui49/does_data_warehouse_serve_my_system_endusers/,0,False,False,False,False
192mvcy,Run ETL Pipelines & Store Data in your GitHub Repos,"Hi r/dataengineering! 

We figured out how to scale GitHub repos to handle large files. One of the side effects of this is that you can use GitHub actions to run data ETL pipelines and store the results in the same repo as your ETL code. No need to manage S3 credentials or setup a bunch of extra tools (e.g. logging / observability).

I wrote a short tutorial here: [https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions](https://about.xethub.com/blog/simple-etl-pipelines-git-xet-github-actions)

This is our starter repo if you want to try this yourself:

[https://github.com/xetdata/easy-etl-template](https://github.com/xetdata/easy-etl-template)",1,5,semicausal,2024-01-09 19:16:52,https://www.reddit.com/r/dataengineering/comments/192mvcy/run_etl_pipelines_store_data_in_your_github_repos/,0,False,False,False,False
192bova,Static Code Analysis for Data Lineage in SAS,"Hi all, I've been tasked with pulling out a view of the data lineage for a number of variables from a series of SAS scripts. 

The scripts look like standard SAS code with data and proc steps. 

I need to pull out what tables the variables come from, the transformation steps and the what tables the variables end up in. 

I'm thinking about building a tool to extract the information in SAS but was wondering about any tips anyone had? 

Thank you all in advance.",1,8,KarmaIssues,2024-01-09 10:27:35,https://www.reddit.com/r/dataengineering/comments/192bova/static_code_analysis_for_data_lineage_in_sas/,0,False,False,False,False
192blx8,Spark connect + Jupuyter Notebook + Docker Cluster,"Hello, I have been trying to solve this problem for several weeks. First of all I'll summarize the case and then I'll do it with all the details. I come here because I can not find anything that solves the problem in forums or even chatGPT, because spark connect was implemented in Spark version 3.4, so it is quite recent.

My goal is to create a spark cluster with docker containers and then use spark connect on a jupyter notebook but this time installed on the host machine (not a container) so I can practice with spark code, while seeing how the tasks run on the container spark cluster. I work on windows, which I think is important to solve the problem I will expose at the end with containers.

Well, being as brief as possible, I am able to install spark on the containers with ubuntu, run the master and connect the workers to the master. When I run ./spark-connect-server.sh -packages... as it says in the documentation I don't know if that spark server is connected or related to the cluster I have set up and I would like to understand the details and steps to follow for this configuration.The other idea that comes to me is that spark connect is independent of the cluster running in the containers but I wouldn't understand its use either.

On the other hand, if in the jupyter notebook with python I use import pyspark and then pyspark.sql following the steps in the documentation, when using sparkSession.builder.remote(""ip and port of the container"") I do not know which is the ip to use, if the one of sparkconnect executed (and how to know this ip), or the one of the master ""ip container:7077"". This is the part that loses me the most because I don't even understand the errors that the program returns. It may be important that I run jupyter from VSCode.


Finally, I would like to explain a problem of connections with the ports of Docker containers. I always have to map the container ports, for example the 8080 of the container running the master, with those of the host machine because otherwise I can't use the container ip 127.17.0.1:8080 from the browser to access the Spark web UI, I can only access as [localhost:8080](http://localhost:8080) with the port mapped in the container creation.


Thanks for reading the post regardless of whether you can provide info or not, and thanks also if you comment providing a solution to one or more of the problems exposed.


Best regards.",1,1,Omaroto,2024-01-09 10:21:59,https://www.reddit.com/r/dataengineering/comments/192blx8/spark_connect_jupuyter_notebook_docker_cluster/,1,False,False,False,False
192ad4h,Dependencies of apache-airflow on unicodecsv,"I'm not sure what I'm doing wrong.

I'm trying to setup Airflow using a fairly standard `pip install apache-airflow`, but it somehow has a dependency on unicodecsv that I have a hard time installing.

The only version I found only had support up to 3.5, which I was able to install on Python 3.5, but how does Airflow support 3.8 when one of its dependency only goes up to 3.5?",1,1,ZirePhiinix,2024-01-09 08:55:17,https://www.reddit.com/r/dataengineering/comments/192ad4h/dependencies_of_apacheairflow_on_unicodecsv/,1,False,False,False,False
191m9g1,Data appending task help,"Let's say I have normalised a table to create my database schema. However, how would new data coming in via the original denormalised format be appended to the normalised version of the table?",1,3,No-Pineapple7188,2024-01-08 14:19:55,https://www.reddit.com/r/dataengineering/comments/191m9g1/data_appending_task_help/,1,False,False,False,False
191lomk,how to apply same changes from dev database to prod?,"i have learned that creating a separate database for development is better than using the live database -- for which i was doing for a while, so i am applying in my database this practice that i learned. Now, whenever there are changes in my development database, I just perform exactly the same (changes in columns, create new table, alter procedure, etc) to the live database.  
It is somehow tedious, so I think, there might be a better way than what I was doing right now. Can you share how do you do it? I am using postgre",1,3,WildNumber7303,2024-01-08 13:52:14,https://www.reddit.com/r/dataengineering/comments/191lomk/how_to_apply_same_changes_from_dev_database_to/,1,False,False,False,False
191jzqt,Unstructured data in snowflake,"Hi,
I have been working on data connector app, so for that I need to store my unstructed data (image) inside snowflake workspace ! Similar to databricks where we can store it in volumes  within db workspace.
Is there any similar way in snowflake, or I can only store a data with structured format with reference to public urls of image ? 
Any similar blogs or reference would be helpful ? 
TIA",1,1,MelodicHyena5029,2024-01-08 12:20:16,https://www.reddit.com/r/dataengineering/comments/191jzqt/unstructured_data_in_snowflake/,1,False,False,False,False
191ittr,"Looking for dbt alternatives in development, with less yml 😬","I wonder if there are dbt altnernatives, even in development phase, that do not use yml, but code. I think what [pulumi](https://www.pulumi.com/) is for terraform, no more yml, only code that you run. I wonder if there is the equivalent in sql transformation",1,5,Srammmy,2024-01-08 11:08:51,https://www.reddit.com/r/dataengineering/comments/191ittr/looking_for_dbt_alternatives_in_development_with/,0,False,False,False,False
191hlw3,Needs for Distributed Processing,"Hello everyone!

I’d like to know what you think are the needs for distributed processing (which focuses more precisely on MapReduce treatments, Spark/Hadoop frameworks, etc.) in the industry atm.

I’m currently working for key accounts customers and I don’t see any need really. Still, that’s the career I’d like to pursue.

Is it outdated already? Too niche?

Do you have any advices on:
- Kinds of structures I could aim for,
- Most relevant companies and/or
- City hubs to work in/for?",1,0,Cultural_Guarantee69,2024-01-08 09:47:08,https://www.reddit.com/r/dataengineering/comments/191hlw3/needs_for_distributed_processing/,0,False,False,False,False
190ztmw,Seeking Advice: Automating ClickHouse Views from JSON Schema/OpenAPI,"Greetings!

I'm currently seeking advice on generating materialized views from OpenAPI/JSON Schema. Are there recommended tools or best practices for achieving this without manual conversions/reinventing the wheel? 

More details: we have an OpenAPI YML schema that describes what kind of data arrives from the product to the S3 server. And from it, to the database.

We convert the YML to JSON Schema for the validator to deploy, and, fetched from the S3 and validated against that JSON Schema protocol, the data comes to our ClickHouse database. We have different versions of the protocol, and we select the particular version based on the ""version"" field in the data.

And so we get the validated/unvalidated raw data. But we need to parse the huge JSON that comprises the most information, with all the game analytics fields and such. At the moment, we write Materialized Views for this task manually. All of the information needed for writing these MatViews is contained withing the protocol, but we do not use it.

I was thinking of writing a Python script that would generate these views based on the JSON Schema variant of the protocol. This is kinda intuitive, and based on the actual data that the validator processes.

But still I wonder. I doubt that I am the only one who got a task like that in the whole industry. Are there any tools, or maybe best practices, to accomplish that task?

Your insights are highly appreciated. 

Thank you!",1,3,soltiamosamita,2024-01-07 19:14:43,https://www.reddit.com/r/dataengineering/comments/190ztmw/seeking_advice_automating_clickhouse_views_from/,0,False,False,False,False
190go68,Architecting Global Data Collaboration with Delta Sharing,,1,0,mjgcfb,2024-01-07 01:57:56,https://www.databricks.com/blog/architecting-global-data-collaboration-delta-sharing,0,False,False,False,False
190fzgv,Crazy to go from DE/AE -> DA?,"TLDR: Trying to figure out if it’s a terrible career move to go from Analytics Engineer (technically Data Engineer job title) to Senior Data Analyst.


Background:
- Have BS and MS in Engineering
- Worked for an airframer (like Boeing, Lockheed, etc). Loved it. Job was analyzing manufactured parts, but lots of SQL data analysis.
- After 6 years was making $90k
- Left for fully remote tech company that does online retail, after two years at tech company, make $140k
- Current job is “data engineer” but really Analytics Engineer. 95% SQL, dbt, fivetran, Snowflake. This has me worried about career growth because I’m not actually learning Data Engineering, so the high pay DE jobs that need streaming/kafka, heavy python, spark, etc all seem out of reach.


I’m now getting recruited by an airline to join them as a Senior Data Analyst. Pay would go down to $130k.


Pros of taking new job:
- It’s an industry I’m passionate about
- I’m best and happiest doing business analytics questions anyway (building pipelines, the rare times I do, is dull. Answering business questions is fun. Also I’m a people person.) 
- Unlimited standby travel perks
- I see a path for eventual career advancement to management, maybe onward to VP etc.


Cons of taking new job:
- Hybrid
- Corporate bloat/HR/etc
- $10k pay cut


Pros of keeping current job:
- They love me at the tech startup. It’s also 100% remote and easy work I do well.
- Gotten good raises every year I’ve been there.
- Pay for data/analytics engineers is obviously generally higher than for analysts.


Cons of current job:
- Stagnating in current job skill wise. Also career wise because we’re a small team, so pay raises keep coming but there’s no one else in Data to manage or move into a leadership position over.


Would I be limiting future career/salary by going DE -> DA? Everyone seems to be trying to go the opposite direction!",1,0,LiterallyNotUsername,2024-01-07 01:24:30,https://www.reddit.com/r/dataengineering/comments/190fzgv/crazy_to_go_from_deae_da/,1,False,False,False,False
1904hil,Tables management in dbt,"is there any way to manage how may schema/tables will appear in my bigquery based on the data source? 
i mean something like this:

my_dbt_project 
    - twitter 
         -table 1 
         -table 2 
    - facebook 
         -table 1 
         -table 2",1,0,OddElk1083,2024-01-06 17:04:36,https://www.reddit.com/r/dataengineering/comments/1904hil/tables_management_in_dbt/,1,False,False,False,False
19048h1,Career advice - next steps,"I have about 7 years industry experience in the following track:

\- 1 year BI developer using Tableau

\- 2 year Corporate Financial Analyst using Tableau / Excel

\- 2 year Data Analyst using Tableau / SQL

\- 2 year Analytics Engineer using Tableau / SQL / dbt / basic Python

I studied Econ and Math in college and basically am self taught all the technical skills via on the job training.  I've done a lot of Dashboard Building, Data Analysis projects recommending the biz,  data modeling, and (most recently) migrating all of our models to dbt (architecting and re-writing the code more efficiently).

I like that I have a blend of perspectives from the Corporate Finance and Data Teams but am curious what others would recommend my next step be.  Open to all advice and criticisms! ",1,6,Snoo-74514,2024-01-06 16:53:29,https://www.reddit.com/r/dataengineering/comments/19048h1/career_advice_next_steps/,0,False,False,False,False
18zl897,DBT - Unit Test Macros,How many people are unit testing their DBT macros. If you are what mechanism are you using?,1,1,Culpgrant21,2024-01-05 23:38:36,https://www.reddit.com/r/dataengineering/comments/18zl897/dbt_unit_test_macros/,0,False,False,False,False
18ze0yd,Project Topics Dataset,"Greetings Everyone !  
I am creating a Project Recommendation System using ML which will recommend exciting projects for engineering students. Project Topics will be related to different domains.  
I am looking for a project dataset containing Title, Description, Keywords etc. dwhich will help me train my model, I am unable to find such dataset on kaggle or dataworld.   


If you guys are aware about any such dataset do let me know .  
Thanks !",1,0,mangoresoham,2024-01-05 18:38:13,https://www.reddit.com/r/dataengineering/comments/18ze0yd/project_topics_dataset/,1,False,False,False,False
18zcxm4,Databricks - Best way to create test or prod catalog based on existing dev catalog,"Just hoping for some clarity or suggestions so thank you in advance

We have an existing dev catalog and a test catalog is soon to be created.  The current plan of action it seems is that once the new environment(catalog) is up we will be creating the schemas and tables based on ddl exported from dev, essentially the same method we used to create the schemas and tables manually in dev.  Is there a simpler and/or better method of creating the schemas and tables (with or without data from dev)?",1,4,in-Ron-Howards-voice,2024-01-05 17:53:20,https://www.reddit.com/r/dataengineering/comments/18zcxm4/databricks_best_way_to_create_test_or_prod/,0,False,False,False,False
18zb1nw,JSON to CSV,"Not sure if this is the right group but I am trying to get data from an API JSON to a CSV file hosted online. I would like to CSV file name to be static so I can read it into Excel and have the API automated to run daily. 

Does one know of a site(s) I can look at to achieve this. I have been looking at: Make, retool and nocodb",1,6,bird_egg0,2024-01-05 16:34:58,https://www.reddit.com/r/dataengineering/comments/18zb1nw/json_to_csv/,0,False,False,False,False
18z9fbb,"Are there any quick and easy dockerized authentication apps that I can run with an app and reverse proxy, preferably persisting user accounts in Postgres?","Hello,

Curious if this golden goose might exist. Are there any open source apps that I can just spin up as a docker service, that offer a frontend for login and user account management in Postgres? Maybe with an `admin` page just for manually managing accounts, though ideally it should be able to manage accounts in an automated fashion.

I would want a reverse proxy to send new sessions to this for login, and upon successful login redirect the user to the desired service (based on url).

I guess it can work by sending any request without a valid session cookie to the authentication service. The user performs login and gets a session cookie. The reverse proxy then sends them to their desired application. Invite only, no sign up.",1,2,DuckDatum,2024-01-05 15:26:16,https://www.reddit.com/r/dataengineering/comments/18z9fbb/are_there_any_quick_and_easy_dockerized/,1,False,False,False,False
18ywtl4,Suggestions from QE to DE,"I’m a QE - ETL Lead with over a decade experience having strong knowledge in SQL,  good knowledge on Python, ETL & DW concepts, Data modelling. 

Planning to switch into DE. Only reason for this switch is the routine tasks, which is not challenging in my current role. I’m more inclined towards technical side of things.

Want to understand the challenges switching my career option. I’m a quick learner, how much time would it normally take to be data engineer. 

Thank you",1,5,FaithlessnessDry4116,2024-01-05 03:30:46,https://www.reddit.com/r/dataengineering/comments/18ywtl4/suggestions_from_qe_to_de/,1,False,False,False,False
18ys2ca,Adapting Career Path,"I have a BSc  in CS (mostly software engineering) and an MSc in NLP (a good UK uni). I then did most of a PhD in CS (esoteric stuff not really DE) but didn’t finish for personal reasons. After that I did 7 years in a research engineer role. In that I’ve done many tasks including: training DL models, full stack development, some basic AWS, conceptualising new projects and presentations, writing reports, setting up data preprocessing and databases + dashboards, scraping and I help co-organise a discussion group with a well known university on A.I. (mostly conceptual).

I should be in a senior position but… I mostly did what feels like random projects that didn’t have much external impact. I never really published (just a couple of times). I did a lot of things but without proper supervision or a team so I question my standards and I feel I have very broad but shallow experience. I’m mostly self taught with ML though I did take some courses and my math level is mostly conceptual: I’ve read a bunch of text books and I get the high level concepts but it’s not internalised. I couldn’t apply it to novel problems easily.

I crave working with a team in a constructive environment (very people oriented) but I have no idea what actual in-the-wild projects involve and how to position myself in the market. I want to be genuinely useful and not bullshit my way into a job. Any advice?",1,3,readthereadit,2024-01-04 23:53:58,https://www.reddit.com/r/dataengineering/comments/18ys2ca/adapting_career_path/,1,False,False,False,False
18yq23g,Do we really need to perform dimensional modeling?,"Tasked with leading a BI/BW team for a mid sized organization.  Plenty of $$$, moderate amount of data.  Many data sources (100+). I am not an experienced data engineer, but I did stay at a Holiday Inn Express last night.  

Doing my research I have read up on Kimball and dimensional modeling and data normalization.  Sounds fucking painful.

Me and my team are Pretty good with Power BI.  Fucking love the Transform data within Power BI.  Using Azure Synapse/ Data Lake for central repository.

The dimensional modeling and normalization seems to be a massive lift.  Feels like we could spend years working on that - meanwhile the rest of the organization is wondering what the fuck we are doing?

I can take two dimensional tables from various sources and just model them in PowerBI.  Do it 1 project at a time.  Can turn a project around in just a few weeks.  People can see we are producing some dashboards in the near future.

With MPP processing power and columnar data bases - am I crazy to say fuck Kimball and fuck OLAP cubes?  I just want to store denormalized tables in the Datalake and model them in PowerBi as needed for individual projects.

Part of me thinks this might be short sighted, but I don’t feel like I have years available to work through fucking Kimball methods for 100+ data sources.

Feedback / suggestions are appreciated.",1,53,AdhesivenessOne6188,2024-01-04 22:31:50,https://www.reddit.com/r/dataengineering/comments/18yq23g/do_we_really_need_to_perform_dimensional_modeling/,0,False,False,False,False
18ypqtc,How do you consume an OData feed for ETL?,"Hello everyone,

I’m working with a company who makes their data available only through UI and OData. Before breaking out selenium, I’m curious if I can consume the data via OData.

I see PyOData seems only to support the v2 protocol out of all 4 versions. Is there a more mature way?",1,3,DuckDatum,2024-01-04 22:20:22,https://www.reddit.com/r/dataengineering/comments/18ypqtc/how_do_you_consume_an_odata_feed_for_etl/,1,False,False,False,False
18yp76x,How to view task durations over time when Airflow web UI Task Duration screen for DAG is laggy / non-responsive?,"Airflow v1.10.7 running in `LocalExecutor` mode.

Trying to find a way to determine what tasks in a DAG have recently jumped up in duration time; the Task Duration web UI for the DAG is being slow and unresponsive. Is there another way to do this? (Any way to debug the slowness of the web UI?)

Trying to view the Task Duration screen for a DAG with many tasks (mostly Apache Sqoop jobs), but web browser keeps hanging when attempting this (happens regardless of web browser type and even when trying locally on the machine running the `airflow-webserver`).

If I wait very long, I can see the graph load (showing way more than the default 'last 25 number of runs' for some reason) and can see that around the past month a few tasks made a step-wise jump in duration and have since stayed at that elevated level for each subsequent run. Now I am trying to figure out what those tasks are, but the fact that the web UI is so laggy when looking at this DAG is making the Task Duration page impossible to use.

Any other common best-practices way to debug this situation? (Any suggestions for debugging the web UI so I can use the Task Duration page again?)

Thanks.",1,1,Anxious_Reporter,2024-01-04 21:58:31,https://www.reddit.com/r/dataengineering/comments/18yp76x/how_to_view_task_durations_over_time_when_airflow/,1,False,False,False,False
18yifsy,Looking for assistance with managing data,"I need help trying to recreate this data format in a different program. We use our current system 

[Original old system](https://preview.redd.it/et78viqfbgac1.jpg?width=517&format=pjpg&auto=webp&s=f5db0a9a5118acb99ede0bbdf00d491064df280f)

[This is what exporting the data looks like into Google Sheets.](https://preview.redd.it/34y7beegbgac1.jpg?width=614&format=pjpg&auto=webp&s=f44f702995fadb16b21076ad5a430c274036ce28)

[This is an edited version of the above data to attempt to make it more understandable.](https://preview.redd.it/oyfpwpvgbgac1.jpg?width=1075&format=pjpg&auto=webp&s=f2638052e655a34a2427d13acc672ac88470fcff)",1,4,No-Introduction111,2024-01-04 17:21:36,https://www.reddit.com/r/dataengineering/comments/18yifsy/looking_for_assistance_with_managing_data/,1,False,False,False,False
18ygih2,Simple document management system.,"Feels like this should be trivial to find, yet, I haven't find a straight forward solution yet.

I'm looking for a document management solution that will:

* Allow us to upload files to a backend that is hosted locally on our network.
* Set custom metadata for each files.
* Allow to search and retrieve the documents based on their metadata.
* Documents can be of any type of files, text and binaries.
* On premise only.

Bonus point if:

* There's some level of access control.
* There's an API.

Note: we have access to Microsoft Suites of products like OneDrive, SharePoint, Teams, PowerApps, etc.

Feels like at this point in time, there should be a project somewhere that does just that. Basic document management with custom metadata and a search engine.

Any suggestions?

&#x200B;

&#x200B;",1,16,HighTechPipefitter,2024-01-04 16:01:22,https://www.reddit.com/r/dataengineering/comments/18ygih2/simple_document_management_system/,1,False,False,False,False
18yf2ya,"Starting Data Engineer Career, potential first data job","Hello all!

&#x200B;

I am an aspiring data engineer introducing myself for the first time.  I am hoping to receive advice and insight from time to time for this community.

A little about me;

* I am currently an IT Technician (going on 3 years) for a large company (2500+ employees).
* My company is currently in the process of building a Data Warehouse Department.  I have been assisting the Data Warehouse Manager sporadically for the past two years.  So far, I have created a few Power BI reports, given some presentations, and helped build an ETL pipeline using Azure Synapse.
* My current skillset is somewhere between beginner/intermediate SQL, Python, and using Synapse.

This year, the Data Warehouse Manager has finally been given the green light to hire two data engineers, a junior and a senior.  I have been told by the manager to apply for the positions and other managers recommend me for the position as well.  

The first green-lit project for the department is to load data to an on-prem data warehouse using SSIS.  I haven't used SSIS before, so the Data Warehouse Manager has given me the heads up to go ahead and learn it. 

What recommendations do you guys have for learning SSIS, what resources to use and how to go about it?  What advice do you all have for trying to make it into data engineering and being part of a team that will be building a data warehouse from the ground up?

&#x200B;

Thanks everybody,

&#x200B;",1,3,l2enel,2024-01-04 14:58:03,https://www.reddit.com/r/dataengineering/comments/18yf2ya/starting_data_engineer_career_potential_first/,0,False,False,False,False
18ydzes,Airbyte as data integration tool,"I want to explore open source tool Airbyte for data integration.  Some of my data sources are on-prem MSSQL, SAP BW, S4-Hana, salesforce(via API) etc.. my destination would be Snowflake.  We work on AWS.  
Data load would be either full load or delta mode. 
Can I get some suggestions on where to host Airbyte (open source ) and build data ingestion pipelines? Any lead would be helpful. Thanks.",1,2,Liily_07,2024-01-04 14:06:53,https://www.reddit.com/r/dataengineering/comments/18ydzes/airbyte_as_data_integration_tool/,0,False,False,False,False
18yc2u2,New project for dbt?,"what is better storing the raw and transformed data in the same bigquery project in GCP, or create a new one and store them in separate? and why?",1,0,OddElk1083,2024-01-04 12:31:38,https://www.reddit.com/r/dataengineering/comments/18yc2u2/new_project_for_dbt/,1,False,False,False,False
18y98b0,Help need to build realtime dashboard,"I have form builder app, similar to google forms. We want to track the number of views,  number of submissions and manage subscriptions at realtime. We want to show the number of views,  number of submissions at realtime. So can anyone please suggest the architecture in AWS or any open source also ? We are expecting around 30 number of views ,  10 number of submissions per second. This is our development environment url : [https://dev-forms.dculus.com/](https://dev-forms.dculus.com/)  
Thank you.",1,1,natheeshkumar,2024-01-04 09:35:55,https://www.reddit.com/r/dataengineering/comments/18y98b0/help_need_to_build_realtime_dashboard/,1,False,False,False,False
18y8vun,Do you snapshot your metrics ? Or only source data ?,"I know that ""technically"", if you have the history of the data source you can recompute everything. 

But in practice I have never done that, yet I have questions on the metric being changed.

Would it be good practice to apply SCD on downstream data ?",1,2,Srammmy,2024-01-04 09:12:28,https://www.reddit.com/r/dataengineering/comments/18y8vun/do_you_snapshot_your_metrics_or_only_source_data/,1,False,False,False,False
18xrpfl,Seasoned Professionals - What should I learn after Python. C++ or Java,"Like the title says, I'm looking to get another language under my belt besides Python. Those working in Data Engineering, do you think C++ or Java would be more ""bang for my buck""?

I've been using Python for almost a decade and would like to pick up another programming language. Took two Scala courses a while back, but other than working in Spark (Which I don't do for work) I found few other uses for it. I took Java MOOC, but didn't keep up using it cause I got confused a bit with the framework that we were using. I have done some Rust, but I think they hype around it is dying down and I honestly think it will go on to be a good system programming language, but not really a good one for every day use. Haven't used C++ since high school, but willing to re-learn/play around with it.",1,10,Scalar_Mikeman,2024-01-03 19:30:01,https://www.reddit.com/r/dataengineering/comments/18xrpfl/seasoned_professionals_what_should_i_learn_after/,0,False,False,False,False
18xp1se,Testers needed - Snowflake optimization tool,"We are developing a tool that can scan and optimize your snowflake environment for you. The best part? It's completely free in our closed beta! All it takes is 5 minutes of your time and you'll receive a fancy report that could save you a lot of money. 

**Interested? Please ping me for details.** 

&#x200B;

P.S.

The report might include some Memes :)

&#x200B;

https://preview.redd.it/syg585a9h9ac1.jpg?width=750&format=pjpg&auto=webp&s=fa36b9cd0fe8e81e510f949104a3489ae399eb35",1,0,st-yoni,2024-01-03 17:44:38,https://www.reddit.com/r/dataengineering/comments/18xp1se/testers_needed_snowflake_optimization_tool/,0,False,False,False,False
18xp1k3,Fivetran API Client,,1,0,kharigardner,2024-01-03 17:44:24,https://github.com/kharigardner/pyfivetran,1,False,False,False,False
18xnd6m,What projects can demonstrate both of data engineering skills and devops skills ?,They are different roles iam aware and i know that data ops are more close of being data engineer but just curious about my question to increase my skills,1,3,Single-Sound-1865,2024-01-03 16:27:00,https://www.reddit.com/r/dataengineering/comments/18xnd6m/what_projects_can_demonstrate_both_of_data/,1,False,False,False,False
18xfota,Most suitable low-code data integration tools for SaaS,"Hello everyone,   
I am currently researching low-code tools topic to integrate our company's SaaS solutions with business client systems (ERP, CRM, DW, or data lakes mostly). We are looking for something with capabilities to integrate with the most popular systems (aka SAP or Microsoft software, both cloud and on-premises) with predefined connectors, but also with options to connect to other less popular systems (it can be either API or direct db connection).   
For now, I am trying Azure Data Factory for this purpose, but still not sure if it will be suitable for us, do you have any other suggestions?  It might be both cloud or non-cloud tools. Thank you for your answers in advance and hope you have a great day :)",1,1,Myhasik,2024-01-03 09:48:21,https://www.reddit.com/r/dataengineering/comments/18xfota/most_suitable_lowcode_data_integration_tools_for/,0,False,False,False,False
18x6piq,Data architecture for analytics framework," 

Hello all, first post here on reddit. Below I’ve outlined a potential data architecture for a general sports betting analytics framework and would like to solicit feedback as well as any potential off the shelf alternatives that could accomplish the same goals. The data architecture needs to primarily support the following:

1. entity definitions. Things like players, teams, games, etc.
2. entity features. Some quantifiable piece of information about an entity. Usually associated with a given day. I.e. features are often time series for a given entity. Things like player ratings prior to a game, weather impacts on game day, etc. Can be arbitrarily complex - more than simple aggregations.
3. point-in-timeliness. To support proper backtesting, all entities and metrics should be queryable as they existed at some point in time. 
4. ability to iterate quickly on feature research. Should be able to define a complex feature and run on the entire dataset. Entities \* features \* number of days \* number of revisions could be in the millions. Queries here will often be vertical in nature. E.g. you might ask a question like “give me these 3 features across all entities on all days for all revisions”.
5. quick real time feature calculation. Will be used for real time predictions. These queries will often be horizontal in nature e.g. you might ask “give me these 100 features for these 5 entities on this day”. Can also be vertical if a given feature requires history of itself or others.

Given the requirements for both 4/5, I’m leaning towards a polyglot data model, utilizing multiple data architectures to support the different query patterns. Off the top of my head, I’m thinking something like postgres for entity management and point-in-timeliness, MongoDB as the feature store for horizontal queries, and some type of columnar store like parquet files on s3 for fast querying of entire feature history.

Obvious complications are synchronization and consistency across data stores. My intuition says this could be managed crudely with triggers on the entity tables and/or tracking update timestamps across all 3 stores, but I’m well aware that this could snowball into a fragile and extremely complex system. Happy to provide more details if necessary, but I’m hoping there’s enough here to know that I’m either potentially headed down the right path or completely misguided. I’d also be interested in any off the shelf, open source or relatively cheap solutions that exist to fulfill these requirements, as I haven’t found any that satisfy all of them. While I'm committed to spending a decent amount of $ on this, I'm a solo dev who has a day job, so cost does play a major role. Thanks in advance!",1,1,Ashamed-Diamond-4859,2024-01-03 01:31:09,https://www.reddit.com/r/dataengineering/comments/18x6piq/data_architecture_for_analytics_framework/,1,False,False,False,False
18wxfo5,EBCIDIC in spark,"We have a requirement to read mainframe ebcdic files using spark. Has anyone had any luck with this?

Upon research I came across ""cobrix"". Has anyone used this package before? . Unfortunately that's the only one we have come across.

Please suggest if there are any other options that are avaliable to read these files on spark.",1,2,soujoshi,2024-01-02 19:10:35,https://www.reddit.com/r/dataengineering/comments/18wxfo5/ebcidic_in_spark/,0,False,False,False,False
18wq815,Need help with a weird OnPrem Setup which uses Task Scheduler currently?,"Well I have a weird setup to fetch data from one of our onprem systems, it only works via windows and it c# wrapper which we have developed into a python package.

Current we have scripts to fetch data from that onprem system using that python wrapper and then store it as a parquet file which other scripts upload to an azure blob. However currently it runs on windows scheduler whereas I would like to run on something more robust. 

Things I have tried looking into are local ADF, however the way I can see it cant run python scripts from a local machine.

Another one was trying on airflow however windows doesn't support airflow and vice versa, wsl works however that python doesn't work on any form of Linux because it uses some windows features.

Am I being stupid in wanting to move on from it or is task scheduler my best bet for this right now?

Answers to commonly asked question:

1) No it cannot run in container or any form of linux, needs to be windows only.

2) And it needs to run python

3) The package was developed by me so I have tried to make it work on linux however it doesnt. Reason for this approach over the SQL interface we do have for those systems is that this is over 20x-100x faster.",1,1,tecedu,2024-01-02 14:06:46,https://www.reddit.com/r/dataengineering/comments/18wq815/need_help_with_a_weird_onprem_setup_which_uses/,1,False,False,False,False
18we8em,DE skill for LLM project,"I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?

Edit 0:

Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).

I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))

I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?",1,6,basic_of_basic,2024-01-02 02:35:49,https://www.reddit.com/r/dataengineering/comments/18we8em/de_skill_for_llm_project/,0,False,False,False,False
18w6mjf,Which book do I need?,"Hello, I’m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.
I need a book to improve my conceptual understanding of data engineering not only the practical part, note that I’m someone who is losing interest on reading continuously and don’t read books easily.
Which one should I read first?

[View Poll](https://www.reddit.com/poll/18w6mjf)",1,2,Bassemustafa,2024-01-01 21:01:50,https://www.reddit.com/r/dataengineering/comments/18w6mjf/which_book_do_i_need/,0,False,False,False,False
18vyu06,"50+ Incredible Big Data Statistics for 2024: Facts, Market Size & Industry Growth",,1,0,Veerans,2024-01-01 15:21:52,https://bigdataanalyticsnews.com/big-data-statistics/,0,False,False,False,False
18vko16,DB solution for remote student project,"Happy new year! I, a grad student, am planning on working on a data analytics project with my friend who is living in a different state. In this project we are trying to simulate a data lifecycle. We will use a big 'data' set from Kaggle (please recommend any better sources). I (acting as DE and DBA) will create a data engineering pipeline to perform ETL tasks using Python and load this data into a SQL based software that can be remotely accessed by both of us where I will act as a DB admin. My friend(acting as DA) will have access to this database and use it to analyze the data and create reports. This project is very basic and completely theoretical in terms of how the data is connected from one point to the next.

My questions are, 

* Is there a SQL software that permits this? Please suggest some names that are available for free or are open source. 
* Is there a way to connect the flow of data? (I'm considering to completely use SQL to perform the ETL tasks)
* Is there anything we can do to improve the project? 

&#x200B;",1,0,Dull-Atmosphere8478,2024-01-01 00:10:53,https://www.reddit.com/r/dataengineering/comments/18vko16/db_solution_for_remote_student_project/,1,False,False,False,False
18vb7l8,What product/service under GCP/AWS equate to delta live tables in Databricks?,Can't seem to piece that in the puzzle,1,2,Ok-Tradition-3450,2023-12-31 16:25:48,https://www.reddit.com/r/dataengineering/comments/18vb7l8/what_productservice_under_gcpaws_equate_to_delta/,1,False,False,False,False
18v078v,DLT || Python || Aggregate Functions recomputing all the records,"Hi all,   


I am building a realtime dashboard using Databricks Delta Live Tables Pipeline and using the following steps : -   


**Bronze Table** : Using the autoloader functionality provided by databricks, its incrementally ingesting new files records into a bronze table.  
**Silver Table** : Using the **read\_stream function** provided in spark for structured streaming, we are creating the silver table by filtering the records and selecting few fields from the bronze table that are required.  
**Gold Table** : Using the **read function** provided in spark for reading complete record, we are creating the gold table, which is the materialized view and also using aggregate function (SUM), and group by clause to create it.  


**Problem :**   
Bronze and silver table are doing incremental ingestion, however incase of gold table, the entire record in the table is getting recomputed everytime a new record is received in the silver table.  


What I want to ensure is that for the particular group by clause only updates should be performed and rest of the records are locked and dont require any update.   


I have also tried using streaming table instead of materialized view for gold as well, in this case also the entire records are getting recomputed.  
Any help would be appreciated.",1,2,cerebral-assassin26,2023-12-31 05:17:15,https://www.reddit.com/r/dataengineering/comments/18v078v/dlt_python_aggregate_functions_recomputing_all/,1,False,False,False,False
18u58g9,Need Suggestions and Opinions,"I’m a Azure Data Engineer with 3 years of experience I have been working with Azure Services to Migrate and Ingest Data into SQL Data warehouse using ADF, ADLS, ADB, Unix and some other Azure Services which are required in between like VMs and all

I have been stuck and not able to switch jobs as Azure Data Engineer as many need AWS 

Give me some suggestions about how up improve my current skills or should I move to AWS stack or some Project of PySpark and Data Engineering ones which can help me grow


I’m mostly confused with what to do next",1,2,dhruvbaslas,2023-12-30 03:02:44,https://www.reddit.com/r/dataengineering/comments/18u58g9/need_suggestions_and_opinions/,0,False,False,False,False
18tf85z,Messy Data,"What is the best way to clean up messy customer address data and names? Right now, the data is landing into snowflake from Fivetran.",1,4,Used_Ad_2628,2023-12-29 05:20:17,https://www.reddit.com/r/dataengineering/comments/18tf85z/messy_data/,1,False,False,False,False
18tcx87,Mimic Blob Storage on Linux Box(es)?,Working on a project that will include some Linux boxes for servers and want to have essentially a basic S3-esque layer on top of them. Is Minio my best choice? Most objects will be 50-250MB in size and I don’t need a data lake/advanced partitioning of any sort,1,4,ReporterNervous6822,2023-12-29 03:21:50,https://www.reddit.com/r/dataengineering/comments/18tcx87/mimic_blob_storage_on_linux_boxes/,0,False,False,False,False
18t5qul,documentation attributes and metrics in microstrategy / snowflake,"Hello, I'm like DA, it's my first job in BI, in a retail company, it has 92 stores, 3 distribution plants, 1000 microstrategy licenses

The fact is that we have many tables, many attributes and metrics, many users when validating in microstrategy do not know how to cross-reference the attributes and metrics and the new reports they want to create are broken, is there some practical way/template or tool that allows them to instruct them with documentation so they know how to cross-reference the attributes and metrics without breaking the reports",1,0,Icy_Cricket_779,2023-12-28 21:56:02,https://www.reddit.com/r/dataengineering/comments/18t5qul/documentation_attributes_and_metrics_in/,0,False,False,False,False
18t58n3,What Data Integrations are needed for your job?,"Hey everyone.

I'm working on an OpenSource project in the data engineering field.

Honestly, I was tired of FiveTran and their pricing at my previous job so I want to create something that people can use on their own.

I already managed to accomplish the core of the system and create basic connectors like PG Logical Replication into Kafka, WebSockets, and Postgres.

I wonder what data source and destination pairs people need. Like what third-party platforms you want to bring the data from and store it in the database, etc.

Feel free to share your thoughts on that :)  
",1,6,warphere,2023-12-28 21:34:49,https://www.reddit.com/r/dataengineering/comments/18t58n3/what_data_integrations_are_needed_for_your_job/,0,False,False,False,False
18stjub,Apache Superset help needed,"I have an on-premise Apache Superset 2.1.0, it works in a docker container. It's basically in a PoC stage, where we are trying to assess if Superset is the right fit for our purposes.

One requirement that we have is to be able to share created dashboards with people without accounts. Someone in the higher management, for example. 

Ideally, it would work in a user-friendly way, when dashboard creator can make it public on his own. Without having to provide access to the underlying datasets to a specific role or having us do that for him. Just a simple ""make it public"" kind of a button :)

I've tried multiple things with the help of Google Bard, none of which worked.

I've then tried [this guide](https://github.com/apache/superset/discussions/25299), but it didn't work either. In general, searching for help on Superset is frustrating, because it seems that everyone uses a different version with different options on basically anything.

Can you guys show me the way? How tf do I make these dashboards easily available to people without logins?

*Bonus question: Redis container is a part of the package, but superset init shows that it uses local cache for some reason. Am I doing something wrong here?*",1,3,zlobendog,2023-12-28 13:09:30,https://www.reddit.com/r/dataengineering/comments/18stjub/apache_superset_help_needed/,1,False,False,False,False
1abpsxs,Microsoft Access alternative,"As the title states, I am looking for an alternative to Microsoft Access. A system that can intake data as well as store it. A couple that come to mind for me include: Microsoft forms and/SharePoint lists. A bonus would be to be able to auto-populate fields on a form when filling it out. For example, if you entered a serial number, associated information would populate for that asset.",0,2,FuelYourEpic,2024-01-26 18:53:16,https://www.reddit.com/r/dataengineering/comments/1abpsxs/microsoft_access_alternative/,0,False,False,False,False
1abnkaz,The Only Free Course You Need To Become a Professional Data Engineer,,0,6,kingabzpro,2024-01-26 17:18:26,https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-professional-data-engineer,0,False,False,False,False
1ablw4u,Getting UTC time from an unknown timestamp in ADF pipeline,"I have an dataset in ADF, that is connected to an MSSQL database and pointing at a \`BookReservations\` table:

&#x200B;

|*id*|*book\_id*|*booking\_date*|
|:-|:-|:-|
|512|99|2024-01-26T09:18:58.79|
|1024|8732|2024-01-05T14:44:31.22|

I want to add a column in my Data Flow activity called \`booking\_date\_utc\`, but I am struggling to figure out how to do it properly. I do not know what timezone is the server is running, and well it could change as part of the configuration.

After some reasearch, I was able to find this:

    SELECT GETUTCDATE() AS ServerUtcTime, GETDATE() AS ServerLocalTime;

I added the \`GetServerTime\` lookup activity, then saved the time difference in minutes into a pipeline variable \`offsetForUtcVar\` with this expression:

    @div(sub(
        ticks(activity('GetServerDates').output.firstRow.ServerUtcTime), 
        ticks(activity('GetServerDates').output.firstRow.ServerLocalTime)),
     600000000)

But I am stuck here. I cannot seem to figure out how to get the UTC time with the existing \`booking\_date\` in the data flow. Can any1 help, please? ",1,1,ealoles,2024-01-26 16:10:15,https://www.reddit.com/r/dataengineering/comments/1ablw4u/getting_utc_time_from_an_unknown_timestamp_in_adf/,0,False,False,False,False
1abkdrj,Unusual question,"What do you guys think about this subreddit?   
Are there some things that you don't like or would like to be different?

How many of you love programming and consider yourself software engineers specialized in Big Data?",0,16,yinshangyi,2024-01-26 15:04:34,https://www.reddit.com/r/dataengineering/comments/1abkdrj/unusual_question/,0,False,False,False,False
1abeofl,Career prospects from Data Platform Operations role?,"Hi all, hopefully I’ll be receiving an offer today and I was wondering what other roles someone could move into after a few years in this role (I’m not necessarily planning on doing this, but would like to know anyway). I’ve just come out of a junior data engineering role and hadn’t heard of this option before.",0,2,psyyducck,2024-01-26 09:42:48,https://www.reddit.com/r/dataengineering/comments/1abeofl/career_prospects_from_data_platform_operations/,0,False,False,False,False
19f8qkh,Data engineering job transition...need advice,"Hi guys,

Currently I am working as an  automation test engineer  at a well reputed product company for 2.11 yes 
But I want to shift into Data Engineering.
I also cracked some interviews for automation engineer ......a company approached me with 80  per hike for Automation test engineer role
Should I take the job or upskill into a data engineer which might take more 2 to 3 months.
The new job needs me to reallocate to a different location.
So reallocating and again preparing for data engineering will take a much longer time than I estimated.",0,7,Significant_Dot_9732,2024-01-25 12:57:46,https://www.reddit.com/r/dataengineering/comments/19f8qkh/data_engineering_job_transitionneed_advice/,0,False,False,False,False
19f8ppr,LinkedIn hackerank test,"Hi folks, any idea what kind of ds algo to expect in Li senior software Engineer data engineering hackerrank test.",0,0,RepulsiveCry8412,2024-01-25 12:56:25,https://www.reddit.com/r/dataengineering/comments/19f8ppr/linkedin_hackerank_test/,0,False,False,False,False
19f3xxg,Has anyone successfully integrated Airflow to Datahub using the Datahub plugin v2?,"I've already tried but always failed and encountered the same issue. Please check this issue post in the Datahub GitHub repository. Thanks!  


Link to the issue: [https://github.com/datahub-project/datahub/issues/9702](https://github.com/datahub-project/datahub/issues/9702)",0,0,Additional-Breath-12,2024-01-25 07:32:56,https://www.reddit.com/r/dataengineering/comments/19f3xxg/has_anyone_successfully_integrated_airflow_to/,0,False,False,False,False
19exjj3,[Guide] How to Enable System Schemas in Azure Databricks,,0,0,mdixon1010,2024-01-25 01:36:56,https://medium.com/p/7e53131df3ff,0,False,False,False,False
19elg9f,data analysis - logistics,"Hello friends, I'm new to the data area and I've been carrying out some projects focused on engineering and data science.

I am currently working with a dataset from a logistics company that has the following columns below.

I managed to carry out all the pre-processing and now I have a ""clean"" base to carry out analyses. I would like analysis suggestions to better explore the data I have to apply to a machine learning algorithm to improve and further develop my skills.

&#x200B;

NOTE: The database has a 1-year history series with more or less 2 million lines.

&#x200B;

NUMERO\_PEDIDO - 123456789

REGIAO - NORDESTE

BASE\_ENTREGA - BA\_01

MOTORISTA - motorista\_01

ASSINATURA - assinado

VALOR\_MERCADORIA - R$ 10

CEP - 222222222

ENDERECO\_DETALHADO - Rua 1234

CIDADE - Teixeira de Freitas

BAIRRO - Bela vista

DESTINATARIO - comprador\_01

DATA\_ENTREGA - 01-01-2024	

CPF - 12345678

PESO\_COBRAVEL - 0.5

&#x200B;

&#x200B;",0,1,Lopes_BRRJ,2024-01-24 16:44:22,https://www.reddit.com/r/dataengineering/comments/19elg9f/data_analysis_logistics/,0,False,False,False,False
19eiqz6,Why does Redshift's SUM() evaluate differently for VARCHAR vs NUMERIC?,"I have a column that contains a bunch of decimal numbers (USD$), but the datatype is currently VARCHAR(50).  When SUM() is run over this column, I get the incorrect sum.  If I first CAST(.. AS DECIMAL()) though, it evaluates the correct sum.

Anybody know what might be causing the difference in the two SUMs?

And yes, I'm altering the column to be a numeric datatype now!  Just curious what's happening under the hood.

Thanks!",0,9,stonetelescope,2024-01-24 14:47:24,https://www.reddit.com/r/dataengineering/comments/19eiqz6/why_does_redshifts_sum_evaluate_differently_for/,0,False,False,False,False
19ebf07,Hackerrank DE- Python/SQL,"Hello, Does anyone have experience with the HackerRank coding round for a Data Engineering position at Salesforce? What's the difficulty level like, and what types of questions did they ask? Any insights or tips would be greatly appreciated! Thanks in advance!",0,1,Far_Local9130,2024-01-24 07:17:11,https://www.reddit.com/r/dataengineering/comments/19ebf07/hackerrank_de_pythonsql/,0,False,False,False,False
19dm78e,GIST OF DRAGONFLY {www.dragonflydatahq.com} :-A No Nonsense Data Quality Monitoring Platform,"1) Dragonfly checks a company's data processing steps to create pipelines, making sure the data flows correctly.

2) It connects health checks with components, allowing companies to configure listeners for events and get notified when data events occur.

3.) Dragonfly protects against incorrect data by performing Health Checks that assess various aspects of data and generate a Data Confidence Rating (DCR).

4.) Users can choose from a library of pre-written data quality checks or use Dragonfly GPT to convert simple English demands into Health Checks.

5.) Dragonfly is an API-first platform, allowing integration with various databases and data platforms, ensuring data quality reports are easily accessible.

6.) Dragonfly does not modify data, can work with read-only access, and does not copy data from the warehouse, maintaining the Data Confidence Rating (DCR).

7.) Dragonfly converts checks and DCR reports into events, enabling users to receive alerts via email or SMS, or integrate events into systems like Apache Kafka or AWS SNS.

8) As an API-first platform, Dragonfly integrates smoothly with various databases, data warehouses, and platforms like PostgreSQL, MySQL, AWS Redshift, and more

9.)Dragonfly is a Data Quality Monitoring platform that aims to make data processing secure and accurate for companies.  


**Would love to have your valuable feedback and reviews as i am one of the co-founders for this B2B Saas product**   
",0,1,Decent_Ice1528,2024-01-23 11:19:23,https://www.reddit.com/r/dataengineering/comments/19dm78e/gist_of_dragonfly_wwwdragonflydatahqcom_a_no/,0,False,False,False,False
19bcijh,Do you data lakehouse?,"Do you…

- currently use a data lakehouse

- if so, do you like it?

- if not, do you want to?

- if not, why not?

(Data lakehouse = doing more analytics from your data lake using table formats like iceberg/delta/hudi to use the lake more like a traditional warehouse)",0,21,AMDataLake,2024-01-20 14:07:03,https://www.reddit.com/r/dataengineering/comments/19bcijh/do_you_data_lakehouse/,0,False,False,False,False
19azrc6,DE Project - Created a csv file of data through python. What other tools should I use?,I was planning to take the data from csv file to snow flake and then tableau. I was wondering if I should use any other softwares as I will be trying multiple DE Projects. Looking to get a job in DE Thanks!,0,0,ForUAlwayz,2024-01-20 01:25:55,https://www.reddit.com/r/dataengineering/comments/19azrc6/de_project_created_a_csv_file_of_data_through/,0,False,False,False,False
19aqqab,Interview with Head of IT engineer - how should I prepare?,"I have a interview for a full remote position as Data engineer with some data Analyst activities.

My stack is Azure and few little projects on GCP with 5 YoE in data (engineer, analyst..).

 This company uses GCP + DBT + Looker and Data studio.

What kind of answer should I prepare for? I've never talked with head of IT",0,1,CauliflowerJolly4599,2024-01-19 19:00:10,https://www.reddit.com/r/dataengineering/comments/19aqqab/interview_with_head_of_it_engineer_how_should_i/,0,False,False,False,False
19anfkz,Fresh out of CS Bachelor,"Hi guys,

I've finally graduated from my Computer Science Bachelor course. Being way more interested in Data than pretty much anything else, especially AI (not interested), I'd like to not go into the CS Master, which is basically 3/4 AI and 1/4 anything else. Do you think I'm well positioned to go for a Data engineer carrier with my degree? 

If yes, where do I start learning the skills of a Data Engineer so that I can strike an entry level job in the data field? I read that there are usually other roles before one starts in before getting enough ""trust"" put in their ability to cover the DE role.",0,6,Di4mond4rr3l,2024-01-19 16:44:34,https://www.reddit.com/r/dataengineering/comments/19anfkz/fresh_out_of_cs_bachelor/,0,False,False,False,False
19a8h1q,Worth quitting full time DE job for FAANG Contract? (not Amazon),"Hi everyone, I work as a data engineer at a large entertainment company. I recently received a 12-month contract offer from a FAANG company, offering almost double my current salary. I’m wondering if it’s a smart idea to quit a fulltime position for a FAANG title on my resume? I’m more inclined towards saying no, but I also wanted to know your opinions.",0,6,mnronyasa,2024-01-19 02:43:51,https://www.reddit.com/r/dataengineering/comments/19a8h1q/worth_quitting_full_time_de_job_for_faang/,0,False,False,False,False
199qz6u,Databricks Unity Catalog,"Hi,

I am trying to set up databricks Unity catalog and as I am fairly new to all of this I am struggling with the concept of UC. 

What I am interested the most is whether or not the default location for managed tables stated when establishing UC should be separate Storage accnt or? Also there are three separate Storage accnts for different enviroments (with bronze, silver, gold containers in it) and landing zone in separate accnt and different workspaces for different enviroments. 

What would be the best ""organization"" of Storage?

Thanks!",0,0,MahoYami,2024-01-18 14:09:41,https://www.reddit.com/r/dataengineering/comments/199qz6u/databricks_unity_catalog/,0,False,False,False,False
199nt3j,How to expose the data resides in snowflake via REST API for downstream consumption?,"&#x200B;

https://i.redd.it/lnl14vvul6dc1.gif

[Article is here](https://blog.devgenius.io/exposing-snowflake-data-as-a-rest-api-a-step-by-step-guide-746ceefecaa3)",0,0,BigNo3623,2024-01-18 11:13:25,https://www.reddit.com/r/dataengineering/comments/199nt3j/how_to_expose_the_data_resides_in_snowflake_via/,0,False,False,False,False
199mah5,LSTMs according to their inventor Jürgen Schmidhuber,,0,0,dnulcon,2024-01-18 09:30:52,https://youtu.be/MUnGNI0I2dg?si=0sl1yfQ3LcAKpmOG,0,False,False,False,False
199b7vx,Data Reading,"What would be some recommended reading / projects for a new azure data consultant covering end to end analytics primarily

Heard kimball’s the data warehouse toolkit is a good start but it talks about ETL, and I hear ELT is the new meta?",0,1,FewEstablishment4364,2024-01-17 23:29:28,https://www.reddit.com/r/dataengineering/comments/199b7vx/data_reading/,0,False,False,False,False
1999rtf,First Time DE Job Hunting (Any Tips?),"Regular here, US based, I am finally applying for DE jobs after getting a few Microsoft azure certs under my belt and a few projects too:

- Resume website that’s hosted in the cloud (got this idea from the Cloud Resume Challenge)

- end to end ETL pipeline (to be fair this was a YouTube walk through, it took me two weeks to finish as this was my first time ever being in “the cloud” but finishing solidified that I want to really learn all I can to get into this field.

- currently working on a devops project.

I’ve learned a lot of these last 9 months:
- SQL
- Python
- Azure technologies
- JavaScript 
- HTML
- .CSS
- PySpark

I am a Econ major coming from an excel heavy DA role for the last 4 years.

Does anybody have any tips for someone finally making the jump into job searching for Data Engineering/Cloud/ETL work?  

I would share my resume in the post but I don’t think that’s allowed. 

Thanks",0,5,PoloParachutes,2024-01-17 22:28:27,https://www.reddit.com/r/dataengineering/comments/1999rtf/first_time_de_job_hunting_any_tips/,0,False,False,False,False
1999gr1,Internship interview help,"I am a student who has completed two semesters. Up until this semester I had no idea what I wanted to focus on, so I was a generalist and focused mainly on web development with the goal of improving my python. I had zero coding experience before starting uni.

Anyways, towards the end of semester I decided to focus on data engineering between I love maths and I love programming. I was also a student assistant for python, helping new students learn.

Anyway, last week I decided to apply for a data engineering internship and to my shock, they selected me for an interview. Now I’m freaking out a bit.

I’m in the process of teaching myself some sequel statements and will work on a project over the weekend to improve on my current knowledge.

What can I expect during an interview for a student position?",0,0,IOnlyDrinkWater_22,2024-01-17 22:15:41,https://www.reddit.com/r/dataengineering/comments/1999gr1/internship_interview_help/,0,False,False,False,False
196rt7n,What entry level roles to target to prepare for data engineering?,Hey all. I am coming up on the tail end of a CS degree and was wondering what would be roles a CS grad qualify for that would be best suited to transition into DE? I’m aware that certain jobs are taking much more of a hit in this market than others so does anyone have any suggestions for what roles would be the easiest for a fresh CS grad to get to eventually transition to a DE career? Software engineering seems like trying to win the lottery right now so any other suggestions would be great. I have a decent grasp on Python and SQL. Trying to squeeze an internship before I graduate and want to know what keywords to search for.,0,6,Kylerhanley,2024-01-14 22:10:40,https://www.reddit.com/r/dataengineering/comments/196rt7n/what_entry_level_roles_to_target_to_prepare_for/,0,False,False,False,False
196ogxq,Salary progression?,"I work in insurnace in London and this is my first job out of uni. Currently earning 38k.

Salary review is in April which I will have a little over 1 year experience. What would be a realistic salary increase in my current role?

I know by looking on LinkedIn I should be on average 45-50k with 1 year experience but would like to know from you guys what I could expect. Last year my company gave a 5% increase to my team but the company performed about 3 times worse last year compared to this year.

Many thanks looking forward to reading your responses",0,1,Due_Statistician2604,2024-01-14 19:52:45,https://www.reddit.com/r/dataengineering/comments/196ogxq/salary_progression/,0,False,False,False,False
193mc6o,Segment vs. FiveTran,"Hey y'all, long-time follower of this sub. I'm working on creating a data pipeline at work as a data analyst and am debating between using FiveTran or Segment as the data ingestion tool. I'm looking to funnel data from our Zendesk instance to our SnowFlake DWH, which will eventually be used to create reports on Looker, allow other teams to self-service the data, and hopefully use the data in natural language processing models. I also plan on using DBT to create the database model as well. 

In any normal scenario, I'd probably go with FiveTran for ELT but it seems that they have not been reviewed by my company, which could potentially take months to do and stall the project. We already have an existing relationship with Segment so it's readily available to use. 

Some of the data we're looking to funnel into Snowflake from Zendesk include ticketing metrics, messages sent from the user (e.g. verbatims), tags, ticket links, dates, etc. From what I've read regarding Segment, it's a great ingestion tool for customer event data. Considering I've never built a pipeline before, I'm wondering if there are any drawbacks to using Segment in terms of long-term scalability and reliability as opposed to FiveTran. Any info would be greatly appreciated! ",0,1,callmechin,2024-01-10 23:15:15,https://www.reddit.com/r/dataengineering/comments/193mc6o/segment_vs_fivetran/,0,False,False,False,False
193dh1g,Data warehouse design,"Sorry if this isn’t the right sub. My company is moving down the path of building a traditional dimensionally modeled data warehouse using consultants that are more strategy and report tool focused and seem to know nothing about fact and dimension table design. I’m looking for any recommendation for an established reputable consulting firm that does mostly or entirely dimensional data warehouse design and maybe ETL. We’re a 2 billion revenue specialty construction and contracting firm with projects in maybe 20 states, and yet we don’t have even a basic data warehouse, just JDE accounting system tables. At my prior company we used the Kimball group that Ralph Kimball started and though it was nothing fancy it turned out great. We’re in SF Bay Area, but the consulting firm could probably be anywhere in the U.S. Yes I’ve Googled it and got a couple but hoping to give my boss one or two more option. Thanks.",0,1,jlutt75,2024-01-10 17:17:15,https://www.reddit.com/r/dataengineering/comments/193dh1g/data_warehouse_design/,0,False,False,False,False
1907tpp,We are using databricks notebooks in our project and we have 3 environments and there are multiple teams working in dev environment,"How can we effectively use some branching strategy to ensure new development, bug fixes and code promotions to qa and prod are seamless and there are minimal clashes.

what kind of branching strategy would best suite us given the fact that some code would always be in dev, some being uat tested in qa and some bug fixes for prod issues via hot fixes

Suggestions Please",0,3,azuresnowflake1309,2024-01-06 19:26:11,https://www.reddit.com/r/dataengineering/comments/1907tpp/we_are_using_databricks_notebooks_in_our_project/,0,False,False,False,False
19076x6,Help me redirect my career,"Greetings everyone! To provide some context, I'd like to share my professional background.

I have been working as a Data Engineer for the past six years in Spain, with experience in two major consulting firms, which I'll refer to as Company A and Company B.

At Company A, I spent two years designing and developing data warehouses using a star schema. Subsequently, I created dashboards and OLAP cubes, utilizing Pentaho for ETL processes, SQL Server for data storage, and Cognos for dashboard and data cubes.

Moving on to Company B, where I have been for the past four years and am currently employed. While still involved in ETL processes, the scale of the projects was larger and tasks became more compartmentalized. My role primarily consisted of implementing ETL processes, occasionally contributing to its design. The project involved a data lake stored in MongoDB, with ETL processes developed using Talend and JavaScript. Various technologies such as Jenkins were used for managing different environments, but the main focus remained on MongoDB and Talend. Additionally, Amazon S3 and Azure Blob Storage were utilized for file management.

Despite experiencing a positive salary progression at Company A, my career advancement became VERY stagnant at Company B. A change in management shortly after I joined led to difficulties, they guy just didn't like me and i didn't like him, note that this has been a very unique situation since i never have issues with anybody, i'm a chill guy and most people like to be around me but anyway, I found myself earning between 30-35k€ when my skills and experience warranted at least 40-45k€. While this might seem like a modest difference compared to U.S. salaries, it represents at least a 30% increase.

Facing personal uncertainties, I refrained from making career changes during this period. However, in retrospect, I acknowledge that I should have taken action three years ago. Now actively seeking new opportunities, I'm aware that the technologies I've been working with are somewhat outdated. Considering a shift towards learning Python and obtaining the dp-203 certification independently but i'm concerned about the lack of real work experience in these areas. I wonder if there might be better choices that I'm currently unaware of.

Given this scenario, what approach would you recommend?

Thank you for reading it!",0,5,Davisparrago,2024-01-06 18:59:44,https://www.reddit.com/r/dataengineering/comments/19076x6/help_me_redirect_my_career/,0,False,False,False,False
18zf1fh,FAANG - Data engineer salaries,"Hi everyone
Can you folks share how much does FAANG companies pay to data engineers .
I have heard that they pay lower salaries compared to the SDEs
Would like to know about Meta, Microsoft, Netflix, Apple and Google other similar companies",1,5,UnbiasedGuy_,2024-01-05 19:20:53,https://www.reddit.com/r/dataengineering/comments/18zf1fh/faang_data_engineer_salaries/,0,False,False,False,False
18yg43r,What are the typical patterns for an automation server?,"I’ve got a pipeline built out to move data from a few different sources into my data warehouse. All the infrastructure is hosted except the pipeline itself. I’m ready to put it in a cloud and let it rain.

My idea is to dockerize the project and cron a start schedule. It’ll run every 10 minutes.

However, how do I appropriately plan for growth? Tomorrow I might have 5 automations. Next week I might have 20. Most could run asynchronously (unaware of each other) though their implementation doesn’t need to be asynchronous.

*Sigh…* I feel like the answer is Airflow / Prefect / … isn’t it? Even still, that’s just the orchestration server. How do you organize all your runners / agents?

What are the typical architectures though? One machine per automation, or something like 10 automations per machine? 

I guess you should be right-sizing your infrastructure, but are there any good rules of thumb or high level patterns that typically arise?",0,1,DuckDatum,2024-01-04 15:44:14,https://www.reddit.com/r/dataengineering/comments/18yg43r/what_are_the_typical_patterns_for_an_automation/,0,False,False,False,False
18yb95h,Great expectations tool for generic Data quality framework for Streaming Glue jobs,"1. Does great expectations support streaming jobs?
2. Can we develop a library to run basic checks and generate reports of Data Quality in streaming jobs
 

I know it supports batch but I could not find any documentation if it is supported by streaming pipeline 


Any help is appreciated",0,1,steamed_momos,2024-01-04 11:45:26,https://www.reddit.com/r/dataengineering/comments/18yb95h/great_expectations_tool_for_generic_data_quality/,0,False,False,False,False
18xng3n,MS in Analytics for DE?,"Hey all, I’m thinking of going back to school as my employer will pay for part of it, and I want to do some structured learning. My bachelors degree is non-CS engineering. I’m currently on a DE team (8 months of experience). 
Is an MS in Data Analytics or IT degree useful for data engineers? I’d aim for something like OMSA from Georgia Tech. If the analytics degree has a decent amount of ML, databases, pipeline development on the curriculum, is it useful? 
I’m also considering MS CS programs but I believe those would be harder for me to get into.",0,4,aaloo_chaat,2024-01-03 16:30:19,https://www.reddit.com/r/dataengineering/comments/18xng3n/ms_in_analytics_for_de/,0,False,False,False,False
18vzsfe,How to tackle inconsistency in schemas?,"Hello,   
I've been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We've ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:

Lets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:

SB1 for US is Strength Evaluation

SB2 for US is Power Evaluation

while 

SB1 for GB is Power Evaluation

SB2 for GB is Strength Evaluation  


and its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that

  
| ID | Market | CK  | SB1 | SB2 | SbX | ColX |
|----|--------|-----|-----|-----|-----|------|
| 1  | US     | 1US | 2   | 1   | 9   | 9    |
| 2  | US     | 2US | 2   | 2   | 9   | 9    |
| 3  | US     | 3US | 1   | 1   | 9   | 9    |
| 1  | GB     | 1GB | 3   | 5   | 9   | 9    |
| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |
| 3  | GB     | 3GB | 5   | 3   | 9   | 9    |

What is expected output in that scenario I guess is (look at SB1 SB2 cols)

| ID | Market | CK  | SB1 | SB2 | SbX | ColX |
|----|--------|-----|-----|-----|-----|------|
| 1  | US     | 1US | 2   | 1   | 9   | 9    |
| 2  | US     | 2US | 2   | 2   | 9   | 9    |
| 3  | US     | 3US | 1   | 1   | 9   | 9    |
| 1  | GB     | 1GB | 5   | 3   | 9   | 9    |
| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |
| 3  | GB     | 3GB | 3   | 5   | 9   | 9    |

and I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can't be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly",0,7,Commercial-Ask971,2024-01-01 16:08:02,https://www.reddit.com/r/dataengineering/comments/18vzsfe/how_to_tackle_inconsistency_in_schemas/,0,False,False,False,False
18vyiap,Reflecting on the Year 2023,"A small blog post on open-source, data engineering, and blogging",0,0,nf_x,2024-01-01 15:05:34,https://ssmertin.com/articles/reflecting-on-year-2023/,0,False,False,False,False
18szr7u,An article on adopting Databricks,https://eash98.medium.com/debunking-5-myths-on-adopting-databricks-18140cc3d183,0,2,eash_98,2023-12-28 17:44:13,https://www.reddit.com/r/dataengineering/comments/18szr7u/an_article_on_adopting_databricks/,0,False,False,False,False
1ab952m,Future of Abinito,I have been working in Abinito since last 10yrs and worked on conduct it and not on BRE or express it. Want to know from you guys whats the future hold for Abinitio .,0,0,Terrible_Mud5318,2024-01-26 03:52:01,https://www.reddit.com/r/dataengineering/comments/1ab952m/future_of_abinito/,0,False,False,False,False
19feora,TPC-H Benchmark: Databend Cloud vs. Snowflake,"Databend achieves a 67% cost reduction in data loading and is  approximately 60% less expensive for query execution compared to  Snowflake.

[https://medium.com/@databend/tpc-h-benchmark-databend-cloud-vs-snowflake-aa97971c32a0](https://medium.com/@databend/tpc-h-benchmark-databend-cloud-vs-snowflake-aa97971c32a0)

*For this benchmark, no special tuning was applied. Both Snowflake and Databend Cloud were used with their default settings. And remember,* **don't just take our word for it - you're encouraged to run and verify these results yourself.**",0,0,PsiACE,2024-01-25 17:28:41,https://www.reddit.com/r/dataengineering/comments/19feora/tpch_benchmark_databend_cloud_vs_snowflake/,0,False,False,False,False
19fap0r,Data Engineering Server,"Hey Guys,  


I'm just looking for more active Data Engineering servers or servers that are very active and are good for Data engineers.   


So far I am active on Seattle Data Guy's server.  


&#x200B;",0,2,Wrecked4days,2024-01-25 14:33:57,https://www.reddit.com/r/dataengineering/comments/19fap0r/data_engineering_server/,0,False,False,False,False
19djvg7,Analyzing Hugging Face Datasets with Databend,,0,1,PsiACE,2024-01-23 08:35:56,https://www.databend.com/blog/2024-01-18-analyzing-hugging-face-datasets-with-databend/,0,False,False,False,False
19aw5tk,Custom GPTs for data science and data engineering,"Originally I built these for my own use; but I’m quite happy to see my custom data science and data engineering GPTs on the OpenAI GPT Marketplace are doing well with 500+ and 100+ users.

They are pretty straight forward, referencing the leading data science and data engineering vendors’ online documentation and some of my favorite resources.

If you have a paid version of ChatGPT, please try them out and let me know if you have any suggestions or feature requests.

https://chat.openai.com/g/g-u9rFlUhxK-data-science-consultant

https://chat.openai.com/g/g-gA1cKi1uR-data-engineer-consultant",0,2,drighten,2024-01-19 22:47:03,https://www.reddit.com/r/dataengineering/comments/19aw5tk/custom_gpts_for_data_science_and_data_engineering/,0,False,False,False,False
19a70zm,Job Security Influencer debate,"Hi All,

So there is a growing debate on LinkedIn about becoming an influencer to ensure job security. One half believes you should be actively posting on LinkedIn about topics in your profession to increase your value. It is thought that companies will line up for you and you'll be less likely to face long bouts of unemployment. 

The other side believes we shouldn't have to become influencers to get a job. 

So my question to all of you seasoned DEs is, how do you feel about this? Are any of you posting on LinkedIn, blogging, making videos, etc and feel like this has secured you a place in the industry? 


For those of you that hire, are you looking for the candidate's LinkedIn profile to be full of content?",0,22,TheMightySilverback,2024-01-19 01:33:33,https://www.reddit.com/r/dataengineering/comments/19a70zm/job_security_influencer_debate/,0,False,False,False,False
198ifht,"beginner here, can i get some advice on which tools to use for each major step of the pipeline? (bonus if little to no cost) dataset is flight data",,0,13,canopey,2024-01-17 00:11:14,https://i.redd.it/n72q2wzg6wcc1.jpeg,0,False,False,False,False
194plfy,Professional Certificate: Google IT Automation with Python,,0,0,iphone6plususer,2024-01-12 07:46:56,https://pythoncoursesonline.com/certificate-google-it-automation/,0,False,False,False,False
1934iew,Free Azure Function Course,"I'm excited to share that my latest course, ""Azure Functions: Building Data-Driven Solutions With Python,"" is now live on Udemy! 🌟 Whether you're starting your cloud journey or leveling up your skills, this course has got you covered.

🎁 First 100 enrollments get FREE access: [Your Free Course Link](https://www.udemy.com/course/azure-functions-building-data-driven-solutions-with-python/?couponCode=EFBFF73C1572653E4597)

I'd be super thankful if you support me and leave a 5-star review. Thank you!",0,1,Kairo1004,2024-01-10 09:36:57,https://www.reddit.com/r/dataengineering/comments/1934iew/free_azure_function_course/,0,False,False,False,False
192mrnf,Database Engineering,"Q&A.

&#x200B;

I am attempting to rearrange the data collection and presentation processes of a business I just joined as I know they are dinosaur in their ways. I have no experience with data engineering. The goal is to create a database to store all of our data, some numerical some text. This database will need to connect to a website and or a mobile app to create a dashboard style presentation of the data. In a long run, hypothetical scenario also be able to integrate with GPT technology to create our own GPT.

&#x200B;

If anyone could point me in right direction, that would be a big help.",0,8,Environmental-Egg541,2024-01-09 19:12:41,https://www.reddit.com/r/dataengineering/comments/192mrnf/database_engineering/,0,False,False,False,False
192ea6q,Loading a large dataset into redshift using pyspark,"Currently I am working on a simple beginner project wherein i pull data from S3 and do some pyspark transformations and push data to AWS Redshift.
Issue that I am facing right now is if I use df.write the job starts but is never completed since the record count is too high. I did try using foreach and foreachpartition but i get this attribute error ‘write’ since they do not support it. So I found that there is foreachbatch which requires streaming dataframe to work but since I have data loaded into a dataframe I cannot use this as well. 
I did go through lot of articles and support pages none helped me.
Can I get help on this on how to load large datasets into AWS?",0,9,Neon_Sorcerer,2024-01-09 13:06:10,https://www.reddit.com/r/dataengineering/comments/192ea6q/loading_a_large_dataset_into_redshift_using/,0,False,False,False,False
191dcxw,What kind of ML do DE jobs usually require knowledge of?,"I know DE is different than MLE but it's true that in quite a few small/medium size companies both positions aren't well defined and can share work and attributions


So I ask: what subset of ML knowledge and exp do you think is asked the most by the market in these cases ? From your personal experiences

1) Statistical ML: More statistical models like Regression and such 

2) Deep Learning and Neural networks in general: Trendy stuff with Tools like TF and Pytorch

2.1) Computer Vision: Subset of the above

2.2) NLP: also a subset


I am an aspiring DE learning the basics of the field but I do want to know a bit about ML for career purposes, but I know the field is very deep and complex, so if I could focus on a subset that's more likely to improve me DE career that would be great

So I am excited to hear your thoughts guys!",0,7,SnooPineapples7791,2024-01-08 05:16:06,https://www.reddit.com/r/dataengineering/comments/191dcxw/what_kind_of_ml_do_de_jobs_usually_require/,0,False,False,False,False
190i0sh,Review for Serious SQL course (Data With Danny )?,"Looking for reviews on this course. I am trying to join a course which has case study approach to teaching SQL, I want to solve most real-world problems as possible to gain more confidence when working with SQL or when getting interviewed on it. Thanks for your input.",0,1,StatusStar,2024-01-07 03:06:07,https://www.reddit.com/r/dataengineering/comments/190i0sh/review_for_serious_sql_course_data_with_danny/,0,False,False,False,False
18z9ku4,Any recommendations for people on Linkedin to follow to stay up to date with the hardware side of technology?,"Title\^

But to be more specific, I'm trying to keep up with how hardware tech development that affects software tech development. 

Things like breakthoughs in CPU or storage, etc. 

Also if you know of any educational people to follow that talk about hardware? I'm quite a novice. ",0,8,Justanotherguy2022,2024-01-05 15:32:53,https://www.reddit.com/r/dataengineering/comments/18z9ku4/any_recommendations_for_people_on_linkedin_to/,0,False,False,False,False
18xf26l,"Migrate from ""MySQL+ClickHouse"" combination to Apache Doris",,0,0,ApacheDoris,2024-01-03 09:04:32,https://doris.apache.org/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics,0,False,False,False,False
198ax1v,Where to start learning the technicalities of what this sub discusses as a PM hoping to get into the tech world.,Transitioning from the Marine Corps soon and getting my MBA. I want to be able to understand what you guys talk about and be able to converse about it for companies like google/amazon. Does any one have any recommendations? Was finance in undergrad and specializing in Data Analytics for my MBA.,0,4,amdPCbro,2024-01-16 19:04:58,https://www.reddit.com/r/dataengineering/comments/198ax1v/where_to_start_learning_the_technicalities_of/,0,False,False,False,False
194728m,What kind of jobs in FAANG for an ETL dev?,"Basically the title, what kind of roles should I look at in FAANG as an ETL dev? My skills are adf, python, sql, warehousing.

Please also tell me how you find the right job for you?",0,3,Jealous-Bat-7812,2024-01-11 17:24:07,https://www.reddit.com/r/dataengineering/comments/194728m/what_kind_of_jobs_in_faang_for_an_etl_dev/,0,False,False,False,False
1924ivy,Poll: What programming language will be important for data engineering in 7 years?,"Feel free to comment other programming languages

[View Poll](https://www.reddit.com/poll/1924ivy)",0,13,Several-Echidna3425,2024-01-09 03:14:38,https://www.reddit.com/r/dataengineering/comments/1924ivy/poll_what_programming_language_will_be_important/,0,False,False,False,False
1910hct,I just want to be a good DE,"Hello everyone. Maybe this sounds silly, but I've been working at a data startup for 2 years. I have worked with Python, PySpark, SQL, multiple AWS services, DBT, and several other things. However, the company lacks good practices, and thanks to my good performance, I am increasingly taking a greater role in the projects.

Is it foolish to have a feeling that the project is ""a time bomb"", believing that at any moment it can explode? Is it something normal?

Finally, if you can recommend ways to learn best practices, or at least what the basics are for a project to be well put together from the beginning, that would be great. (courses, books, YouTube channels, etc.)",0,8,ArgenZet,2024-01-07 19:42:40,https://www.reddit.com/r/dataengineering/comments/1910hct/i_just_want_to_be_a_good_de/,0,False,False,False,False
190pmdj,connectors deployment,"My new work is conne tors deployement and I never worked on that. Anyone has experienece of connectors deployement or any material/ course to learn this? Tried searching online I didn't find much. Please guide me if you know .
Thanks",0,1,Sufficient_Koala_609,2024-01-07 10:49:32,https://www.reddit.com/r/dataengineering/comments/190pmdj/connectors_deployment/,0,False,False,False,False
18xl4za,Semi-Structured Data - The challenges.,,0,1,dataengineeringdude,2024-01-03 14:50:18,https://dataengineeringcentral.substack.com/p/semi-structured-data-the-challenges,0,False,False,False,False
18szyuz,check out my new sub stack,"[https://substack.com/@datasketch](https://substack.com/@datasketch)

I write about my learnings in data world in quest to help others.",0,0,photon223,2023-12-28 17:52:56,https://www.reddit.com/r/dataengineering/comments/18szyuz/check_out_my_new_sub_stack/,0,False,False,False,False
19fgizn,Is this for real? $1090 per month as a Senior Data Analyst?,"For context the salary converts to $1090 (4000 Dirhams) per month. For the pros here, what is an ideal salary for data engineers with the skills listed in the job description? Assume the area of work is HCOL.

https://preview.redd.it/gzfbr2sqsmec1.png?width=500&format=png&auto=webp&s=9c2842dc454f443c8c7df8aded9261b362cf9919",0,19,_areebpasha,2024-01-25 18:45:28,https://www.reddit.com/r/dataengineering/comments/19fgizn/is_this_for_real_1090_per_month_as_a_senior_data/,0,False,False,False,False
19e5hpp,Are you employed ?,"

[View Poll](https://www.reddit.com/poll/19e5hpp)",0,0,mjfnd,2024-01-24 01:52:38,https://www.reddit.com/r/dataengineering/comments/19e5hpp/are_you_employed/,0,False,False,False,False
19dmods,How to handle one to many relationships in tables in a dashboard?,"Like for example, you have two tables - Orders and products. 
Orders table contains - user_id, product_id, num_orders, and 
Products table contains - product_id, product_category. 

Now, one product can belong to multiple categories. So one entry in order table might get mapped to multiple entries in products table which leads to data duplicity. Like, total orders per category or per user might be misleading.

What are the ways to handle this?",0,9,divyaanshDev,2024-01-23 11:49:42,https://www.reddit.com/r/dataengineering/comments/19dmods/how_to_handle_one_to_many_relationships_in_tables/,0,False,False,False,False
199uheh,Transitioning from Jr. SDE to ML engineer or MLOps engineer.,"Hi all,  
I'm currently a SDE at a MNC in India (2yrs of work-ex, but at a niche) ,however since I feel my work is not impactful and I've a master's in Mathametics (done out of interest) with B.E in Computer science, So I'm thinking of transitioning. However since I'm mostly unaware of the kind of work and Impact I could bring in. I'm interested in Mathemetics and unable to see myself as fullstack engineer (with the saturation). Please guide me if there're other options I can explore and work towards.",0,3,Sad-Opening-9626,2024-01-18 16:45:48,https://www.reddit.com/r/dataengineering/comments/199uheh/transitioning_from_jr_sde_to_ml_engineer_or_mlops/,0,False,False,False,False
18z7acj,Data Engineer Learning roadmap for 2024,,0,0,GreekYogurtt,2024-01-05 13:48:29,https://niteshx2.medium.com/faang-data-engineer-learning-roadmap-for-2024-199b1c831bca,0,False,False,False,False
18w58rt,Javascript for data engineering?,"Hey, guys! In recent months I’ve noticed an increase in the job postings listing Javascript as a required skill. In some occasions they want to run aws lambda developed in JS, node.js apps deployed in Fargate or d3.js front-end apps. 

All these services could be replaced by Python versions which, to me, sound like a more natural and sensible approach on the data side. 

What do you think about learning JS if working in data?

Does I make any sense even considering these requirements?

I want to believe this won’t become the norm in upcoming months or years, but in the past I’ve observed how during recessions there is a requirements creep phase of asking for more skills since there are less available jobs and the negotiating power of the candidates is slightly weaker. (Basically companies can recruit more senior candidates to cover more areas of the business for a similar budget, thus 2x1)

Your take on this?",0,8,BlackBird-28,2024-01-01 20:04:15,https://www.reddit.com/r/dataengineering/comments/18w58rt/javascript_for_data_engineering/,0,False,False,False,False
18w3lcc,Celebrating New Year with Postgres and Snowflake,Ever wondered how databases celebrate the New Year? Let's dive into a delightful tale of Postgres and Snowflake sharing holiday cheer! [https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake](https://blog.peerdb.io/celebrating-new-year-with-postgres-and-snowflake)  Happy New Year everyone!,0,0,saipeerdb,2024-01-01 18:55:32,https://www.reddit.com/r/dataengineering/comments/18w3lcc/celebrating_new_year_with_postgres_and_snowflake/,0,False,False,False,False
18u7tyn,Do Companies use dataprep for the data engineering needs?,My manager is more interested in building all our data pipelines in dataprep. I find it absurd tool to work with.,0,4,PurpleCurrent3576,2023-12-30 05:20:35,https://www.reddit.com/r/dataengineering/comments/18u7tyn/do_companies_use_dataprep_for_the_data/,0,False,False,False,False
19edpjz,Sql regular expression,"Hi! Can you help, please
I need to write a regular expression so that the tags are replaced with a space (< ... > and <\ ... > ,there is Latin text inside the brackets), if there is a link in the tag, then notice the tag on the link.
 if the link is a phone number, then replace the entire tag with a space. 
Do not replace the <> sign with a space, because it can be used in a mathematical expression

Example input
URL Transmission: Link to our website <a href=""https://docs.oracle.com/"">Oracle</a> SQL<a> 5*5<>20 <a href=""tel:1122334450"">1122334450</a>;

Output 
URL Transmission: Link to our website https://docs.oracle.com/ Oracle  SQL  5*5<>20  1122334450 ;

UPD: 
I have one idea
`SELECT REGEXP_REPLACE('URL Transmission: Link to our website <a href=""https://docs.oracle.com/"">Oracle</a> SQL<a> 5*5<>20 <a href=""tel:1122334450"">1122334450</a>;',
 '<a\s+[^>]*href=""([^""]*)""[^>]*>|<.*?>', ' \1 ') AS modified_text FROM dual;`
But I can't replace a phone number and don't touch <>(5*5<>20)

UPD2: thank you all for your help,
As result I wrote this solution

`REGEXP_REPLACE(text,'<[^>]+href=""(https:[^""]+)"">|<[^>]+>','\1 ')`

In the first part `<[^>]+href=""(https:[^""]+)"">` taking only links
In the second part `<[^>]+>` replacing the remaining tags with a space",0,10,liann_n,2024-01-24 10:03:25,https://www.reddit.com/r/dataengineering/comments/19edpjz/sql_regular_expression/,0,False,False,False,False
197vsja,16 Best+FREE Data Engineering Courses Online & Certifications- 2024,,0,0,Aqsa81,2024-01-16 06:06:04,https://www.mltut.com/best-data-engineering-courses-online/,0,False,False,False,False
191f3mg,I need a complex calculation and I am lost. I don't know if it's the right thread to post,"given a sql table with a column MASTER\_ID, CHANGE\_DATE, moneyamount, cashiername and the following records

&#x200B;

MASTER\_ID, CHANGE\_DATE, moneyamount, cashiername

1, 1/2/22 , 22, shafee

1, 1/3/22 , 23, shafee

1, 1/8/23 , 25, Labiba

1, 1/7/23 , 25, Labiba

2, 1/2/22 , 22, Abanti

2, 1/9/22 , 22, Abanti

2, 1/5/23 , 25, Labiba

3, 1/6/22 , 23, shafee

3, 1/3/22 , 23, shafee

3, 1/2/22 , 22, shafee

3, 1/3/22 , 23, Abanti

4, 1/8/23 , 25, Labiba

&#x200B;

&#x200B;

I want to generate a new table where I have the all columns except change date, columns but for each column, I want the average number of days between changes based on CHANGE\_DATE

&#x200B;

for example

&#x200B;

&#x200B;

&#x200B;

MASTER\_ID, avg\_moneyamount, avg\_cashiername

1 , 132.67 , 370",0,1,ikyorince,2024-01-08 06:59:02,https://www.reddit.com/r/dataengineering/comments/191f3mg/i_need_a_complex_calculation_and_i_am_lost_i_dont/,0,False,False,False,False
18yofwd,Etl developer from Russia want to work in English,"Hi! I'm a junior etl developer(oracle, odi, odm) from Russia. My level English is about A2. In future I want to work in English. What courses and internships can help me?
What technology and apps usually developers use?
Thank you!",0,4,liann_n,2024-01-04 21:26:51,https://www.reddit.com/r/dataengineering/comments/18yofwd/etl_developer_from_russia_want_to_work_in_english/,0,False,False,False,False
18y3bgk,"I am transitioning from Python/SQL Software Engineer and GCP Architect into Data Engineer in 2024 for a change of atmos and to level up my skills, I am a GCP ONLY kind of guy wanting to AVOID APACHE, how do you guys like or dislike the GCP DE tech suite and are the skills I list in my post enough?","My strengths are:

1. Python
2. SQL
3. GCP (solutions architect)

Things I am learning:

1. Yeet data = ETL
2. BigQuery
3. Pub/Sub
4. DataProc
5. Cloud Storage
6. DataFlow
7. DataPrep
8. DataFusion
9. Looker

Is this enough for GCP Data Engineer jobs?

 Can I straight avoid Databricks Apache certs and Snowflake certs?",0,38,Polyglot_Dev_Travels,2024-01-04 03:43:13,https://www.reddit.com/r/dataengineering/comments/18y3bgk/i_am_transitioning_from_pythonsql_software/,0,False,False,False,False
18vlbsy,Startup Idea Validation,"Hello,

I’m considering creating a data product that combines a data lake service with AI-generated insights. Despite existing products like Databricks and Snowflake, along with various AI-based insight companies - Do you think it’s still a viable venture?

Any comments or suggestions? Are there any gaps in the current market that you think should be addressed?

Thanks!",0,9,Curious_Guy81,2024-01-01 00:47:41,https://www.reddit.com/r/dataengineering/comments/18vlbsy/startup_idea_validation/,0,False,False,False,False
18vi3gn,Azure Data Engineer Interview Help,"Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It's time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. 

Firstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. 

Secondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.

Also, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. 

Thanks a lot. Looking forward to listening from you industry folks.",0,7,Vikinghehe,2023-12-31 21:54:23,https://www.reddit.com/r/dataengineering/comments/18vi3gn/azure_data_engineer_interview_help/,0,False,False,False,False
193eu3t,Question for discussion: Why do companies fail when adopting Modern tooling and practices like in the MDS (Modern Data Stack,"In the blog post below the following possibilities for failure are discussed:

1. **Fear of Change**: Many companies struggle with digital transformation because they are afraid to change their old ways of doing things. They stick to familiar processes instead of trying new, digital methods.
2. **Talk vs. Action**: Companies often talk about embracing digital change but don't follow through or **do something that does not support the digital change**. Sometimes they plan for big changes in technology but continue using outdated systems, which slows down progress.
3. **Following the Crowd**: In many organizations, people just follow what others are doing instead of coming up with new, innovative ideas. **The worst case is when people do try to innovate and are shut down or not supported.** This can result in conformity and/or loss of innovators. Either way, this makes it hard for a company to be truly innovative and take advantage of digital opportunities. Especially when the loudest voices are against change.

If you are interested check out the Blog Post: [https://datacoves.com/post/enterprise-digital-transformation](https://datacoves.com/post/enterprise-digital-transformation)",0,6,Data-Queen-Mayra,2024-01-10 18:11:50,https://www.reddit.com/r/dataengineering/comments/193eu3t/question_for_discussion_why_do_companies_fail/,0,False,False,False,False
19cayv2,Data Engineering future in India,"Greetings, esteemed data engineers!
What do you think is the future of data engineering as a career? Is the field getting saturated with less openings. Also, as far as I have observed, salary is way less compared to software developers for the same experience, atleast in India. Please share your insights.",0,3,EasyTonight07,2024-01-21 19:08:44,https://www.reddit.com/r/dataengineering/comments/19cayv2/data_engineering_future_in_india/,0,False,False,False,False
190ygby,List of Experts in Data Engineering on Linkedin.!!,"Hey Fellas,

I’ll keep it short. I’m trying to create an outstanding connections on Linkedin. So, can everyone plz suggest me Linkedin accounts of Prodigies in Data Engineering whose posts, Blogs, youtube channels can help ACE Data engineering role.",0,2,rajveersolanki9,2024-01-07 18:17:21,https://www.reddit.com/r/dataengineering/comments/190ygby/list_of_experts_in_data_engineering_on_linkedin/,0,False,False,False,False
19dutmk,Hello,"Hello, is it too late for a 23 yo to go for data engineering, I think need at least 5 years of studying, I don’t do anything right now. Is investing the next 5 years in data engineering or data science a good deal that i will benefit from in the future?",0,38,IbnQays,2024-01-23 18:14:11,https://www.reddit.com/r/dataengineering/comments/19dutmk/hello/,0,False,False,False,False
191vw1r,"Engineering manager in 6 years, 3 at an early stage startup. AMA","* started working in 2018
* switched a few times
* currently working at an early stage startup, here since last 3 years. Built the product from scratch
* Responsible for data and infrastructure charter",0,21,Lower_Remote_6074,2024-01-08 21:00:42,https://www.reddit.com/r/dataengineering/comments/191vw1r/engineering_manager_in_6_years_3_at_an_early/,0,False,False,False,False
18z3xr8,Is the demand of data engineers dramatically reduced?,"I have a great skill set and expertise as a data engineer/scientist, but my country(Nepal) has little to none openings.

Is reddit cool to find a potential employer? Can we connect on LinkedIn?

I bet, just one interview and you'll see my potential value as a Data Engineer/Scientist.",0,11,Horror_Comment6061,2024-01-05 10:36:49,https://www.reddit.com/r/dataengineering/comments/18z3xr8/is_the_demand_of_data_engineers_dramatically/,0,False,False,False,False
19cq3qv,Are data engineers shyer than data scientists ?,"I have joined thesee two channel, r/dataengineering and r/datascience , and there is no doubt that the DS channel has much more activities much than the DE side.

And I also posted an easy-to-answer topic, 500+  views but no answers. [https://www.reddit.com/r/dataengineering/comments/19cpb97/the\_best\_way\_to\_reduce\_aws\_emr\_costs/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/dataengineering/comments/19cpb97/the_best_way_to_reduce_aws_emr_costs/?utm_source=share&utm_medium=web2x&context=3)

So,  are data engineers shyer than data scientists ? ",0,11,No_Structure3465,2024-01-22 07:22:11,https://www.reddit.com/r/dataengineering/comments/19cq3qv/are_data_engineers_shyer_than_data_scientists/,0,False,False,False,False
18w3pvz,3 Mistakes Data Engineers Make When Consuming APIs,"Hey everybody, I started lurking in this community just a few months ago.

I just love how responsive the community is to questions that appear.
But I also think there are many skilled people who could spark discussions by sharing there knowledge.
My idea is to post often, maybe even daily and test what happens.
It doesn't look like it's against the rules, but I'll stop if that's not received well.

So, here we are:

🔸 Neglecting Documentation

❓ If you don’t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.

For instance, if you don’t specify the response format, you might get a default format that’s difficult to parse.

I’ve even seen one service promoting their “easier” SOAP API while having a much more flexible and modern RESTful or GraphQL API.

✅ Thoroughly review the API documentation to understand its terms of use and how to make requests.

Check links and standard parameters, and read plain English text. You never know what you’ll find.



🔸 Ignoring Error Handling

❓ Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.

There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.

Let’s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.

✅ Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.

Think of what can fail and handle those scenarios before the scenario where everything works.



🔸 Fetching Too Much Information

❓ Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.

On top of that, you’ll need to pay more for computing costs and API calls if that’s how your provider works.

✅ Minimize unnecessary data transfer and implement caching mechanisms to improve performance.

Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don’t fetch objects you don’t need.

—
What else?",0,31,ivanovyordan,2024-01-01 19:00:51,https://www.reddit.com/r/dataengineering/comments/18w3pvz/3_mistakes_data_engineers_make_when_consuming_apis/,0,False,False,False,False
19a241q,Why the acronym ETL instead of LTE,"Why ‘Extract Transform Load’ and not ‘Load Transform Extract’

When I hear Extract I associate it with Export even if they are not the same thing.

And I associate Load to Upload or Import.

What are your thoughts?",0,6,kedpro,2024-01-18 21:57:40,https://www.reddit.com/r/dataengineering/comments/19a241q/why_the_acronym_etl_instead_of_lte/,0,False,False,False,False
19d9p1u,CEO of Data Engineer Academy Here - Q & A Next 24 Hours. Ask me anything!,"Ask me anything related to data, data engineering, your app process, a blueprint, etc.

My goal is to provide as much value as I possibly can as a thank you to this community.

&#x200B;

We’ve helped hundreds privately and thousands in total to get a role in the data space. We work with staffing agencies, companies, and deal with thousands of apps getting sent out every month. Today, I want to share that inside view in hopes that someone can benefit from it.

&#x200B;

I’ll do my best to get to every question!",0,36,chrisgarzon19,2024-01-22 23:22:46,https://www.reddit.com/r/dataengineering/comments/19d9p1u/ceo_of_data_engineer_academy_here_q_a_next_24/,0,False,False,False,False
18vev5r,Who is hierarchically superior - data engineer or data scientist,"I am apologizing beforehand if somehow is offended by my question.

I am in a dilemma on which role should I venture into - data engineer or data scientist. I am more inclined towards data engineer as it pertains to my skill set but I am in doubt if there is a ceiling in the career growth. Like can a data engineer be offered a executive role or be a part of the board of directors?",0,32,X_Warrior361,2023-12-31 19:17:30,https://www.reddit.com/r/dataengineering/comments/18vev5r/who_is_hierarchically_superior_data_engineer_or/,0,False,False,False,False
